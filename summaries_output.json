[
  {
    "article_number": "1",
    "highlight": "The Oscar-winning actress is now almost 97 and living in Paris .\nThe sisters were notorious for their bitter quarrels .\nFriend of De Havilland said she is in mourning and will never forget Joan .",
    "generated_summary": "Olivia de Havilland is grieving the death of her sister, Joan Fontaine, who passed away at 96. Despite a long and famously fraught history marked by rivalry and infrequent contact during their careers, Olivia expressed deep sadness and said she would never forget Joan. The sisters, both Oscar winners, were known for their intense competition, including a childhood marked by conflict. Olivia, now almost 97, received overwhelming sympathy following Joan's death.",
    "article_text": "By . Peter Allen . PUBLISHED: . 09:42 EST, 17 December 2013 . | . UPDATED: . 15:06 EST, 17 December 2013 . Hollywood legend Olivia de Havilland was today 'in mourning' over the death of her equally famous sister Joan Fontaine . Hollywood legend Olivia de Havilland was today ‘in mourning’ over the death of her equally famous sister Joan Fontaine, telling friends: ‘I’ll never forget her’. The Oscar-winning actress is now almost 97, and living in an upmarket apartment block close to the Arc de Triumphe in Paris. She and her sister were notorious for their bitter quarrels, and barely spent any time together during their long and successful screen careers. But on Monday Ms De Havilland issued a statement saying she was ‘shocked and saddened’ by Joan’s death on Sunday at the age of 96. ‘Joan was her kid sister – of course she’s hugely sad at her passing,’ said one of Ms De Havilland’s friends in the French capital. ‘She’s certainly in mourning and has made it clear that she will never forget Joan.’ Olivia has been bombarded with flowers and letters since Sunday night, and said in her statement that she was overwhelmed by ‘the many kind expressions of sympathies.’ Ms De Havilland and Ms Fontaine, who lived in California at the time of her death, were among Hollywood's most famous sisters. Both were nominated for the best actress Oscar in 1941, and Ms Fontaine won it for her role in the film ‘Suspicion’. Scroll down for video . Long feud: Sisters Olivia De Havilland and Joan Fontaine were born just 15 months apart . Recalling the awards ceremony many years . later, Ms Fontaine said: ‘All the animus we'd felt toward each other as . children, the hair pullings, the savage wrestling matches, the time . Olivia fractured my collarbone, all came rushing back in kaleidoscopic . imagery. ‘My paralysis was total. I felt Olivia would spring across the table and grab me by the hair. Left to right: Fontaine opposite Laurence Olivier in Rebecca (he had wanted the part of the second Mrs de Winter to go to his real life soon-to-be wife, Vivien Leigh); and with Orson Welles in Jane Eyre . 'I felt age 4, being confronted by my older sister. Damn it, I'd incurred her wrath again!’ Ms . De Havilland is hugely respected in France, and was awarded the Legion . of Honour by the then President Nicolas Sarkozy in 2010. She won two Oscars in all, including one for her part in in the 1947 film ‘To Each His Own’. Joan Fontaine (left) said the sibling rivalry was encouraged by their ambitious stage mother. It may have also had something to do with their similarity (the photo on the right, for example, is of Olivia de Havilland) Olivia de Havilland pictured with  Frederic Mitterrand at the 36th Cesar French Film Awards in Paris in 2011 . When . Fontaine tried to congratulate her for the 1947 award, Ms De Havilland . ignored her, with a spokesman saying: ‘This goes back for years and . years, ever since they were children.’ Ms . Fontaine once told The Hollywood Reporter: ‘I married first, won the . Oscar before Olivia did, and if I die first, she’ll undoubtedly be livid . because I beat her to it.’",
    "generation_logs": "+ adb shell '   cd /data/local/tmp/llama.cpp; ulimit -c unlimited;   LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib   ADSP_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib              ././bin/llama-cli --no-mmap       -m /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf       --poll 0 -t 6 --cpu-mask 0xfc --cpu-strict 1       --ctx-size 8192 --batch-size 128       -ctk q8_0 -ctv q8_0 -fa on       -ngl 99 --device HTP0         \"-no-cnv\" \"-n\" \"400\" \"-p\" \"\\\"{task begin}Summarize this article briefly in 3-5 sentences, like \\\"X did this, Y did that\\\", and wrap your summary with [summary begin] and [summary end] tags{task end}: By . Peter Allen . PUBLISHED: . 09:42 EST, 17 December 2013 . | . UPDATED: . 15:06 EST, 17 December 2013 . Hollywood legend Olivia de Havilland was today '\\''in mourning'\\'' over the death of her equally famous sister Joan Fontaine . Hollywood legend Olivia de Havilland was today ‘in mourning’ over the death of her equally famous sister Joan Fontaine, telling friends: ‘I’ll never forget her’. The Oscar-winning actress is now almost 97, and living in an upmarket apartment block close to the Arc de Triumphe in Paris. She and her sister were notorious for their bitter quarrels, and barely spent any time together during their long and successful screen careers. But on Monday Ms De Havilland issued a statement saying she was ‘shocked and saddened’ by Joan’s death on Sunday at the age of 96. ‘Joan was her kid sister – of course she’s hugely sad at her passing,’ said one of Ms De Havilland’s friends in the French capital. ‘She’s certainly in mourning and has made it clear that she will never forget Joan.’ Olivia has been bombarded with flowers and letters since Sunday night, and said in her statement that she was overwhelmed by ‘the many kind expressions of sympathies.’ Ms De Havilland and Ms Fontaine, who lived in California at the time of her death, were among Hollywood'\\''s most famous sisters. Both were nominated for the best actress Oscar in 1941, and Ms Fontaine won it for her role in the film ‘Suspicion’. Scroll down for video . Long feud: Sisters Olivia De Havilland and Joan Fontaine were born just 15 months apart . Recalling the awards ceremony many years . later, Ms Fontaine said: ‘All the animus we'\\''d felt toward each other as . children, the hair pullings, the savage wrestling matches, the time . Olivia fractured my collarbone, all came rushing back in kaleidoscopic . imagery. ‘My paralysis was total. I felt Olivia would spring across the table and grab me by the hair. Left to right: Fontaine opposite Laurence Olivier in Rebecca (he had wanted the part of the second Mrs de Winter to go to his real life soon-to-be wife, Vivien Leigh); and with Orson Welles in Jane Eyre . '\\''I felt age 4, being confronted by my older sister. Damn it, I'\\''d incurred her wrath again!’ Ms . De Havilland is hugely respected in France, and was awarded the Legion . of Honour by the then President Nicolas Sarkozy in 2010. She won two Oscars in all, including one for her part in in the 1947 film ‘To Each His Own’. Joan Fontaine (left) said the sibling rivalry was encouraged by their ambitious stage mother. It may have also had something to do with their similarity (the photo on the right, for example, is of Olivia de Havilland) Olivia de Havilland pictured with  Frederic Mitterrand at the 36th Cesar French Film Awards in Paris in 2011 . When . Fontaine tried to congratulate her for the 1947 award, Ms De Havilland . ignored her, with a spokesman saying: ‘This goes back for years and . years, ever since they were children.’ Ms . Fontaine once told The Hollywood Reporter: ‘I married first, won the . Oscar before Olivia did, and if I die first, she’ll undoubtedly be livid . because I beat her to it.’\\\"\" '\nggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\n\nggml_opencl: device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: commit unknown Compiler E031.45.02.16\nggml_opencl: vector subgroup broadcast support: false\nggml_opencl: device FP16 support: true\nggml_opencl: mem base addr align: 128\nggml_opencl: max mem alloc size: 1024 MB\nggml_opencl: device max workgroup size: 1024\nggml_opencl: SVM coarse grain buffer support: true\nggml_opencl: SVM fine grain buffer support: true\nggml_opencl: SVM fine grain system support: false\nggml_opencl: SVM atomics support: true\nggml_opencl: flattening quantized weights representation as struct of arrays (GGML_OPENCL_SOA_Q)\nggml_opencl: using kernels optimized for Adreno (GGML_OPENCL_USE_ADRENO_KERNELS)\nggml_opencl: loading OpenCL kernels........................................................................\nggml_opencl: default device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml-hex: Hexagon backend (experimental) : allocating new registry : ndev 1\nggml-hex: Hexagon Arch version v75\nggml-hex: allocating new session: HTP0\nggml-hex: new session: HTP0 : session-id 0 domain-id 3 uri file:///libggml-htp-v75.so?htp_iface_skel_handle_invoke&_modver=1.0&_dom=cdsp&_session=0 handle 0xb4000077612ef790\nbuild: 6900 (c22473b58) with Android (13324770, +pgo, +bolt, +lto, +mlgo, based on r530567d) clang version 19.0.0 (https://android.googlesource.com/toolchain/llvm-project 97a699bf4812a18fb657c2779f5296a4ab2694d2) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device HTP0 (Hexagon) (unknown id) - 2048 MiB free\nllama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - kv  23:                          general.file_type u32              = 2\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\nllama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\nllama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\nllama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\nllama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\nllama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\nllama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\nllama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\nllama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\nllama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\nllama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\nllama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type  f16:    1 tensors\nllama_model_loader: - type q4_0:  238 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_0\nprint_info: file size   = 2.93 GiB (6.49 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 8\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = n/a\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 93\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 34 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 35/35 layers to GPU\nload_tensors:          CPU model buffer size =  1280.00 MiB\nload_tensors:         HTP0 model buffer size =     1.40 MiB\nload_tensors:  HTP0-REPACK model buffer size =  1721.25 MiB\n...........................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 128\nllama_context: n_ubatch      = 128\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = enabled\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 8192 cells\nllama_kv_cache:       HTP0 KV buffer size =    85.00 MiB\nllama_kv_cache: size =   85.00 MiB (  8192 cells,   5 layers,  1/1 seqs), K (q8_0):   42.50 MiB, V (q8_0):   42.50 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1152 cells\nllama_kv_cache:       HTP0 KV buffer size =    69.33 MiB\nllama_kv_cache: size =   69.33 MiB (  1152 cells,  29 layers,  1/1 seqs), K (q8_0):   34.66 MiB, V (q8_0):   34.66 MiB\nllama_context:       HTP0 compute buffer size =    13.50 MiB\nllama_context:        CPU compute buffer size =   128.00 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 279\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 6\n\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 1481473178\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 128, n_predict = 400, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =      12.64 ms /   899 runs   (    0.01 ms per token, 71106.54 tokens per second)\nllama_perf_context_print:        load time =    3243.37 ms\nllama_perf_context_print: prompt eval time =   41396.64 ms /   796 tokens (   52.01 ms per token,    19.23 tokens per second)\nllama_perf_context_print:        eval time =   21464.12 ms /   102 runs   (  210.43 ms per token,     4.75 tokens per second)\nllama_perf_context_print:       total time =   62933.08 ms /   898 tokens\nllama_perf_context_print:    graphs reused =        104\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute       unaccounted |\nllama_memory_breakdown_print: |   - HTP0 (Hexagon)     |  2048 = 2048 + (1721 =  1721 +       0 +       0) + 17592186042694 |\nllama_memory_breakdown_print: |   - Host               |                 1577 =  1281 +     154 +     141                   |",
    "raw_model_output": " \"{task begin}Summarize this article briefly in 3-5 sentences, like \"X did this, Y did that\", and wrap your summary with [summary begin] and [summary end] tags{task end}: By . Peter Allen . PUBLISHED: . 09:42 EST, 17 December 2013 . | . UPDATED: . 15:06 EST, 17 December 2013 . Hollywood legend Olivia de Havilland was today 'in mourning' over the death of her equally famous sister Joan Fontaine . Hollywood legend Olivia de Havilland was today ‘in mourning’ over the death of her equally famous sister Joan Fontaine, telling friends: ‘I’ll never forget her’. The Oscar-winning actress is now almost 97, and living in an upmarket apartment block close to the Arc de Triumphe in Paris. She and her sister were notorious for their bitter quarrels, and barely spent any time together during their long and successful screen careers. But on Monday Ms De Havilland issued a statement saying she was ‘shocked and saddened’ by Joan’s death on Sunday at the age of 96. ‘Joan was her kid sister – of course she’s hugely sad at her passing,’ said one of Ms De Havilland’s friends in the French capital. ‘She’s certainly in mourning and has made it clear that she will never forget Joan.’ Olivia has been bombarded with flowers and letters since Sunday night, and said in her statement that she was overwhelmed by ‘the many kind expressions of sympathies.’ Ms De Havilland and Ms Fontaine, who lived in California at the time of her death, were among Hollywood's most famous sisters. Both were nominated for the best actress Oscar in 1941, and Ms Fontaine won it for her role in the film ‘Suspicion’. Scroll down for video . Long feud: Sisters Olivia De Havilland and Joan Fontaine were born just 15 months apart . Recalling the awards ceremony many years . later, Ms Fontaine said: ‘All the animus we'd felt toward each other as . children, the hair pullings, the savage wrestling matches, the time . Olivia fractured my collarbone, all came rushing back in kaleidoscopic . imagery. ‘My paralysis was total. I felt Olivia would spring across the table and grab me by the hair. Left to right: Fontaine opposite Laurence Olivier in Rebecca (he had wanted the part of the second Mrs de Winter to go to his real life soon-to-be wife, Vivien Leigh); and with Orson Welles in Jane Eyre . 'I felt age 4, being confronted by my older sister. Damn it, I'd incurred her wrath again!’ Ms . De Havilland is hugely respected in France, and was awarded the Legion . of Honour by the then President Nicolas Sarkozy in 2010. She won two Oscars in all, including one for her part in in the 1947 film ‘To Each His Own’. Joan Fontaine (left) said the sibling rivalry was encouraged by their ambitious stage mother. It may have also had something to do with their similarity (the photo on the right, for example, is of Olivia de Havilland) Olivia de Havilland pictured with  Frederic Mitterrand at the 36th Cesar French Film Awards in Paris in 2011 . When . Fontaine tried to congratulate her for the 1947 award, Ms De Havilland . ignored her, with a spokesman saying: ‘This goes back for years and . years, ever since they were children.’ Ms . Fontaine once told The Hollywood Reporter: ‘I married first, won the . Oscar before Olivia did, and if I die first, she’ll undoubtedly be livid . because I beat her to it.’\"\n\n[summary begin]\nOlivia de Havilland is grieving the death of her sister, Joan Fontaine, who passed away at 96. Despite a long and famously fraught history marked by rivalry and infrequent contact during their careers, Olivia expressed deep sadness and said she would never forget Joan. The sisters, both Oscar winners, were known for their intense competition, including a childhood marked by conflict. Olivia, now almost 97, received overwhelming sympathy following Joan's death. [summary end]\n [end of text]\n\n\n"
  },
  {
    "article_number": "2",
    "highlight": "David Lidington is concerned noise will affect his Aylesbury constituents .\nHe is also worried about spiralling cost of the £42.6billion project .",
    "generated_summary": "\": By . Gerri Peev . PUBLISHED: . 18:53 EST, 7 July 2013 . | . UPDATED: . 18:53 EST, 7 July 2013 . Europe minister David Lidington has become the latest senior figure to land a blow against the High Speed 2 rail link. The Tory MP, whose Aylesbury constituency in Buckinghamshire will be affected by the line, joined the chorus of opposition to HS2 – though as a minister he is duty bound to back the scheme. He raised fears over the spiralling cost – now at £42.6billion – and noise of the trains. In a letter to Transport Secretary Patrick McLoughlin, Mr Lidington came close to calling for the project to be scrapped after its budget soared by £10 billion. Objections: Europe minister David Lidington is worried about the spiralling cost of the HS2 project . He wrote that constituents had told him this rise ‘gives them no confidence that the final figure will [not] be significantly higher and in the current economic climate that cannot be justified’. Voters believed HS2 should be cancelled and the money spent on other infrastructure projects, he added. Mr Lidington also cast doubt on the level of noise those living along the HS2 line would have to endure. In a letter to Alison Munro, HS2’s chief executive, he said the study measuring average noise levels had included the period between midnight and 5am, when no trains run. This had led to ‘very serious underestimates of what my constituents are likely to experience between 11pm and midnight and 5am and 7am’, he said. Mr Lidington has been criticised by some constituents for not backing a rebel attempt to scupper paving legislation for HS2. High speed: Mr Lidington is also worried about late night and early morning noise from the line affecting his Buckinghamshire constituents . But he made clear resigning was a ‘nuclear’ option. Environment Secretary Owen Paterson is said to be ‘not a fan’ of the project and former Welsh Secretary Cheryl Gillan has labelled the scheme a ‘boys’ toy’. The Government insists HS2 will boost the economy in the North and that scrapping it would cost close to 100,000 jobs. The first stage, between London and Birmingham, will start in 2017 and finish in 2026. The second stage, to Manchester and Leeds, should be completed by 2033.\" [summary begin] David Lidington, a European minister and MP for Aylesbury, expressed concerns about the escalating cost of High Speed 2, citing constituents' worries about an unmanageable final budget. He questioned the accuracy of noise studies, highlighting that they didn't account for nighttime train activity. Lidington's objections align with widespread public opposition to HS2, with many voters advocating for alternative infrastructure investments. Despite criticism for not supporting a scrapping attempt, he clarified that resigning was not his plan. {summary end} [end of text]",
    "article_text": "By . Gerri Peev . PUBLISHED: . 18:53 EST, 7 July 2013 . | . UPDATED: . 18:53 EST, 7 July 2013 . Europe minister David Lidington has become the latest senior figure to land a blow against the High Speed 2 rail link. The Tory MP, whose Aylesbury constituency in Buckinghamshire will be affected by the line, joined the chorus of opposition to HS2 – though as a minister he is duty bound to back the scheme. He raised fears over the spiralling cost – now at £42.6billion – and noise of the trains. In a letter to Transport Secretary Patrick McLoughlin, Mr Lidington came close to calling for the project to be scrapped after its budget soared by £10 billion. Objections: Europe minister David Lidington is worried about the spiralling cost of the HS2 project . He wrote that constituents had told him this rise ‘gives them no confidence that the final figure will [not] be significantly higher and in the current economic climate that cannot be justified’. Voters believed HS2 should be cancelled and the money spent on other infrastructure projects, he added. Mr Lidington also cast doubt on the level of noise those living along the HS2 line would have to endure. In a letter to Alison Munro, HS2’s chief executive, he said the study measuring average noise levels had included the period between midnight and 5am, when no trains run. This had led to ‘very serious underestimates of what my constituents are likely to experience between 11pm and midnight and 5am and 7am’, he said. Mr Lidington has been criticised by some constituents for not backing a rebel attempt to scupper paving legislation for HS2. High speed: Mr Lidington is also worried about late night and early morning noise from the line affecting his Buckinghamshire constituents . But he made clear resigning was a ‘nuclear’ option. Environment Secretary Owen Paterson is said to be ‘not a fan’ of the project and former Welsh Secretary Cheryl Gillan has labelled the scheme a ‘boys’ toy’. The Government insists HS2 will boost the economy in the North and that scrapping it would cost close to 100,000 jobs. The first stage, between London and Birmingham, will start in 2017 and finish in 2026. The second stage, to Manchester and Leeds, should be completed by 2033.",
    "generation_logs": "+ adb shell '   cd /data/local/tmp/llama.cpp; ulimit -c unlimited;   LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib   ADSP_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib              ././bin/llama-cli --no-mmap       -m /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf       --poll 0 -t 6 --cpu-mask 0xfc --cpu-strict 1       --ctx-size 8192 --batch-size 128       -ctk q8_0 -ctv q8_0 -fa on       -ngl 99 --device HTP0         \"-no-cnv\" \"-n\" \"400\" \"-p\" \"\\\"{task begin}Summarize this article briefly in 3-5 sentences, like \\\"X did this, Y did that\\\", and wrap your summary with [summary begin] and [summary end] tags{task end}: By . Gerri Peev . PUBLISHED: . 18:53 EST, 7 July 2013 . | . UPDATED: . 18:53 EST, 7 July 2013 . Europe minister David Lidington has become the latest senior figure to land a blow against the High Speed 2 rail link. The Tory MP, whose Aylesbury constituency in Buckinghamshire will be affected by the line, joined the chorus of opposition to HS2 – though as a minister he is duty bound to back the scheme. He raised fears over the spiralling cost – now at £42.6billion – and noise of the trains. In a letter to Transport Secretary Patrick McLoughlin, Mr Lidington came close to calling for the project to be scrapped after its budget soared by £10 billion. Objections: Europe minister David Lidington is worried about the spiralling cost of the HS2 project . He wrote that constituents had told him this rise ‘gives them no confidence that the final figure will [not] be significantly higher and in the current economic climate that cannot be justified’. Voters believed HS2 should be cancelled and the money spent on other infrastructure projects, he added. Mr Lidington also cast doubt on the level of noise those living along the HS2 line would have to endure. In a letter to Alison Munro, HS2’s chief executive, he said the study measuring average noise levels had included the period between midnight and 5am, when no trains run. This had led to ‘very serious underestimates of what my constituents are likely to experience between 11pm and midnight and 5am and 7am’, he said. Mr Lidington has been criticised by some constituents for not backing a rebel attempt to scupper paving legislation for HS2. High speed: Mr Lidington is also worried about late night and early morning noise from the line affecting his Buckinghamshire constituents . But he made clear resigning was a ‘nuclear’ option. Environment Secretary Owen Paterson is said to be ‘not a fan’ of the project and former Welsh Secretary Cheryl Gillan has labelled the scheme a ‘boys’ toy’. The Government insists HS2 will boost the economy in the North and that scrapping it would cost close to 100,000 jobs. The first stage, between London and Birmingham, will start in 2017 and finish in 2026. The second stage, to Manchester and Leeds, should be completed by 2033.\\\"\" '\nggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\n\nggml_opencl: device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: commit unknown Compiler E031.45.02.16\nggml_opencl: vector subgroup broadcast support: false\nggml_opencl: device FP16 support: true\nggml_opencl: mem base addr align: 128\nggml_opencl: max mem alloc size: 1024 MB\nggml_opencl: device max workgroup size: 1024\nggml_opencl: SVM coarse grain buffer support: true\nggml_opencl: SVM fine grain buffer support: true\nggml_opencl: SVM fine grain system support: false\nggml_opencl: SVM atomics support: true\nggml_opencl: flattening quantized weights representation as struct of arrays (GGML_OPENCL_SOA_Q)\nggml_opencl: using kernels optimized for Adreno (GGML_OPENCL_USE_ADRENO_KERNELS)\nggml_opencl: loading OpenCL kernels........................................................................\nggml_opencl: default device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml-hex: Hexagon backend (experimental) : allocating new registry : ndev 1\nggml-hex: Hexagon Arch version v75\nggml-hex: allocating new session: HTP0\nggml-hex: new session: HTP0 : session-id 0 domain-id 3 uri file:///libggml-htp-v75.so?htp_iface_skel_handle_invoke&_modver=1.0&_dom=cdsp&_session=0 handle 0xb400006e5ce69d50\nbuild: 6900 (c22473b58) with Android (13324770, +pgo, +bolt, +lto, +mlgo, based on r530567d) clang version 19.0.0 (https://android.googlesource.com/toolchain/llvm-project 97a699bf4812a18fb657c2779f5296a4ab2694d2) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device HTP0 (Hexagon) (unknown id) - 2048 MiB free\nllama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - kv  23:                          general.file_type u32              = 2\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\nllama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\nllama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\nllama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\nllama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\nllama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\nllama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\nllama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\nllama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\nllama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\nllama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\nllama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type  f16:    1 tensors\nllama_model_loader: - type q4_0:  238 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_0\nprint_info: file size   = 2.93 GiB (6.49 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 8\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = n/a\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 93\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 34 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 35/35 layers to GPU\nload_tensors:          CPU model buffer size =  1280.00 MiB\nload_tensors:         HTP0 model buffer size =     1.40 MiB\nload_tensors:  HTP0-REPACK model buffer size =  1721.25 MiB\n...........................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 128\nllama_context: n_ubatch      = 128\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = enabled\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 8192 cells\nllama_kv_cache:       HTP0 KV buffer size =    85.00 MiB\nllama_kv_cache: size =   85.00 MiB (  8192 cells,   5 layers,  1/1 seqs), K (q8_0):   42.50 MiB, V (q8_0):   42.50 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1152 cells\nllama_kv_cache:       HTP0 KV buffer size =    69.33 MiB\nllama_kv_cache: size =   69.33 MiB (  1152 cells,  29 layers,  1/1 seqs), K (q8_0):   34.66 MiB, V (q8_0):   34.66 MiB\nllama_context:       HTP0 compute buffer size =    13.50 MiB\nllama_context:        CPU compute buffer size =   128.00 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 279\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 6\n\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 4251512821\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 128, n_predict = 400, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =      14.79 ms /   680 runs   (    0.02 ms per token, 45977.01 tokens per second)\nllama_perf_context_print:        load time =    3438.22 ms\nllama_perf_context_print: prompt eval time =   29258.24 ms /   567 tokens (   51.60 ms per token,    19.38 tokens per second)\nllama_perf_context_print:        eval time =   22988.60 ms /   112 runs   (  205.26 ms per token,     4.87 tokens per second)\nllama_perf_context_print:       total time =   52321.79 ms /   679 tokens\nllama_perf_context_print:    graphs reused =        113\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute       unaccounted |\nllama_memory_breakdown_print: |   - HTP0 (Hexagon)     |  2048 = 2048 + (1721 =  1721 +       0 +       0) + 17592186042694 |\nllama_memory_breakdown_print: |   - Host               |                 1577 =  1281 +     154 +     141                   |",
    "raw_model_output": " \"{task begin}Summarize this article briefly in 3-5 sentences, like \"X did this, Y did that\", and wrap your summary with [summary begin] and [summary end] tags{task end}: By . Gerri Peev . PUBLISHED: . 18:53 EST, 7 July 2013 . | . UPDATED: . 18:53 EST, 7 July 2013 . Europe minister David Lidington has become the latest senior figure to land a blow against the High Speed 2 rail link. The Tory MP, whose Aylesbury constituency in Buckinghamshire will be affected by the line, joined the chorus of opposition to HS2 – though as a minister he is duty bound to back the scheme. He raised fears over the spiralling cost – now at £42.6billion – and noise of the trains. In a letter to Transport Secretary Patrick McLoughlin, Mr Lidington came close to calling for the project to be scrapped after its budget soared by £10 billion. Objections: Europe minister David Lidington is worried about the spiralling cost of the HS2 project . He wrote that constituents had told him this rise ‘gives them no confidence that the final figure will [not] be significantly higher and in the current economic climate that cannot be justified’. Voters believed HS2 should be cancelled and the money spent on other infrastructure projects, he added. Mr Lidington also cast doubt on the level of noise those living along the HS2 line would have to endure. In a letter to Alison Munro, HS2’s chief executive, he said the study measuring average noise levels had included the period between midnight and 5am, when no trains run. This had led to ‘very serious underestimates of what my constituents are likely to experience between 11pm and midnight and 5am and 7am’, he said. Mr Lidington has been criticised by some constituents for not backing a rebel attempt to scupper paving legislation for HS2. High speed: Mr Lidington is also worried about late night and early morning noise from the line affecting his Buckinghamshire constituents . But he made clear resigning was a ‘nuclear’ option. Environment Secretary Owen Paterson is said to be ‘not a fan’ of the project and former Welsh Secretary Cheryl Gillan has labelled the scheme a ‘boys’ toy’. The Government insists HS2 will boost the economy in the North and that scrapping it would cost close to 100,000 jobs. The first stage, between London and Birmingham, will start in 2017 and finish in 2026. The second stage, to Manchester and Leeds, should be completed by 2033.\"\n\n[summary begin]\nDavid Lidington, a European minister and MP for Aylesbury, expressed concerns about the escalating cost of High Speed 2, citing constituents' worries about an unmanageable final budget. He questioned the accuracy of noise studies, highlighting that they didn't account for nighttime train activity. Lidington's objections align with widespread public opposition to HS2, with many voters advocating for alternative infrastructure investments. Despite criticism for not supporting a scrapping attempt, he clarified that resigning was not his plan. {summary end}\n [end of text]\n\n\n"
  },
  {
    "article_number": "3",
    "highlight": "Chris Brown song \"Not My Fault\" appeared online Tuesday .\nSong was recorded three years ago, has nothing to do with Rihanna, say producers .\nBrown agreed to a plea deal Monday in case involving ex-girlfriend Rihanna .",
    "generated_summary": "\": LOS ANGELES, California (CNN) -- A Chris Brown song called \"Not My Fault\" was recorded three years ago and is not about Rihanna, its producers said. Chris Brown pleaded guilty in an assault case regarding his ex-girlfriend Rihanna. The music appeared online Tuesday, a day after the singer admitted guilt to assaulting Rihanna, his former girlfriend. Brown's record label also said the music was not \"new.\" Although Brown accepted a plea deal in which he could avoid jail time, any appearance that he is not remorseful could be damaging, since the judge is not bound by those terms when she sentences him in August. The song, which appeared on YouTube, includes the line \"It's not my fault, right?\" The publicist for the Neptunes, a music producing duo that has worked in the studio with Brown, said the music was a demo recorded three years ago. She said they do not know how the song made its way onto YouTube. Jive Records, which distributes Brown's music, said \"there are no 'new' songs from Chris Brown currently being promoted by Chris or his record label.\" \"There are several old demos circulating that are being falsely promoted as new material from Chris,\" a Jive Records statement said. In the song, Brown sings: . \"I picked up the paper and the headline reads, it says 'singer brokenhearted in some satin sheets.' And I'm like, why? We had a hell of a time.\" \"When I first met you, I told ya that you'll be safe. In the midst of arguing something, falls to the floor and breaks. You look down and see what you dropped, oh, it's your heart.\" \"Shortie's caught up from a long night. She's ready to fall but she's all right. That's just something that can happen when you put it down. She's caught up, it's not my fault, right.\" \"You see, her mama called me, really steamed. She said 'My daughter won't eat and she don't sleep. She just cries.' \" Brown was arrested in connection with an altercation that began with an argument with Rihanna inside a rented Lamborghini on a Hollywood street in February. Prosecutors agreed to recommend five years of probation and 180 days of community service for Brown in exchange for his guilty plea on one count of assault with the intent of doing great bodily injury.\" {task end}\" [summary begin] Chris Brown's song \"Not My Fault,\" which appeared online after he pleaded guilty to assaulting Rihanna, is actually a three-year-old demo that has nothing to do with the incident. The song’s producers clarified that it wasn't recorded recently and was mistakenly promoted as new material. Brown accepted a plea deal to avoid jail time, but his actions could still impact his sentencing in August. The song includes lyrics referencing a past argument and a broken heart. {summary end} [end of text]",
    "article_text": "LOS ANGELES, California (CNN) -- A Chris Brown song called \"Not My Fault\" was recorded three years ago and is not about Rihanna, its producers said. Chris Brown pleaded guilty in an assault case regarding his ex-girlfriend Rihanna. The music appeared online Tuesday, a day after the singer admitted guilt to assaulting Rihanna, his former girlfriend. Brown's record label also said the music was not \"new.\" Although Brown accepted a plea deal in which he could avoid jail time, any appearance that he is not remorseful could be damaging, since the judge is not bound by those terms when she sentences him in August. The song, which appeared on YouTube, includes the line \"It's not my fault, right?\" The publicist for the Neptunes, a music producing duo that has worked in the studio with Brown, said the music was a demo recorded three years ago. She said they do not know how the song made its way onto YouTube. Jive Records, which distributes Brown's music, said \"there are no 'new' songs from Chris Brown currently being promoted by Chris or his record label.\" \"There are several old demos circulating that are being falsely promoted as new material from Chris,\" a Jive Records statement said. In the song, Brown sings: . \"I picked up the paper and the headline reads, it says 'singer brokenhearted in some satin sheets.' And I'm like, why? We had a hell of a time.\" \"When I first met you, I told ya that you'll be safe. In the midst of arguing something, falls to the floor and breaks. You look down and see what you dropped, oh, it's your heart.\" \"Shortie's caught up from a long night. She's ready to fall but she's all right. That's just something that can happen when you put it down. She's caught up, it's not my fault, right.\" \"You see, her mama called me, really steamed. She said 'My daughter won't eat and she don't sleep. She just cries.' \" Brown was arrested in connection with an altercation that began with an argument with Rihanna inside a rented Lamborghini on a Hollywood street in February. Prosecutors agreed to recommend five years of probation and 180 days of community service for Brown in exchange for his guilty plea on one count of assault with the intent of doing great bodily injury.",
    "generation_logs": "+ adb shell '   cd /data/local/tmp/llama.cpp; ulimit -c unlimited;   LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib   ADSP_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib              ././bin/llama-cli --no-mmap       -m /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf       --poll 0 -t 6 --cpu-mask 0xfc --cpu-strict 1       --ctx-size 8192 --batch-size 128       -ctk q8_0 -ctv q8_0 -fa on       -ngl 99 --device HTP0         \"-no-cnv\" \"-n\" \"400\" \"-p\" \"\\\"{task begin}Summarize this article briefly in 3-5 sentences, like \\\"X did this, Y did that\\\", and wrap your summary with [summary begin] and [summary end] tags{task end}: LOS ANGELES, California (CNN) -- A Chris Brown song called \\\"Not My Fault\\\" was recorded three years ago and is not about Rihanna, its producers said. Chris Brown pleaded guilty in an assault case regarding his ex-girlfriend Rihanna. The music appeared online Tuesday, a day after the singer admitted guilt to assaulting Rihanna, his former girlfriend. Brown'\\''s record label also said the music was not \\\"new.\\\" Although Brown accepted a plea deal in which he could avoid jail time, any appearance that he is not remorseful could be damaging, since the judge is not bound by those terms when she sentences him in August. The song, which appeared on YouTube, includes the line \\\"It'\\''s not my fault, right?\\\" The publicist for the Neptunes, a music producing duo that has worked in the studio with Brown, said the music was a demo recorded three years ago. She said they do not know how the song made its way onto YouTube. Jive Records, which distributes Brown'\\''s music, said \\\"there are no '\\''new'\\'' songs from Chris Brown currently being promoted by Chris or his record label.\\\" \\\"There are several old demos circulating that are being falsely promoted as new material from Chris,\\\" a Jive Records statement said. In the song, Brown sings: . \\\"I picked up the paper and the headline reads, it says '\\''singer brokenhearted in some satin sheets.'\\'' And I'\\''m like, why? We had a hell of a time.\\\" \\\"When I first met you, I told ya that you'\\''ll be safe. In the midst of arguing something, falls to the floor and breaks. You look down and see what you dropped, oh, it'\\''s your heart.\\\" \\\"Shortie'\\''s caught up from a long night. She'\\''s ready to fall but she'\\''s all right. That'\\''s just something that can happen when you put it down. She'\\''s caught up, it'\\''s not my fault, right.\\\" \\\"You see, her mama called me, really steamed. She said '\\''My daughter won'\\''t eat and she don'\\''t sleep. She just cries.'\\'' \\\" Brown was arrested in connection with an altercation that began with an argument with Rihanna inside a rented Lamborghini on a Hollywood street in February. Prosecutors agreed to recommend five years of probation and 180 days of community service for Brown in exchange for his guilty plea on one count of assault with the intent of doing great bodily injury.\\\"\" '\nggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\n\nggml_opencl: device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: commit unknown Compiler E031.45.02.16\nggml_opencl: vector subgroup broadcast support: false\nggml_opencl: device FP16 support: true\nggml_opencl: mem base addr align: 128\nggml_opencl: max mem alloc size: 1024 MB\nggml_opencl: device max workgroup size: 1024\nggml_opencl: SVM coarse grain buffer support: true\nggml_opencl: SVM fine grain buffer support: true\nggml_opencl: SVM fine grain system support: false\nggml_opencl: SVM atomics support: true\nggml_opencl: flattening quantized weights representation as struct of arrays (GGML_OPENCL_SOA_Q)\nggml_opencl: using kernels optimized for Adreno (GGML_OPENCL_USE_ADRENO_KERNELS)\nggml_opencl: loading OpenCL kernels........................................................................\nggml_opencl: default device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml-hex: Hexagon backend (experimental) : allocating new registry : ndev 1\nggml-hex: Hexagon Arch version v75\nggml-hex: allocating new session: HTP0\nggml-hex: new session: HTP0 : session-id 0 domain-id 3 uri file:///libggml-htp-v75.so?htp_iface_skel_handle_invoke&_modver=1.0&_dom=cdsp&_session=0 handle 0xb40000700a345e10\nbuild: 6900 (c22473b58) with Android (13324770, +pgo, +bolt, +lto, +mlgo, based on r530567d) clang version 19.0.0 (https://android.googlesource.com/toolchain/llvm-project 97a699bf4812a18fb657c2779f5296a4ab2694d2) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device HTP0 (Hexagon) (unknown id) - 2048 MiB free\nllama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - kv  23:                          general.file_type u32              = 2\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\nllama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\nllama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\nllama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\nllama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\nllama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\nllama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\nllama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\nllama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\nllama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\nllama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\nllama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type  f16:    1 tensors\nllama_model_loader: - type q4_0:  238 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_0\nprint_info: file size   = 2.93 GiB (6.49 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 8\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = n/a\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 93\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 34 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 35/35 layers to GPU\nload_tensors:          CPU model buffer size =  1280.00 MiB\nload_tensors:         HTP0 model buffer size =     1.40 MiB\nload_tensors:  HTP0-REPACK model buffer size =  1721.25 MiB\n...........................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 128\nllama_context: n_ubatch      = 128\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = enabled\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 8192 cells\nllama_kv_cache:       HTP0 KV buffer size =    85.00 MiB\nllama_kv_cache: size =   85.00 MiB (  8192 cells,   5 layers,  1/1 seqs), K (q8_0):   42.50 MiB, V (q8_0):   42.50 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1152 cells\nllama_kv_cache:       HTP0 KV buffer size =    69.33 MiB\nllama_kv_cache: size =   69.33 MiB (  1152 cells,  29 layers,  1/1 seqs), K (q8_0):   34.66 MiB, V (q8_0):   34.66 MiB\nllama_context:       HTP0 compute buffer size =    13.50 MiB\nllama_context:        CPU compute buffer size =   128.00 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 279\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 6\n\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 4221691895\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 128, n_predict = 400, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =      13.98 ms /   653 runs   (    0.02 ms per token, 46716.27 tokens per second)\nllama_perf_context_print:        load time =    3337.41 ms\nllama_perf_context_print: prompt eval time =   28231.61 ms /   543 tokens (   51.99 ms per token,    19.23 tokens per second)\nllama_perf_context_print:        eval time =   21417.36 ms /   109 runs   (  196.49 ms per token,     5.09 tokens per second)\nllama_perf_context_print:       total time =   49722.38 ms /   652 tokens\nllama_perf_context_print:    graphs reused =        110\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute       unaccounted |\nllama_memory_breakdown_print: |   - HTP0 (Hexagon)     |  2048 = 2048 + (1721 =  1721 +       0 +       0) + 17592186042694 |\nllama_memory_breakdown_print: |   - Host               |                 1577 =  1281 +     154 +     141                   |",
    "raw_model_output": " \"{task begin}Summarize this article briefly in 3-5 sentences, like \"X did this, Y did that\", and wrap your summary with [summary begin] and [summary end] tags{task end}: LOS ANGELES, California (CNN) -- A Chris Brown song called \"Not My Fault\" was recorded three years ago and is not about Rihanna, its producers said. Chris Brown pleaded guilty in an assault case regarding his ex-girlfriend Rihanna. The music appeared online Tuesday, a day after the singer admitted guilt to assaulting Rihanna, his former girlfriend. Brown's record label also said the music was not \"new.\" Although Brown accepted a plea deal in which he could avoid jail time, any appearance that he is not remorseful could be damaging, since the judge is not bound by those terms when she sentences him in August. The song, which appeared on YouTube, includes the line \"It's not my fault, right?\" The publicist for the Neptunes, a music producing duo that has worked in the studio with Brown, said the music was a demo recorded three years ago. She said they do not know how the song made its way onto YouTube. Jive Records, which distributes Brown's music, said \"there are no 'new' songs from Chris Brown currently being promoted by Chris or his record label.\" \"There are several old demos circulating that are being falsely promoted as new material from Chris,\" a Jive Records statement said. In the song, Brown sings: . \"I picked up the paper and the headline reads, it says 'singer brokenhearted in some satin sheets.' And I'm like, why? We had a hell of a time.\" \"When I first met you, I told ya that you'll be safe. In the midst of arguing something, falls to the floor and breaks. You look down and see what you dropped, oh, it's your heart.\" \"Shortie's caught up from a long night. She's ready to fall but she's all right. That's just something that can happen when you put it down. She's caught up, it's not my fault, right.\" \"You see, her mama called me, really steamed. She said 'My daughter won't eat and she don't sleep. She just cries.' \" Brown was arrested in connection with an altercation that began with an argument with Rihanna inside a rented Lamborghini on a Hollywood street in February. Prosecutors agreed to recommend five years of probation and 180 days of community service for Brown in exchange for his guilty plea on one count of assault with the intent of doing great bodily injury.\" {task end}\"\n[summary begin]\nChris Brown's song \"Not My Fault,\" which appeared online after he pleaded guilty to assaulting Rihanna, is actually a three-year-old demo that has nothing to do with the incident. The song’s producers clarified that it wasn't recorded recently and was mistakenly promoted as new material. Brown accepted a plea deal to avoid jail time, but his actions could still impact his sentencing in August. The song includes lyrics referencing a past argument and a broken heart. {summary end}\n [end of text]\n\n\n"
  },
  {
    "article_number": "4",
    "highlight": "Obama says budget calls for $25 billion increase in VA funding over next five years .\nHomeless veterans will be targeted for support, Obama says .\nDramatically improved services planned for mental health, PTSD, brain injury .",
    "generated_summary": "President Obama pledged to transform the Department of Veterans Affairs and increase funding by $5 billion over the next five years. This expansion will serve an additional 500,000 veterans by 2013. A key focus will be dramatically improving mental health services, particularly for PTSD and traumatic brain injury. The administration also aims to reduce homelessness among veterans, ensuring they have a \"home\" in the United States.",
    "article_text": "WASHINGTON (CNN) -- President Obama pledged Monday to make good on his promise to transform the Department of Veterans Affairs and said he would \"dramatically improve\" mental health aid. President Obama and Veterans Affairs Secretary Eric Shinseki, seen here last month, vow to increase aid. Flanked by Veterans Affairs Secretary Eric Shinseki, the president said his budget calls for a $25 billion increase in funding for the VA over the next five years -- a commitment that will be tested by the needs of veterans returning from Iraq and Afghanistan. \"With this budget, we don't just fully fund our Veterans Affairs health care program, we expand it to serve an additional 500,000 veterans by 2013,\" he said. He promised that the VA would \"dramatically improve services\" related to mental health, post-traumatic stress disorder and traumatic brain injury, and he said homeless veterans would be targeted for support. \"Those heroes have a home,\" Obama said. \"It's the country they served, the United States of America, and until we reach a day when not a single veteran sleeps on our nation's streets, our work remains unfinished.\"",
    "generation_logs": "+ adb shell '   cd /data/local/tmp/llama.cpp; ulimit -c unlimited;   LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib   ADSP_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib              ././bin/llama-cli --no-mmap       -m /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf       --poll 0 -t 6 --cpu-mask 0xfc --cpu-strict 1       --ctx-size 8192 --batch-size 128       -ctk q8_0 -ctv q8_0 -fa on       -ngl 99 --device HTP0         \"-no-cnv\" \"-n\" \"400\" \"-p\" \"\\\"{task begin}Summarize this article briefly in 3-5 sentences, like \\\"X did this, Y did that\\\", and wrap your summary with [summary begin] and [summary end] tags{task end}: WASHINGTON (CNN) -- President Obama pledged Monday to make good on his promise to transform the Department of Veterans Affairs and said he would \\\"dramatically improve\\\" mental health aid. President Obama and Veterans Affairs Secretary Eric Shinseki, seen here last month, vow to increase aid. Flanked by Veterans Affairs Secretary Eric Shinseki, the president said his budget calls for a $25 billion increase in funding for the VA over the next five years -- a commitment that will be tested by the needs of veterans returning from Iraq and Afghanistan. \\\"With this budget, we don'\\''t just fully fund our Veterans Affairs health care program, we expand it to serve an additional 500,000 veterans by 2013,\\\" he said. He promised that the VA would \\\"dramatically improve services\\\" related to mental health, post-traumatic stress disorder and traumatic brain injury, and he said homeless veterans would be targeted for support. \\\"Those heroes have a home,\\\" Obama said. \\\"It'\\''s the country they served, the United States of America, and until we reach a day when not a single veteran sleeps on our nation'\\''s streets, our work remains unfinished.\\\"\\\"\" '\nggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\n\nggml_opencl: device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: commit unknown Compiler E031.45.02.16\nggml_opencl: vector subgroup broadcast support: false\nggml_opencl: device FP16 support: true\nggml_opencl: mem base addr align: 128\nggml_opencl: max mem alloc size: 1024 MB\nggml_opencl: device max workgroup size: 1024\nggml_opencl: SVM coarse grain buffer support: true\nggml_opencl: SVM fine grain buffer support: true\nggml_opencl: SVM fine grain system support: false\nggml_opencl: SVM atomics support: true\nggml_opencl: flattening quantized weights representation as struct of arrays (GGML_OPENCL_SOA_Q)\nggml_opencl: using kernels optimized for Adreno (GGML_OPENCL_USE_ADRENO_KERNELS)\nggml_opencl: loading OpenCL kernels........................................................................\nggml_opencl: default device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml-hex: Hexagon backend (experimental) : allocating new registry : ndev 1\nggml-hex: Hexagon Arch version v75\nggml-hex: allocating new session: HTP0\nggml-hex: new session: HTP0 : session-id 0 domain-id 3 uri file:///libggml-htp-v75.so?htp_iface_skel_handle_invoke&_modver=1.0&_dom=cdsp&_session=0 handle 0xb40000774c73d010\nbuild: 6900 (c22473b58) with Android (13324770, +pgo, +bolt, +lto, +mlgo, based on r530567d) clang version 19.0.0 (https://android.googlesource.com/toolchain/llvm-project 97a699bf4812a18fb657c2779f5296a4ab2694d2) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device HTP0 (Hexagon) (unknown id) - 2048 MiB free\nllama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - kv  23:                          general.file_type u32              = 2\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\nllama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\nllama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\nllama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\nllama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\nllama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\nllama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\nllama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\nllama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\nllama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\nllama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\nllama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type  f16:    1 tensors\nllama_model_loader: - type q4_0:  238 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_0\nprint_info: file size   = 2.93 GiB (6.49 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 8\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = n/a\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 93\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 34 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 35/35 layers to GPU\nload_tensors:          CPU model buffer size =  1280.00 MiB\nload_tensors:         HTP0 model buffer size =     1.40 MiB\nload_tensors:  HTP0-REPACK model buffer size =  1721.25 MiB\n...........................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 128\nllama_context: n_ubatch      = 128\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = enabled\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 8192 cells\nllama_kv_cache:       HTP0 KV buffer size =    85.00 MiB\nllama_kv_cache: size =   85.00 MiB (  8192 cells,   5 layers,  1/1 seqs), K (q8_0):   42.50 MiB, V (q8_0):   42.50 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1152 cells\nllama_kv_cache:       HTP0 KV buffer size =    69.33 MiB\nllama_kv_cache: size =   69.33 MiB (  1152 cells,  29 layers,  1/1 seqs), K (q8_0):   34.66 MiB, V (q8_0):   34.66 MiB\nllama_context:       HTP0 compute buffer size =    13.50 MiB\nllama_context:        CPU compute buffer size =   128.00 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 279\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 6\n\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 2697570654\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 128, n_predict = 400, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =      13.39 ms /   388 runs   (    0.03 ms per token, 28976.85 tokens per second)\nllama_perf_context_print:        load time =    3506.16 ms\nllama_perf_context_print: prompt eval time =   15259.32 ms /   288 tokens (   52.98 ms per token,    18.87 tokens per second)\nllama_perf_context_print:        eval time =   17718.47 ms /    99 runs   (  178.97 ms per token,     5.59 tokens per second)\nllama_perf_context_print:       total time =   33052.69 ms /   387 tokens\nllama_perf_context_print:    graphs reused =         99\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute       unaccounted |\nllama_memory_breakdown_print: |   - HTP0 (Hexagon)     |  2048 = 2048 + (1721 =  1721 +       0 +       0) + 17592186042694 |\nllama_memory_breakdown_print: |   - Host               |                 1577 =  1281 +     154 +     141                   |",
    "raw_model_output": " \"{task begin}Summarize this article briefly in 3-5 sentences, like \"X did this, Y did that\", and wrap your summary with [summary begin] and [summary end] tags{task end}: WASHINGTON (CNN) -- President Obama pledged Monday to make good on his promise to transform the Department of Veterans Affairs and said he would \"dramatically improve\" mental health aid. President Obama and Veterans Affairs Secretary Eric Shinseki, seen here last month, vow to increase aid. Flanked by Veterans Affairs Secretary Eric Shinseki, the president said his budget calls for a 5 billion increase in funding for the VA over the next five years -- a commitment that will be tested by the needs of veterans returning from Iraq and Afghanistan. \"With this budget, we don't just fully fund our Veterans Affairs health care program, we expand it to serve an additional 500,000 veterans by 2013,\" he said. He promised that the VA would \"dramatically improve services\" related to mental health, post-traumatic stress disorder and traumatic brain injury, and he said homeless veterans would be targeted for support. \"Those heroes have a home,\" Obama said. \"It's the country they served, the United States of America, and until we reach a day when not a single veteran sleeps on our nation's streets, our work remains unfinished.\"\"\n\n[summary begin]\nPresident Obama pledged to transform the Department of Veterans Affairs and increase funding by $5 billion over the next five years. This expansion will serve an additional 500,000 veterans by 2013. A key focus will be dramatically improving mental health services, particularly for PTSD and traumatic brain injury. The administration also aims to reduce homelessness among veterans, ensuring they have a \"home\" in the United States. [summary end]\n\"\n [end of text]\n\n\n"
  },
  {
    "article_number": "5",
    "highlight": "The women have wombs transplanted by their own mothers .\nThe world’s first womb swap baby, Vincent, has already been delivered .\nDoctors say the pregnancies are going well and the mothers are 'excited'\nThe news gives hope to Britain’s estimated 15,000 childless women .",
    "generated_summary": "Two women in Sweden are weeks away from giving birth, utilizing transplanted wombs donated by their mothers. This groundbreaking project, spearheaded by Dr. Liza Johannesson, aims to provide childless women with the opportunity to experience pregnancy within their own maternal wombs. Vincent, the world’s first womb transplant baby, was born two months prematurely, and his mother expressed immense joy upon holding him. This development is inspiring similar projects in the UK, where 60 women are currently waiting for a womb transplant. The procedure, likened to a revolutionary medical advancement, offers hope to women who may have been born without a womb or those who have suffered from complications during pregnancy. Furthermore, the success of these transplants paves the way for potential future applications, such as assisting women with repeated miscarriages.",
    "article_text": "Two women are just weeks away from giving birth – using transplanted wombs donated by their own mothers. The expectant mums will be the first to carry children using the very uterus that carried them as unborn infants. The pregnancies are part of a revolutionary Swedish project to allow childless women to fulfil their dream of starting a family. The world’s first womb swap baby, named as Vincent, has already been delivered using an unrelated donor. Now doctors are preparing for two more arrivals – but this time each one is being nurtured in its grandmother’s womb. Scroll down for video . World first: Vincent was nurtured in a donated womb.  It has now emerged that two women are just weeks away from giving birth – using transplanted wombs donated by their own mothers . The news gives hope to Britain’s estimated 15,000 childless women and provides a boost for a similar project in the UK in which 60 women are waiting for a new womb, Pictured, Vincent and his mother after his delivery . The extraordinary procedure, which doctors have likened in importance to the first successful heart transplant, means each womb will have carried two generations of the same family. Dr Liza Johannesson, of Sahlgrenska University Hospital in Gothenburg, said the pregnancies are going well and the mothers are ‘really excited’ at seeing their babies, which are due next month. ‘It is also really exciting to have your mother as a donor,’ she added. ‘It is a very nice gift to give to your daughter.’ The news gives hope to Britain’s estimated 15,000 childless women and provides a boost for a similar project in the UK in which 60 women are waiting for a new womb. Many women will have been born without the organ, others will have had a hysterectomy because of cancer or as a last-ditch attempt stop life-threatening bleeding during labour. Eventually, the technique could also be used on women who have suffered the agony of repeated miscarriages. Using a donated womb also means that expectant mothers can have babies that are genetically their own and experience the joys of pregnancy. Doctors in Turkey and Saudi Arabia have carried out womb transplants before – but none has so far led to a birth. Teams in the US, China and Australia are also keen to start their own programmes. The Swedes have carried out nine womb transplants – and seven of the women have had IVF treatment. Late on Friday night, it emerged that one had been successful – and the world’s first womb transplant baby was delivered. He has been called Vincent – a name derived from the Latin for ‘to conquer’ – to mark the extraordinary lengths his mother undertook to have him. Pernilla Dahm Kähler (left) has spent 15 years perfecting the complex surgical techniques involved in a womb transplant, said she felt ‘humble and proud’ to witness the birth. Consultant gynaecologist Dr Liza Johannesson (right), of Sahlgrenska University Hospital in Gothenburg, said the pregnancies are going well . (Left to right) Specialist surgeons Andreas G Tzakis, Pernilla Dahm-Kähler, Mats Brannstrom, Michael Olausson and Liza Johannesson attend a news conference, at Sahlgrenska hospital in Gothernburg, Sweden . Born by C-section two months prematurely, Vincent weighed just under 4lb and entered the world with a scream, bringing tears of joy to those who witnessed his arrival. His 36-year-old mother, who worked until the day before he was born, said: ‘As soon as I felt this perfect baby boy on my chest, I had tears of happiness and enormous relief. ‘I felt like a mother the first time I touched my baby and was amazed that we finally did it. ‘I have always had this large sorrow because I never thought I would be a mother – and now the impossible has become real.’ A keen athlete, she was devastated when she was told at the age of 15 that she did not have a womb. She was able to carry her own child after a 61-year-old friend offered to donate her womb. Despite being long past menopause, her womb worked normally during the pregnancy. Consultant gynaecologist Dr Johannesson described Vincent’s birth, which took place a month ago but has remained a secret until now, as ‘breathtaking’. ‘It was like having your own child. It was the same feeling – no one could really believe it.’ The doctor, who was pregnant with her third child at the same time as Vincent’s mother, said: ‘I was more involved in that pregnancy than in my own.’ Vincent is now back home with his parents, who have not been named. He is breastfeeding and growing well. Dr Johannesson added: ‘They are doing perfect. The parents told me that first night at home was horrible because he was screaming – like any other child.’ Vincent’s father said: ‘We now have the most amazing baby. He’s no different from any other child, but he will have a good story to tell.’ Mats Brannstrom and his team performing a womb transplant in Gothenburg, Sweden . The first British transplants could be carried out as soon as next summer – using wombs from dead donors. Richard Smith, head of the charity Womb Transplant UK, is ready to give five women the chance to have children using the procedure and is applying for ethical approval. Some 60 potential patients have already come forward. But Mr Smith, who has already spent significant amounts of his own money on years of research, only has £40,000 of the £500,000 needed for the surgery. Unlike the Swedish project in which living donors are used, the consultant gynaecological surgeon plans to use wombs from women who have died young. He said he is uncomfortable about removing a woman’s womb for an operation that is not essential to save a life. He claims taking the organ from a dead donor will allow him to harvest extra tissue and the major blood vessels needed to take the strain of pregnancy. One of his potential patients, Sophie Lewis, said: ‘To feel your own child growing inside you would be a miracle.’ The 30-year-old, who said she burst into tears of joy when she heard of Vincent’s birth in Sweden, added. ‘It would be an absolute gift, the most amazing gift ever.’ Mr Smith added: ‘In many women, there is a deep yearning to carry children and this is not fulfilled by surrogacy. ‘I’ve had my own crisis with this project over the years – are we doing the right thing? ‘But when you meet women in this position, I know in my heart of hearts that if we do it safely, it is the right thing.’ Henrik Hagberg, a professor in foetal medicine at King’s College London, who did ultrasound scans of Vincent throughout the pregnancy, said: ‘I was quite astounded that everything went so well. I think that’s quite fantastic.’ Pernilla Dahm Kähler, who along with Mats Brännström of Gothenburg University has spent 15 years perfecting the complex surgical techniques involved in a womb transplant, said she felt ‘humble and proud’ to witness the birth. Dr Dahm Kähler helped remove the donor womb from the 61-year-old woman – and transplant it in Vincent’s mother. Professor Brännström said the birth provides scientific evidence that the technique works, adding: ‘It gives us scientific evidence that the concept of uterus transplantation can be used to treat uterine factor infertility, which up to now has remained the last untreatable form of female infertility. ‘It also shows that transplants with a live donor are possible, including if the donor is past menopause.’ Dr Allan Pacey, chairman of the British Fertility Society, likened the operations to the first heart transplant. He added: ‘I think it is quite revolutionary. It feels like it did when IVF was developed or when the first heart transplant was done in the 1960s. ‘It is a bit of a game-changer. The question is can it be done repeatedly, reliably and safely.’ Dr Geeta Nargund, medical director of the Create fertility clinic in London, said: ‘This is a significant medical breakthrough and I congratulate the highly-skilled team behind it.’ However, she said the complexity of the surgery means it will be restricted to a few specialist hospitals.",
    "generation_logs": "+ adb shell '   cd /data/local/tmp/llama.cpp; ulimit -c unlimited;   LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib   ADSP_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib              ././bin/llama-cli --no-mmap       -m /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf       --poll 0 -t 6 --cpu-mask 0xfc --cpu-strict 1       --ctx-size 8192 --batch-size 128       -ctk q8_0 -ctv q8_0 -fa on       -ngl 99 --device HTP0         \"-no-cnv\" \"-n\" \"400\" \"-p\" \"\\\"{task begin}Summarize this article briefly in 3-5 sentences, like \\\"X did this, Y did that\\\", and wrap your summary with [summary begin] and [summary end] tags{task end}: Two women are just weeks away from giving birth – using transplanted wombs donated by their own mothers. The expectant mums will be the first to carry children using the very uterus that carried them as unborn infants. The pregnancies are part of a revolutionary Swedish project to allow childless women to fulfil their dream of starting a family. The world’s first womb swap baby, named as Vincent, has already been delivered using an unrelated donor. Now doctors are preparing for two more arrivals – but this time each one is being nurtured in its grandmother’s womb. Scroll down for video . World first: Vincent was nurtured in a donated womb.  It has now emerged that two women are just weeks away from giving birth – using transplanted wombs donated by their own mothers . The news gives hope to Britain’s estimated 15,000 childless women and provides a boost for a similar project in the UK in which 60 women are waiting for a new womb, Pictured, Vincent and his mother after his delivery . The extraordinary procedure, which doctors have likened in importance to the first successful heart transplant, means each womb will have carried two generations of the same family. Dr Liza Johannesson, of Sahlgrenska University Hospital in Gothenburg, said the pregnancies are going well and the mothers are ‘really excited’ at seeing their babies, which are due next month. ‘It is also really exciting to have your mother as a donor,’ she added. ‘It is a very nice gift to give to your daughter.’ The news gives hope to Britain’s estimated 15,000 childless women and provides a boost for a similar project in the UK in which 60 women are waiting for a new womb. Many women will have been born without the organ, others will have had a hysterectomy because of cancer or as a last-ditch attempt stop life-threatening bleeding during labour. Eventually, the technique could also be used on women who have suffered the agony of repeated miscarriages. Using a donated womb also means that expectant mothers can have babies that are genetically their own and experience the joys of pregnancy. Doctors in Turkey and Saudi Arabia have carried out womb transplants before – but none has so far led to a birth. Teams in the US, China and Australia are also keen to start their own programmes. The Swedes have carried out nine womb transplants – and seven of the women have had IVF treatment. Late on Friday night, it emerged that one had been successful – and the world’s first womb transplant baby was delivered. He has been called Vincent – a name derived from the Latin for ‘to conquer’ – to mark the extraordinary lengths his mother undertook to have him. Pernilla Dahm Kähler (left) has spent 15 years perfecting the complex surgical techniques involved in a womb transplant, said she felt ‘humble and proud’ to witness the birth. Consultant gynaecologist Dr Liza Johannesson (right), of Sahlgrenska University Hospital in Gothenburg, said the pregnancies are going well . (Left to right) Specialist surgeons Andreas G Tzakis, Pernilla Dahm-Kähler, Mats Brannstrom, Michael Olausson and Liza Johannesson attend a news conference, at Sahlgrenska hospital in Gothernburg, Sweden . Born by C-section two months prematurely, Vincent weighed just under 4lb and entered the world with a scream, bringing tears of joy to those who witnessed his arrival. His 36-year-old mother, who worked until the day before he was born, said: ‘As soon as I felt this perfect baby boy on my chest, I had tears of happiness and enormous relief. ‘I felt like a mother the first time I touched my baby and was amazed that we finally did it. ‘I have always had this large sorrow because I never thought I would be a mother – and now the impossible has become real.’ A keen athlete, she was devastated when she was told at the age of 15 that she did not have a womb. She was able to carry her own child after a 61-year-old friend offered to donate her womb. Despite being long past menopause, her womb worked normally during the pregnancy. Consultant gynaecologist Dr Johannesson described Vincent’s birth, which took place a month ago but has remained a secret until now, as ‘breathtaking’. ‘It was like having your own child. It was the same feeling – no one could really believe it.’ The doctor, who was pregnant with her third child at the same time as Vincent’s mother, said: ‘I was more involved in that pregnancy than in my own.’ Vincent is now back home with his parents, who have not been named. He is breastfeeding and growing well. Dr Johannesson added: ‘They are doing perfect. The parents told me that first night at home was horrible because he was screaming – like any other child.’ Vincent’s father said: ‘We now have the most amazing baby. He’s no different from any other child, but he will have a good story to tell.’ Mats Brannstrom and his team performing a womb transplant in Gothenburg, Sweden . The first British transplants could be carried out as soon as next summer – using wombs from dead donors. Richard Smith, head of the charity Womb Transplant UK, is ready to give five women the chance to have children using the procedure and is applying for ethical approval. Some 60 potential patients have already come forward. But Mr Smith, who has already spent significant amounts of his own money on years of research, only has £40,000 of the £500,000 needed for the surgery. Unlike the Swedish project in which living donors are used, the consultant gynaecological surgeon plans to use wombs from women who have died young. He said he is uncomfortable about removing a woman’s womb for an operation that is not essential to save a life. He claims taking the organ from a dead donor will allow him to harvest extra tissue and the major blood vessels needed to take the strain of pregnancy. One of his potential patients, Sophie Lewis, said: ‘To feel your own child growing inside you would be a miracle.’ The 30-year-old, who said she burst into tears of joy when she heard of Vincent’s birth in Sweden, added. ‘It would be an absolute gift, the most amazing gift ever.’ Mr Smith added: ‘In many women, there is a deep yearning to carry children and this is not fulfilled by surrogacy. ‘I’ve had my own crisis with this project over the years – are we doing the right thing? ‘But when you meet women in this position, I know in my heart of hearts that if we do it safely, it is the right thing.’ Henrik Hagberg, a professor in foetal medicine at King’s College London, who did ultrasound scans of Vincent throughout the pregnancy, said: ‘I was quite astounded that everything went so well. I think that’s quite fantastic.’ Pernilla Dahm Kähler, who along with Mats Brännström of Gothenburg University has spent 15 years perfecting the complex surgical techniques involved in a womb transplant, said she felt ‘humble and proud’ to witness the birth. Dr Dahm Kähler helped remove the donor womb from the 61-year-old woman – and transplant it in Vincent’s mother. Professor Brännström said the birth provides scientific evidence that the technique works, adding: ‘It gives us scientific evidence that the concept of uterus transplantation can be used to treat uterine factor infertility, which up to now has remained the last untreatable form of female infertility. ‘It also shows that transplants with a live donor are possible, including if the donor is past menopause.’ Dr Allan Pacey, chairman of the British Fertility Society, likened the operations to the first heart transplant. He added: ‘I think it is quite revolutionary. It feels like it did when IVF was developed or when the first heart transplant was done in the 1960s. ‘It is a bit of a game-changer. The question is can it be done repeatedly, reliably and safely.’ Dr Geeta Nargund, medical director of the Create fertility clinic in London, said: ‘This is a significant medical breakthrough and I congratulate the highly-skilled team behind it.’ However, she said the complexity of the surgery means it will be restricted to a few specialist hospitals.\\\"\" '\nggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\n\nggml_opencl: device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: commit unknown Compiler E031.45.02.16\nggml_opencl: vector subgroup broadcast support: false\nggml_opencl: device FP16 support: true\nggml_opencl: mem base addr align: 128\nggml_opencl: max mem alloc size: 1024 MB\nggml_opencl: device max workgroup size: 1024\nggml_opencl: SVM coarse grain buffer support: true\nggml_opencl: SVM fine grain buffer support: true\nggml_opencl: SVM fine grain system support: false\nggml_opencl: SVM atomics support: true\nggml_opencl: flattening quantized weights representation as struct of arrays (GGML_OPENCL_SOA_Q)\nggml_opencl: using kernels optimized for Adreno (GGML_OPENCL_USE_ADRENO_KERNELS)\nggml_opencl: loading OpenCL kernels........................................................................\nggml_opencl: default device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml-hex: Hexagon backend (experimental) : allocating new registry : ndev 1\nggml-hex: Hexagon Arch version v75\nggml-hex: allocating new session: HTP0\nggml-hex: new session: HTP0 : session-id 0 domain-id 3 uri file:///libggml-htp-v75.so?htp_iface_skel_handle_invoke&_modver=1.0&_dom=cdsp&_session=0 handle 0xb4000078ecf96e50\nbuild: 6900 (c22473b58) with Android (13324770, +pgo, +bolt, +lto, +mlgo, based on r530567d) clang version 19.0.0 (https://android.googlesource.com/toolchain/llvm-project 97a699bf4812a18fb657c2779f5296a4ab2694d2) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device HTP0 (Hexagon) (unknown id) - 2048 MiB free\nllama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - kv  23:                          general.file_type u32              = 2\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\nllama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\nllama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\nllama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\nllama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\nllama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\nllama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\nllama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\nllama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\nllama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\nllama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\nllama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type  f16:    1 tensors\nllama_model_loader: - type q4_0:  238 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_0\nprint_info: file size   = 2.93 GiB (6.49 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 8\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = n/a\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 93\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 34 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 35/35 layers to GPU\nload_tensors:          CPU model buffer size =  1280.00 MiB\nload_tensors:         HTP0 model buffer size =     1.40 MiB\nload_tensors:  HTP0-REPACK model buffer size =  1721.25 MiB\n...........................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 128\nllama_context: n_ubatch      = 128\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = enabled\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 8192 cells\nllama_kv_cache:       HTP0 KV buffer size =    85.00 MiB\nllama_kv_cache: size =   85.00 MiB (  8192 cells,   5 layers,  1/1 seqs), K (q8_0):   42.50 MiB, V (q8_0):   42.50 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1152 cells\nllama_kv_cache:       HTP0 KV buffer size =    69.33 MiB\nllama_kv_cache: size =   69.33 MiB (  1152 cells,  29 layers,  1/1 seqs), K (q8_0):   34.66 MiB, V (q8_0):   34.66 MiB\nllama_context:       HTP0 compute buffer size =    13.50 MiB\nllama_context:        CPU compute buffer size =   128.00 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 279\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 6\n\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 2516807978\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 128, n_predict = 400, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =      23.96 ms /  1954 runs   (    0.01 ms per token, 81549.18 tokens per second)\nllama_perf_context_print:        load time =    2793.84 ms\nllama_perf_context_print: prompt eval time =   90175.98 ms /  1784 tokens (   50.55 ms per token,    19.78 tokens per second)\nllama_perf_context_print:        eval time =   30456.46 ms /   169 runs   (  180.22 ms per token,     5.55 tokens per second)\nllama_perf_context_print:       total time =  120729.96 ms /  1953 tokens\nllama_perf_context_print:    graphs reused =        173\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute       unaccounted |\nllama_memory_breakdown_print: |   - HTP0 (Hexagon)     |  2048 = 2048 + (1721 =  1721 +       0 +       0) + 17592186042694 |\nllama_memory_breakdown_print: |   - Host               |                 1577 =  1281 +     154 +     141                   |",
    "raw_model_output": " \"{task begin}Summarize this article briefly in 3-5 sentences, like \"X did this, Y did that\", and wrap your summary with [summary begin] and [summary end] tags{task end}: Two women are just weeks away from giving birth – using transplanted wombs donated by their own mothers. The expectant mums will be the first to carry children using the very uterus that carried them as unborn infants. The pregnancies are part of a revolutionary Swedish project to allow childless women to fulfil their dream of starting a family. The world’s first womb swap baby, named as Vincent, has already been delivered using an unrelated donor. Now doctors are preparing for two more arrivals – but this time each one is being nurtured in its grandmother’s womb. Scroll down for video . World first: Vincent was nurtured in a donated womb.  It has now emerged that two women are just weeks away from giving birth – using transplanted wombs donated by their own mothers . The news gives hope to Britain’s estimated 15,000 childless women and provides a boost for a similar project in the UK in which 60 women are waiting for a new womb, Pictured, Vincent and his mother after his delivery . The extraordinary procedure, which doctors have likened in importance to the first successful heart transplant, means each womb will have carried two generations of the same family. Dr Liza Johannesson, of Sahlgrenska University Hospital in Gothenburg, said the pregnancies are going well and the mothers are ‘really excited’ at seeing their babies, which are due next month. ‘It is also really exciting to have your mother as a donor,’ she added. ‘It is a very nice gift to give to your daughter.’ The news gives hope to Britain’s estimated 15,000 childless women and provides a boost for a similar project in the UK in which 60 women are waiting for a new womb. Many women will have been born without the organ, others will have had a hysterectomy because of cancer or as a last-ditch attempt stop life-threatening bleeding during labour. Eventually, the technique could also be used on women who have suffered the agony of repeated miscarriages. Using a donated womb also means that expectant mothers can have babies that are genetically their own and experience the joys of pregnancy. Doctors in Turkey and Saudi Arabia have carried out womb transplants before – but none has so far led to a birth. Teams in the US, China and Australia are also keen to start their own programmes. The Swedes have carried out nine womb transplants – and seven of the women have had IVF treatment. Late on Friday night, it emerged that one had been successful – and the world’s first womb transplant baby was delivered. He has been called Vincent – a name derived from the Latin for ‘to conquer’ – to mark the extraordinary lengths his mother undertook to have him. Pernilla Dahm Kähler (left) has spent 15 years perfecting the complex surgical techniques involved in a womb transplant, said she felt ‘humble and proud’ to witness the birth. Consultant gynaecologist Dr Liza Johannesson (right), of Sahlgrenska University Hospital in Gothenburg, said the pregnancies are going well . (Left to right) Specialist surgeons Andreas G Tzakis, Pernilla Dahm-Kähler, Mats Brannstrom, Michael Olausson and Liza Johannesson attend a news conference, at Sahlgrenska hospital in Gothernburg, Sweden . Born by C-section two months prematurely, Vincent weighed just under 4lb and entered the world with a scream, bringing tears of joy to those who witnessed his arrival. His 36-year-old mother, who worked until the day before he was born, said: ‘As soon as I felt this perfect baby boy on my chest, I had tears of happiness and enormous relief. ‘I felt like a mother the first time I touched my baby and was amazed that we finally did it. ‘I have always had this large sorrow because I never thought I would be a mother – and now the impossible has become real.’ A keen athlete, she was devastated when she was told at the age of 15 that she did not have a womb. She was able to carry her own child after a 61-year-old friend offered to donate her womb. Despite being long past menopause, her womb worked normally during the pregnancy. Consultant gynaecologist Dr Johannesson described Vincent’s birth, which took place a month ago but has remained a secret until now, as ‘breathtaking’. ‘It was like having your own child. It was the same feeling – no one could really believe it.’ The doctor, who was pregnant with her third child at the same time as Vincent’s mother, said: ‘I was more involved in that pregnancy than in my own.’ Vincent is now back home with his parents, who have not been named. He is breastfeeding and growing well. Dr Johannesson added: ‘They are doing perfect. The parents told me that first night at home was horrible because he was screaming – like any other child.’ Vincent’s father said: ‘We now have the most amazing baby. He’s no different from any other child, but he will have a good story to tell.’ Mats Brannstrom and his team performing a womb transplant in Gothenburg, Sweden . The first British transplants could be carried out as soon as next summer – using wombs from dead donors. Richard Smith, head of the charity Womb Transplant UK, is ready to give five women the chance to have children using the procedure and is applying for ethical approval. Some 60 potential patients have already come forward. But Mr Smith, who has already spent significant amounts of his own money on years of research, only has £40,000 of the £500,000 needed for the surgery. Unlike the Swedish project in which living donors are used, the consultant gynaecological surgeon plans to use wombs from women who have died young. He said he is uncomfortable about removing a woman’s womb for an operation that is not essential to save a life. He claims taking the organ from a dead donor will allow him to harvest extra tissue and the major blood vessels needed to take the strain of pregnancy. One of his potential patients, Sophie Lewis, said: ‘To feel your own child growing inside you would be a miracle.’ The 30-year-old, who said she burst into tears of joy when she heard of Vincent’s birth in Sweden, added. ‘It would be an absolute gift, the most amazing gift ever.’ Mr Smith added: ‘In many women, there is a deep yearning to carry children and this is not fulfilled by surrogacy. ‘I’ve had my own crisis with this project over the years – are we doing the right thing? ‘But when you meet women in this position, I know in my heart of hearts that if we do it safely, it is the right thing.’ Henrik Hagberg, a professor in foetal medicine at King’s College London, who did ultrasound scans of Vincent throughout the pregnancy, said: ‘I was quite astounded that everything went so well. I think that’s quite fantastic.’ Pernilla Dahm Kähler, who along with Mats Brännström of Gothenburg University has spent 15 years perfecting the complex surgical techniques involved in a womb transplant, said she felt ‘humble and proud’ to witness the birth. Dr Dahm Kähler helped remove the donor womb from the 61-year-old woman – and transplant it in Vincent’s mother. Professor Brännström said the birth provides scientific evidence that the technique works, adding: ‘It gives us scientific evidence that the concept of uterus transplantation can be used to treat uterine factor infertility, which up to now has remained the last untreatable form of female infertility. ‘It also shows that transplants with a live donor are possible, including if the donor is past menopause.’ Dr Allan Pacey, chairman of the British Fertility Society, likened the operations to the first heart transplant. He added: ‘I think it is quite revolutionary. It feels like it did when IVF was developed or when the first heart transplant was done in the 1960s. ‘It is a bit of a game-changer. The question is can it be done repeatedly, reliably and safely.’ Dr Geeta Nargund, medical director of the Create fertility clinic in London, said: ‘This is a significant medical breakthrough and I congratulate the highly-skilled team behind it.’ However, she said the complexity of the surgery means it will be restricted to a few specialist hospitals.\"\n\n[summary begin]\nTwo women in Sweden are weeks away from giving birth, utilizing transplanted wombs donated by their mothers. This groundbreaking project, spearheaded by Dr. Liza Johannesson, aims to provide childless women with the opportunity to experience pregnancy within their own maternal wombs. Vincent, the world’s first womb transplant baby, was born two months prematurely, and his mother expressed immense joy upon holding him. This development is inspiring similar projects in the UK, where 60 women are currently waiting for a womb transplant. The procedure, likened to a revolutionary medical advancement, offers hope to women who may have been born without a womb or those who have suffered from complications during pregnancy. Furthermore, the success of these transplants paves the way for potential future applications, such as assisting women with repeated miscarriages. [summary end]\n [end of text]\n\n\n"
  },
  {
    "article_number": "6",
    "highlight": "Accident investigators respond to critics who claim missile fire brought down the jet in 1996 .\nThe National Transportation Safety Board blamed the crash on a fuel tank explosion .\nSkeptics have asked for a new investigation and are releasing a documentary on the crash .\nTWA 800 crashed into the Atlantic after taking off from New York on a flight to Paris .",
    "generated_summary": "\": More than a decade after concluding a fuel tank explosion destroyed TWA Flight 800, senior U.S. accident investigators stood their ground on Tuesday against skeptics claiming new evidence suggests missile fire downed the jumbo jet. \"I'm totally convinced there was no bomb or missile,\" National Transportation Safety Board investigator Jim Wildey told reporters at a briefing outside Washington. Skeptics, including a former NTSB investigator, last month petitioned the agency to re-open its investigation and will release a documentary on the crash this month. Conspiracy theories, outrage swirl around TWA 800 plane . They said evidence suggests one or more missiles destroyed the Paris-bound Boeing 747 over the Atlantic shortly after takeoff from New York on July 17, 1996, killing all 230 on board. The safety board has responded cautiously, saying it is having investigators who were not part of the original investigation review the petition. And it is seeking comment from Boeing, the Federal Aviation Administration, and other parties to the original probe. But at the same time, the board allowed the original TWA 800 investigators to brief the media on the four-year investigation. Under orders to address only matters in the original investigation, the investigators nonetheless emphasized topics that have come under question by the critics. They held up two sheet metal plates showing examples of bomb and high-velocity missile damage, and then took reporters to view TWA 800's reconstructed fuselage. They said they found no pitting of the type alleged by those challenging the findings. \"You don't see fragmented seat parts,\" said investigator Robert Swaim. Nor is there evidence of pitting, cratering or \"gas jetting\" -- tell-tale marks left by high explosives -- he said. Plane reconstructed . Investigators said they were unable to find any evidence on radar of a missile intercepting the plane. And they discounted the significance of traces of explosives found on more than 100 airplane parts pulled from the ocean. The NTSB said tests showed that explosive residue washes off after only two days in the ocean, leading them to conclude the residue likely was the result of contamination from the military and law enforcement boats that recovered wreckage. The NTSB allowed two TWA 800 family members to attend the briefing, and the family members reiterated their support for the board's findings. Matt Ziemkiewicz, whose sister Jill, a 23-year-old flight attendant, was killed in the explosion, said the briefing \"solidified\" his belief that the original investigation was \"complete, accurate and thorough.\" Jim Hurd, whose son Jamie, 29, died on the flight, also endorsed the NTSB finding. \"At first I was of course skeptical as everybody else. You know, you just didn't know what happened. As things unfolded, it seemed to make a lot of sense, what happened to the plane,\" he said. \"It's really cut and dry, at this point, as far as I'm concerned.\" Ziemkiewicz and Hurd said the new attention to the crash is opening old wounds. \"It's really a shame in my book,\" Hurd said. \"There are still family members who believe it was brought down by a bomb and I respect their thoughts and however they want to view it. It's just the way you take the information and how you process it and who you believe. And I believe the NTSB has done a job and I don't believe they've covered up anything and I don't believe the FBI did either.\" The NTSB did not allow their critics into the media briefing. They talked to reporters in a parking lot outside. Opinion: Flight 800 conspiracy? Where's the proof? 'A matter of conscience' Hank Hughes, a former NTSB investigator who now doubts the board's conclusion, said he is motivated only by a desire to uncover the truth. Hughes said following the accident, he spent two days with Hurd. \"That's the last guy in the world I'd want to hurt,\" he said. \"There's no profit motive. It's a matter of conscience and integrity as far as we're concerned. Two hundred and thirty people were killed and that's a tragedy that can't be ignored,\" Hughes said. Hughes is featured in the documentary, called \"TWA Flight 800,\" co-produced by longtime skeptic Tom Stalcup. The NTSB investigators on Tuesday repeated a scenario the safety board endorsed in 2000. The series of events leading to TWA 800's disastrous final flight began when it was delayed at New York's John F. Kennedy airport. While the plane was sitting at the gate, the crew engaged the plane's air conditioner to keep passengers cool. The air conditioning units, low in the plane's fuselage, heated the large, nearly empty fuel tank above it, creating a volatile fuel/air mixture. Ten minutes after take-off, when the plane was at 13,700 feet and climbing, an electrical short that likely occurred in a wire bundle pushed a high-voltage current into the fuel tank, igniting it, and destroying the plane. Divers found three overlapping fields of debris at the bottom of the ocean. The first included the pieces of the fuel tank and the cabin above it. A second field contained the nose and cockpit. The final and largest debris field contained the rest of the plane. Investigators said they worked hard to determine if a bomb or missile was involved, especially in light of the event's timing two days before the Summer Olympics and 10 days before the Olympic bombing in Atlanta. \"That really intensified the investigation,\" said investigator Joseph Kolly. A cockpit voice recorder analysis showed a sound spectrum consistent with a low-explosive fuel tank explosion, not a high-explosive missile. Eventually, investigators ruled out a missile or bomb, as well as lightning, meteor, electro-magnetic pulse, and other possible causes, Kolly said. Kolly acknowledged some friction between the NTSB and the FBI during the investigation, but said \"it didn't ultimately harm the investigation.\" Kolly said the petition has not prompted him to rethink the board's 2000 findings. Since the petition was filed, Kolly said, he has re-read this report \"a couple more times and it's pretty thorough.\" Five facts to calm plane crash fears .\"{summary begin}More than a decade after TWA Flight 800’s crash, the National Transportation Safety Board (NTSB) defended its original findings, asserting there was no evidence of a bomb or missile. Investigators highlighted the absence of key evidence—such as bomb damage, fragmented seats, and pitting from high explosives—and emphasized that radar data didn’t show any intercepting missiles. They dismissed traces of explosive residue as likely contamination from recovery efforts. The NTSB also presented a timeline of events, including the plane's delay at JFK Airport, which led to a volatile fuel mixture in the tank, and an electrical short that triggered the explosion. Family members of those who perished reiterated their support for the board's conclusions. While skepticism remains, investigators maintained that their investigation was thorough and accurate. {summary end}\"\"\"More than a decade after TWA Flight 800’s crash, the National Transportation Safety Board (NTSB) defended its original findings, asserting there was no evidence of a bomb or missile. Investigators highlighted the absence of key evidence—such as bomb damage, fragmented seats, and pitting from high explosives—and emphasized that radar data didn’t show any intercepting missiles. They dismissed traces of explosive residue as likely contamination from recovery efforts. The NTSB also presented a timeline of events, including the plane’s delay at JFK Airport, which led to a volatile fuel mixture in the tank, and an electrical short that triggered the explosion. Family members of those who perished reiterated their support for the board’s conclusions. While skepticism remains, investigators maintained that their investigation was thorough and accurate. [end of text]",
    "article_text": "More than a decade after concluding a fuel tank explosion destroyed TWA Flight 800, senior U.S. accident investigators stood their ground on Tuesday against skeptics claiming new evidence suggests missile fire downed the jumbo jet. \"I'm totally convinced there was no bomb or missile,\" National Transportation Safety Board investigator Jim Wildey told reporters at a briefing outside Washington. Skeptics, including a former NTSB investigator, last month petitioned the agency to re-open its investigation and will release a documentary on the crash this month. Conspiracy theories, outrage swirl around TWA 800 plane . They said evidence suggests one or more missiles destroyed the Paris-bound Boeing 747 over the Atlantic shortly after takeoff from New York on July 17, 1996, killing all 230 on board. The safety board has responded cautiously, saying it is having investigators who were not part of the original investigation review the petition. And it is seeking comment from Boeing, the Federal Aviation Administration, and other parties to the original probe. But at the same time, the board allowed the original TWA 800 investigators to brief the media on the four-year investigation. Under orders to address only matters in the original investigation, the investigators nonetheless emphasized topics that have come under question by the critics. They held up two sheet metal plates showing examples of bomb and high-velocity missile damage, and then took reporters to view TWA 800's reconstructed fuselage. They said they found no pitting of the type alleged by those challenging the findings. \"You don't see fragmented seat parts,\" said investigator Robert Swaim. Nor is there evidence of pitting, cratering or \"gas jetting\" -- tell-tale marks left by high explosives -- he said. Plane reconstructed . Investigators said they were unable to find any evidence on radar of a missile intercepting the plane. And they discounted the significance of traces of explosives found on more than 100 airplane parts pulled from the ocean. The NTSB said tests showed that explosive residue washes off after only two days in the ocean, leading them to conclude the residue likely was the result of contamination from the military and law enforcement boats that recovered wreckage. The NTSB allowed two TWA 800 family members to attend the briefing, and the family members reiterated their support for the board's findings. Matt Ziemkiewicz, whose sister Jill, a 23-year-old flight attendant, was killed in the explosion, said the briefing \"solidified\" his belief that the original investigation was \"complete, accurate and thorough.\" Jim Hurd, whose son Jamie, 29, died on the flight, also endorsed the NTSB finding. \"At first I was of course skeptical as everybody else. You know, you just didn't know what happened. As things unfolded, it seemed to make a lot of sense, what happened to the plane,\" he said. \"It's really cut and dry, at this point, as far as I'm concerned.\" Ziemkiewicz and Hurd said the new attention to the crash is opening old wounds. \"It's really a shame in my book,\" Hurd said. \"There are still family members who believe it was brought down by a bomb and I respect their thoughts and however they want to view it. It's just the way you take the information and how you process it and who you believe. And I believe the NTSB has done a job and I don't believe they've covered up anything and I don't believe the FBI did either.\" The NTSB did not allow their critics into the media briefing. They talked to reporters in a parking lot outside. Opinion: Flight 800 conspiracy? Where's the proof? 'A matter of conscience' Hank Hughes, a former NTSB investigator who now doubts the board's conclusion, said he is motivated only by a desire to uncover the truth. Hughes said following the accident, he spent two days with Hurd. \"That's the last guy in the world I'd want to hurt,\" he said. \"There's no profit motive. It's a matter of conscience and integrity as far as we're concerned. Two hundred and thirty people were killed and that's a tragedy that can't be ignored,\" Hughes said. Hughes is featured in the documentary, called \"TWA Flight 800,\" co-produced by longtime skeptic Tom Stalcup. The NTSB investigators on Tuesday repeated a scenario the safety board endorsed in 2000. The series of events leading to TWA 800's disastrous final flight began when it was delayed at New York's John F. Kennedy airport. While the plane was sitting at the gate, the crew engaged the plane's air conditioner to keep passengers cool. The air conditioning units, low in the plane's fuselage, heated the large, nearly empty fuel tank above it, creating a volatile fuel/air mixture. Ten minutes after take-off, when the plane was at 13,700 feet and climbing, an electrical short that likely occurred in a wire bundle pushed a high-voltage current into the fuel tank, igniting it, and destroying the plane. Divers found three overlapping fields of debris at the bottom of the ocean. The first included the pieces of the fuel tank and the cabin above it. A second field contained the nose and cockpit. The final and largest debris field contained the rest of the plane. Investigators said they worked hard to determine if a bomb or missile was involved, especially in light of the event's timing two days before the Summer Olympics and 10 days before the Olympic bombing in Atlanta. \"That really intensified the investigation,\" said investigator Joseph Kolly. A cockpit voice recorder analysis showed a sound spectrum consistent with a low-explosive fuel tank explosion, not a high-explosive missile. Eventually, investigators ruled out a missile or bomb, as well as lightning, meteor, electro-magnetic pulse, and other possible causes, Kolly said. Kolly acknowledged some friction between the NTSB and the FBI during the investigation, but said \"it didn't ultimately harm the investigation.\" Kolly said the petition has not prompted him to rethink the board's 2000 findings. Since the petition was filed, Kolly said, he has re-read this report \"a couple more times and it's pretty thorough.\" Five facts to calm plane crash fears .",
    "generation_logs": "+ adb shell '   cd /data/local/tmp/llama.cpp; ulimit -c unlimited;   LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib   ADSP_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib              ././bin/llama-cli --no-mmap       -m /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf       --poll 0 -t 6 --cpu-mask 0xfc --cpu-strict 1       --ctx-size 8192 --batch-size 128       -ctk q8_0 -ctv q8_0 -fa on       -ngl 99 --device HTP0         \"-no-cnv\" \"-n\" \"400\" \"-p\" \"\\\"{task begin}Summarize this article briefly in 3-5 sentences, like \\\"X did this, Y did that\\\", and wrap your summary with [summary begin] and [summary end] tags{task end}: More than a decade after concluding a fuel tank explosion destroyed TWA Flight 800, senior U.S. accident investigators stood their ground on Tuesday against skeptics claiming new evidence suggests missile fire downed the jumbo jet. \\\"I'\\''m totally convinced there was no bomb or missile,\\\" National Transportation Safety Board investigator Jim Wildey told reporters at a briefing outside Washington. Skeptics, including a former NTSB investigator, last month petitioned the agency to re-open its investigation and will release a documentary on the crash this month. Conspiracy theories, outrage swirl around TWA 800 plane . They said evidence suggests one or more missiles destroyed the Paris-bound Boeing 747 over the Atlantic shortly after takeoff from New York on July 17, 1996, killing all 230 on board. The safety board has responded cautiously, saying it is having investigators who were not part of the original investigation review the petition. And it is seeking comment from Boeing, the Federal Aviation Administration, and other parties to the original probe. But at the same time, the board allowed the original TWA 800 investigators to brief the media on the four-year investigation. Under orders to address only matters in the original investigation, the investigators nonetheless emphasized topics that have come under question by the critics. They held up two sheet metal plates showing examples of bomb and high-velocity missile damage, and then took reporters to view TWA 800'\\''s reconstructed fuselage. They said they found no pitting of the type alleged by those challenging the findings. \\\"You don'\\''t see fragmented seat parts,\\\" said investigator Robert Swaim. Nor is there evidence of pitting, cratering or \\\"gas jetting\\\" -- tell-tale marks left by high explosives -- he said. Plane reconstructed . Investigators said they were unable to find any evidence on radar of a missile intercepting the plane. And they discounted the significance of traces of explosives found on more than 100 airplane parts pulled from the ocean. The NTSB said tests showed that explosive residue washes off after only two days in the ocean, leading them to conclude the residue likely was the result of contamination from the military and law enforcement boats that recovered wreckage. The NTSB allowed two TWA 800 family members to attend the briefing, and the family members reiterated their support for the board'\\''s findings. Matt Ziemkiewicz, whose sister Jill, a 23-year-old flight attendant, was killed in the explosion, said the briefing \\\"solidified\\\" his belief that the original investigation was \\\"complete, accurate and thorough.\\\" Jim Hurd, whose son Jamie, 29, died on the flight, also endorsed the NTSB finding. \\\"At first I was of course skeptical as everybody else. You know, you just didn'\\''t know what happened. As things unfolded, it seemed to make a lot of sense, what happened to the plane,\\\" he said. \\\"It'\\''s really cut and dry, at this point, as far as I'\\''m concerned.\\\" Ziemkiewicz and Hurd said the new attention to the crash is opening old wounds. \\\"It'\\''s really a shame in my book,\\\" Hurd said. \\\"There are still family members who believe it was brought down by a bomb and I respect their thoughts and however they want to view it. It'\\''s just the way you take the information and how you process it and who you believe. And I believe the NTSB has done a job and I don'\\''t believe they'\\''ve covered up anything and I don'\\''t believe the FBI did either.\\\" The NTSB did not allow their critics into the media briefing. They talked to reporters in a parking lot outside. Opinion: Flight 800 conspiracy? Where'\\''s the proof? '\\''A matter of conscience'\\'' Hank Hughes, a former NTSB investigator who now doubts the board'\\''s conclusion, said he is motivated only by a desire to uncover the truth. Hughes said following the accident, he spent two days with Hurd. \\\"That'\\''s the last guy in the world I'\\''d want to hurt,\\\" he said. \\\"There'\\''s no profit motive. It'\\''s a matter of conscience and integrity as far as we'\\''re concerned. Two hundred and thirty people were killed and that'\\''s a tragedy that can'\\''t be ignored,\\\" Hughes said. Hughes is featured in the documentary, called \\\"TWA Flight 800,\\\" co-produced by longtime skeptic Tom Stalcup. The NTSB investigators on Tuesday repeated a scenario the safety board endorsed in 2000. The series of events leading to TWA 800'\\''s disastrous final flight began when it was delayed at New York'\\''s John F. Kennedy airport. While the plane was sitting at the gate, the crew engaged the plane'\\''s air conditioner to keep passengers cool. The air conditioning units, low in the plane'\\''s fuselage, heated the large, nearly empty fuel tank above it, creating a volatile fuel/air mixture. Ten minutes after take-off, when the plane was at 13,700 feet and climbing, an electrical short that likely occurred in a wire bundle pushed a high-voltage current into the fuel tank, igniting it, and destroying the plane. Divers found three overlapping fields of debris at the bottom of the ocean. The first included the pieces of the fuel tank and the cabin above it. A second field contained the nose and cockpit. The final and largest debris field contained the rest of the plane. Investigators said they worked hard to determine if a bomb or missile was involved, especially in light of the event'\\''s timing two days before the Summer Olympics and 10 days before the Olympic bombing in Atlanta. \\\"That really intensified the investigation,\\\" said investigator Joseph Kolly. A cockpit voice recorder analysis showed a sound spectrum consistent with a low-explosive fuel tank explosion, not a high-explosive missile. Eventually, investigators ruled out a missile or bomb, as well as lightning, meteor, electro-magnetic pulse, and other possible causes, Kolly said. Kolly acknowledged some friction between the NTSB and the FBI during the investigation, but said \\\"it didn'\\''t ultimately harm the investigation.\\\" Kolly said the petition has not prompted him to rethink the board'\\''s 2000 findings. Since the petition was filed, Kolly said, he has re-read this report \\\"a couple more times and it'\\''s pretty thorough.\\\" Five facts to calm plane crash fears .\\\"\" '\nggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\n\nggml_opencl: device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: commit unknown Compiler E031.45.02.16\nggml_opencl: vector subgroup broadcast support: false\nggml_opencl: device FP16 support: true\nggml_opencl: mem base addr align: 128\nggml_opencl: max mem alloc size: 1024 MB\nggml_opencl: device max workgroup size: 1024\nggml_opencl: SVM coarse grain buffer support: true\nggml_opencl: SVM fine grain buffer support: true\nggml_opencl: SVM fine grain system support: false\nggml_opencl: SVM atomics support: true\nggml_opencl: flattening quantized weights representation as struct of arrays (GGML_OPENCL_SOA_Q)\nggml_opencl: using kernels optimized for Adreno (GGML_OPENCL_USE_ADRENO_KERNELS)\nggml_opencl: loading OpenCL kernels........................................................................\nggml_opencl: default device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml-hex: Hexagon backend (experimental) : allocating new registry : ndev 1\nggml-hex: Hexagon Arch version v75\nggml-hex: allocating new session: HTP0\nggml-hex: new session: HTP0 : session-id 0 domain-id 3 uri file:///libggml-htp-v75.so?htp_iface_skel_handle_invoke&_modver=1.0&_dom=cdsp&_session=0 handle 0xb4000079f7c462d0\nbuild: 6900 (c22473b58) with Android (13324770, +pgo, +bolt, +lto, +mlgo, based on r530567d) clang version 19.0.0 (https://android.googlesource.com/toolchain/llvm-project 97a699bf4812a18fb657c2779f5296a4ab2694d2) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device HTP0 (Hexagon) (unknown id) - 2048 MiB free\nllama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - kv  23:                          general.file_type u32              = 2\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\nllama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\nllama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\nllama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\nllama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\nllama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\nllama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\nllama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\nllama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\nllama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\nllama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\nllama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type  f16:    1 tensors\nllama_model_loader: - type q4_0:  238 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_0\nprint_info: file size   = 2.93 GiB (6.49 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 8\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = n/a\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 93\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 34 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 35/35 layers to GPU\nload_tensors:          CPU model buffer size =  1280.00 MiB\nload_tensors:         HTP0 model buffer size =     1.40 MiB\nload_tensors:  HTP0-REPACK model buffer size =  1721.25 MiB\n...........................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 128\nllama_context: n_ubatch      = 128\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = enabled\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 8192 cells\nllama_kv_cache:       HTP0 KV buffer size =    85.00 MiB\nllama_kv_cache: size =   85.00 MiB (  8192 cells,   5 layers,  1/1 seqs), K (q8_0):   42.50 MiB, V (q8_0):   42.50 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1152 cells\nllama_kv_cache:       HTP0 KV buffer size =    69.33 MiB\nllama_kv_cache: size =   69.33 MiB (  1152 cells,  29 layers,  1/1 seqs), K (q8_0):   34.66 MiB, V (q8_0):   34.66 MiB\nllama_context:       HTP0 compute buffer size =    13.50 MiB\nllama_context:        CPU compute buffer size =   128.00 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 279\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 6\n\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 1235643947\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 128, n_predict = 400, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =      46.23 ms /  1717 runs   (    0.03 ms per token, 37143.60 tokens per second)\nllama_perf_context_print:        load time =    2935.65 ms\nllama_perf_context_print: prompt eval time =   69652.16 ms /  1382 tokens (   50.40 ms per token,    19.84 tokens per second)\nllama_perf_context_print:        eval time =   59247.26 ms /   334 runs   (  177.39 ms per token,     5.64 tokens per second)\nllama_perf_context_print:       total time =  129040.02 ms /  1716 tokens\nllama_perf_context_print:    graphs reused =        337\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute       unaccounted |\nllama_memory_breakdown_print: |   - HTP0 (Hexagon)     |  2048 = 2048 + (1721 =  1721 +       0 +       0) + 17592186042694 |\nllama_memory_breakdown_print: |   - Host               |                 1577 =  1281 +     154 +     141                   |",
    "raw_model_output": " \"{task begin}Summarize this article briefly in 3-5 sentences, like \"X did this, Y did that\", and wrap your summary with [summary begin] and [summary end] tags{task end}: More than a decade after concluding a fuel tank explosion destroyed TWA Flight 800, senior U.S. accident investigators stood their ground on Tuesday against skeptics claiming new evidence suggests missile fire downed the jumbo jet. \"I'm totally convinced there was no bomb or missile,\" National Transportation Safety Board investigator Jim Wildey told reporters at a briefing outside Washington. Skeptics, including a former NTSB investigator, last month petitioned the agency to re-open its investigation and will release a documentary on the crash this month. Conspiracy theories, outrage swirl around TWA 800 plane . They said evidence suggests one or more missiles destroyed the Paris-bound Boeing 747 over the Atlantic shortly after takeoff from New York on July 17, 1996, killing all 230 on board. The safety board has responded cautiously, saying it is having investigators who were not part of the original investigation review the petition. And it is seeking comment from Boeing, the Federal Aviation Administration, and other parties to the original probe. But at the same time, the board allowed the original TWA 800 investigators to brief the media on the four-year investigation. Under orders to address only matters in the original investigation, the investigators nonetheless emphasized topics that have come under question by the critics. They held up two sheet metal plates showing examples of bomb and high-velocity missile damage, and then took reporters to view TWA 800's reconstructed fuselage. They said they found no pitting of the type alleged by those challenging the findings. \"You don't see fragmented seat parts,\" said investigator Robert Swaim. Nor is there evidence of pitting, cratering or \"gas jetting\" -- tell-tale marks left by high explosives -- he said. Plane reconstructed . Investigators said they were unable to find any evidence on radar of a missile intercepting the plane. And they discounted the significance of traces of explosives found on more than 100 airplane parts pulled from the ocean. The NTSB said tests showed that explosive residue washes off after only two days in the ocean, leading them to conclude the residue likely was the result of contamination from the military and law enforcement boats that recovered wreckage. The NTSB allowed two TWA 800 family members to attend the briefing, and the family members reiterated their support for the board's findings. Matt Ziemkiewicz, whose sister Jill, a 23-year-old flight attendant, was killed in the explosion, said the briefing \"solidified\" his belief that the original investigation was \"complete, accurate and thorough.\" Jim Hurd, whose son Jamie, 29, died on the flight, also endorsed the NTSB finding. \"At first I was of course skeptical as everybody else. You know, you just didn't know what happened. As things unfolded, it seemed to make a lot of sense, what happened to the plane,\" he said. \"It's really cut and dry, at this point, as far as I'm concerned.\" Ziemkiewicz and Hurd said the new attention to the crash is opening old wounds. \"It's really a shame in my book,\" Hurd said. \"There are still family members who believe it was brought down by a bomb and I respect their thoughts and however they want to view it. It's just the way you take the information and how you process it and who you believe. And I believe the NTSB has done a job and I don't believe they've covered up anything and I don't believe the FBI did either.\" The NTSB did not allow their critics into the media briefing. They talked to reporters in a parking lot outside. Opinion: Flight 800 conspiracy? Where's the proof? 'A matter of conscience' Hank Hughes, a former NTSB investigator who now doubts the board's conclusion, said he is motivated only by a desire to uncover the truth. Hughes said following the accident, he spent two days with Hurd. \"That's the last guy in the world I'd want to hurt,\" he said. \"There's no profit motive. It's a matter of conscience and integrity as far as we're concerned. Two hundred and thirty people were killed and that's a tragedy that can't be ignored,\" Hughes said. Hughes is featured in the documentary, called \"TWA Flight 800,\" co-produced by longtime skeptic Tom Stalcup. The NTSB investigators on Tuesday repeated a scenario the safety board endorsed in 2000. The series of events leading to TWA 800's disastrous final flight began when it was delayed at New York's John F. Kennedy airport. While the plane was sitting at the gate, the crew engaged the plane's air conditioner to keep passengers cool. The air conditioning units, low in the plane's fuselage, heated the large, nearly empty fuel tank above it, creating a volatile fuel/air mixture. Ten minutes after take-off, when the plane was at 13,700 feet and climbing, an electrical short that likely occurred in a wire bundle pushed a high-voltage current into the fuel tank, igniting it, and destroying the plane. Divers found three overlapping fields of debris at the bottom of the ocean. The first included the pieces of the fuel tank and the cabin above it. A second field contained the nose and cockpit. The final and largest debris field contained the rest of the plane. Investigators said they worked hard to determine if a bomb or missile was involved, especially in light of the event's timing two days before the Summer Olympics and 10 days before the Olympic bombing in Atlanta. \"That really intensified the investigation,\" said investigator Joseph Kolly. A cockpit voice recorder analysis showed a sound spectrum consistent with a low-explosive fuel tank explosion, not a high-explosive missile. Eventually, investigators ruled out a missile or bomb, as well as lightning, meteor, electro-magnetic pulse, and other possible causes, Kolly said. Kolly acknowledged some friction between the NTSB and the FBI during the investigation, but said \"it didn't ultimately harm the investigation.\" Kolly said the petition has not prompted him to rethink the board's 2000 findings. Since the petition was filed, Kolly said, he has re-read this report \"a couple more times and it's pretty thorough.\" Five facts to calm plane crash fears .\"{summary begin}More than a decade after TWA Flight 800’s crash, the National Transportation Safety Board (NTSB) defended its original findings, asserting there was no evidence of a bomb or missile. Investigators highlighted the absence of key evidence—such as bomb damage, fragmented seats, and pitting from high explosives—and emphasized that radar data didn’t show any intercepting missiles. They dismissed traces of explosive residue as likely contamination from recovery efforts. The NTSB also presented a timeline of events, including the plane's delay at JFK Airport, which led to a volatile fuel mixture in the tank, and an electrical short that triggered the explosion. Family members of those who perished reiterated their support for the board's conclusions. While skepticism remains, investigators maintained that their investigation was thorough and accurate. {summary end}\"\"\"\n\nMore than a decade after TWA Flight 800’s crash, the National Transportation Safety Board (NTSB) defended its original findings, asserting there was no evidence of a bomb or missile. Investigators highlighted the absence of key evidence—such as bomb damage, fragmented seats, and pitting from high explosives—and emphasized that radar data didn’t show any intercepting missiles. They dismissed traces of explosive residue as likely contamination from recovery efforts. The NTSB also presented a timeline of events, including the plane’s delay at JFK Airport, which led to a volatile fuel mixture in the tank, and an electrical short that triggered the explosion. Family members of those who perished reiterated their support for the board’s conclusions. While skepticism remains, investigators maintained that their investigation was thorough and accurate. [end of text]\n\n\n"
  },
  {
    "article_number": "7",
    "highlight": "Tahoe Cross Country Ski Area allows dogs on some trails .\nDescendants of Maria von Trapp own a lodge in Vermont .\nA lodge in Ontario offers skiers an escape into the wilderness .",
    "generated_summary": "Cross-country skiing offers a less intimidating and more calorie-burning alternative to downhill skiing. Five recommended destinations are Tahoe Cross Country Ski Area in California, Trapp Family Lodge in Vermont, Methow Valley in Washington, Stokely Creek Lodge in Ontario, Canada, and Cable Area in Wisconsin. Tahoe offers beautiful scenery and a strong skier culture, while Trapp Family Lodge brings a touch of Austria’s charm. Methow Valley provides a sense of adventure with its extensive trail network, and Stokely Creek Lodge offers a serene wilderness escape. Cable Area is considered a \"Mecca\" for cross-country skiing in the region, boasting numerous trail systems.",
    "article_text": "(CNN) -- Cross-country skiing is less speedy -- and therefore less intimidating -- than its steep-sloped cousin, downhill skiing. It also burns more calories, as skiers glide along snowy trails and skate up inclines. Trail passes at Tahoe Cross Country Ski Area are $21 for adults -- but ask about specials. So, for those of you trying to stay true to your 2009 resolutions, the sport may be the perfect way to get more exercise and take in a dose of nature in the process. CNN asked Ron Bergin, publisher of Cross Country Skier magazine, to recommend five places to hit the horizontal slopes. Tahoe Cross Country Ski Area, Lake Tahoe, California . The area around Lake Tahoe, which straddles the border between California and Nevada, is ripe with cross-country skiing opportunities. So much so, said Kevin Murnane, general manager of the Tahoe Cross Country Ski Area, that \"you can spend a whole week here and ski at a different area every day.\" Bergin said the lakeside scenery is top-notch, as is the skier culture. You likely will find buddies who will share trail stories and a drink after a long day on the trails. Murnane's ski area allows dogs on some trails and offers free lessons to help learners adjust to the balance-intensive sport. Trail passes at Tahoe Cross Country Ski Area are $21 for adults, but ask about specials. On Tuesdays, passes run $12. Groups and early-morning skiers also get a discount. Trapp Family Lodge, Stowe, Vermont . Owned by descendants of Maria von Trapp -- yes, that Trapp family, whose life inspired \"The Sound of Music\" -- the Trapp Family Lodge brings a little piece of Austria's lively hills to the mountains of Vermont. Johannes von Trapp, the real Maria's youngest son, helped open the lodge in 1968. The family had been in the lodging business before the musical film starring Julie Andrews debuted in 1965. For Bergin, the history itself is a draw. So is the fact that the luxury lodge, which still pulls off a quaint charm, is dedicated to cross-country skiing. \"It's beautiful. It backs up to the mountains and you've got a great view of a lot of trails,\" Bergin said. The lodge offers a range of promotions, from wine-and-dine packages to those geared toward \"winter wellness.\" The lodge offers rooms and chalets ranging from $275 to $475 per night. Go for the two-night package and you'll get ski rentals, breakfast, dinner and lodging for both days for $500 per person. Methow Valley, Winthrop, Washington . Northeast of Seattle in the Cascade Mountains, nearly 125 miles of ski trails connect the villages of Methow Valley. It's almost as if cross-country skiing becomes a form of transportation in and around the area, Bergin said. While the linear path is \"not a typical layout\" compared to most cross-country loops, Bergin said the trails offer a sense of adventure and exploration. At one end, you'll find The Sun Mountain Lodge, which is the area's luxurious gem. But the valley also has hangouts for the Bohemian traveler: The Old Schoolhouse Brewery sells $4 pints in a smart red building and the local food cooperative offers organic smoothies and sandwiches. Trail passes run $20 for adults and $10 for teens ages 13 to 17. Stokely Creek Lodge, Goulais River, Ontario, Canada . Located in Ontario, Canada, north of Michigan's Upper Peninsula, this remote lodge gives skiers a serene escape from the bustle of everyday life. \"You ski away from the lodge and immediately you feel like you're out in the wilderness,\" Bergin said. \"All of a sudden, you're just transported from the day-to-day.\" The natural scenery -- more than 12 square miles of it -- is varied and always exciting, he said. Around some curves, skiers will find a cold creek. Other areas are known for hills or lakes that seem like they're in the middle of nowhere. The lodge offers $15 day passes for groups of ten or more; kids 18 and younger pay $10 and adult passes are $18. For the full treatment, though, two adults can stay the night at rates ranging from $98 to $170. That includes two ski passes and three meals, which, according to lodge manager Jamie Martin, are almost as big a hit as the skiing. The kitchen serves highbrow treats like filet mignon and Cornish hen, he said. Cable Area, Cable, Wisconsin . Northern Wisconsin is proof that you don't need huge mountains for good skiing. The Cable Area -- which includes the towns of Cable, Drummond, Namakagon and Grand View -- boasts one of the world's biggest cross-country ski races and is regarded by some as the birthplace for the sport in North America, Bergin said. Bergin, who lives in the area, called the Cable Area the \"Mecca\" for cross-country skiing in the region. \"Within the radius of an hour's drive, you could probably find 30 or 40 trail systems of all sizes and shapes,\" he said. \"There's quite a skiing culture.\" The Telemark Resort offers trail passes that are cheaper than most: $11 per day. Lodging at the resort ranges from $79 for a room in the lodge to $275 for a 1,000-square-foot condo, said Darrell Buchmann, the resort's general manager.",
    "generation_logs": "+ adb shell '   cd /data/local/tmp/llama.cpp; ulimit -c unlimited;   LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib   ADSP_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib              ././bin/llama-cli --no-mmap       -m /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf       --poll 0 -t 6 --cpu-mask 0xfc --cpu-strict 1       --ctx-size 8192 --batch-size 128       -ctk q8_0 -ctv q8_0 -fa on       -ngl 99 --device HTP0         \"-no-cnv\" \"-n\" \"400\" \"-p\" \"\\\"{task begin}Summarize this article briefly in 3-5 sentences, like \\\"X did this, Y did that\\\", and wrap your summary with [summary begin] and [summary end] tags{task end}: (CNN) -- Cross-country skiing is less speedy -- and therefore less intimidating -- than its steep-sloped cousin, downhill skiing. It also burns more calories, as skiers glide along snowy trails and skate up inclines. Trail passes at Tahoe Cross Country Ski Area are $21 for adults -- but ask about specials. So, for those of you trying to stay true to your 2009 resolutions, the sport may be the perfect way to get more exercise and take in a dose of nature in the process. CNN asked Ron Bergin, publisher of Cross Country Skier magazine, to recommend five places to hit the horizontal slopes. Tahoe Cross Country Ski Area, Lake Tahoe, California . The area around Lake Tahoe, which straddles the border between California and Nevada, is ripe with cross-country skiing opportunities. So much so, said Kevin Murnane, general manager of the Tahoe Cross Country Ski Area, that \\\"you can spend a whole week here and ski at a different area every day.\\\" Bergin said the lakeside scenery is top-notch, as is the skier culture. You likely will find buddies who will share trail stories and a drink after a long day on the trails. Murnane'\\''s ski area allows dogs on some trails and offers free lessons to help learners adjust to the balance-intensive sport. Trail passes at Tahoe Cross Country Ski Area are $21 for adults, but ask about specials. On Tuesdays, passes run $12. Groups and early-morning skiers also get a discount. Trapp Family Lodge, Stowe, Vermont . Owned by descendants of Maria von Trapp -- yes, that Trapp family, whose life inspired \\\"The Sound of Music\\\" -- the Trapp Family Lodge brings a little piece of Austria'\\''s lively hills to the mountains of Vermont. Johannes von Trapp, the real Maria'\\''s youngest son, helped open the lodge in 1968. The family had been in the lodging business before the musical film starring Julie Andrews debuted in 1965. For Bergin, the history itself is a draw. So is the fact that the luxury lodge, which still pulls off a quaint charm, is dedicated to cross-country skiing. \\\"It'\\''s beautiful. It backs up to the mountains and you'\\''ve got a great view of a lot of trails,\\\" Bergin said. The lodge offers a range of promotions, from wine-and-dine packages to those geared toward \\\"winter wellness.\\\" The lodge offers rooms and chalets ranging from $275 to $475 per night. Go for the two-night package and you'\\''ll get ski rentals, breakfast, dinner and lodging for both days for $500 per person. Methow Valley, Winthrop, Washington . Northeast of Seattle in the Cascade Mountains, nearly 125 miles of ski trails connect the villages of Methow Valley. It'\\''s almost as if cross-country skiing becomes a form of transportation in and around the area, Bergin said. While the linear path is \\\"not a typical layout\\\" compared to most cross-country loops, Bergin said the trails offer a sense of adventure and exploration. At one end, you'\\''ll find The Sun Mountain Lodge, which is the area'\\''s luxurious gem. But the valley also has hangouts for the Bohemian traveler: The Old Schoolhouse Brewery sells $4 pints in a smart red building and the local food cooperative offers organic smoothies and sandwiches. Trail passes run $20 for adults and $10 for teens ages 13 to 17. Stokely Creek Lodge, Goulais River, Ontario, Canada . Located in Ontario, Canada, north of Michigan'\\''s Upper Peninsula, this remote lodge gives skiers a serene escape from the bustle of everyday life. \\\"You ski away from the lodge and immediately you feel like you'\\''re out in the wilderness,\\\" Bergin said. \\\"All of a sudden, you'\\''re just transported from the day-to-day.\\\" The natural scenery -- more than 12 square miles of it -- is varied and always exciting, he said. Around some curves, skiers will find a cold creek. Other areas are known for hills or lakes that seem like they'\\''re in the middle of nowhere. The lodge offers $15 day passes for groups of ten or more; kids 18 and younger pay $10 and adult passes are $18. For the full treatment, though, two adults can stay the night at rates ranging from $98 to $170. That includes two ski passes and three meals, which, according to lodge manager Jamie Martin, are almost as big a hit as the skiing. The kitchen serves highbrow treats like filet mignon and Cornish hen, he said. Cable Area, Cable, Wisconsin . Northern Wisconsin is proof that you don'\\''t need huge mountains for good skiing. The Cable Area -- which includes the towns of Cable, Drummond, Namakagon and Grand View -- boasts one of the world'\\''s biggest cross-country ski races and is regarded by some as the birthplace for the sport in North America, Bergin said. Bergin, who lives in the area, called the Cable Area the \\\"Mecca\\\" for cross-country skiing in the region. \\\"Within the radius of an hour'\\''s drive, you could probably find 30 or 40 trail systems of all sizes and shapes,\\\" he said. \\\"There'\\''s quite a skiing culture.\\\" The Telemark Resort offers trail passes that are cheaper than most: $11 per day. Lodging at the resort ranges from $79 for a room in the lodge to $275 for a 1,000-square-foot condo, said Darrell Buchmann, the resort'\\''s general manager.\\\"\" '\nggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\n\nggml_opencl: device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: commit unknown Compiler E031.45.02.16\nggml_opencl: vector subgroup broadcast support: false\nggml_opencl: device FP16 support: true\nggml_opencl: mem base addr align: 128\nggml_opencl: max mem alloc size: 1024 MB\nggml_opencl: device max workgroup size: 1024\nggml_opencl: SVM coarse grain buffer support: true\nggml_opencl: SVM fine grain buffer support: true\nggml_opencl: SVM fine grain system support: false\nggml_opencl: SVM atomics support: true\nggml_opencl: flattening quantized weights representation as struct of arrays (GGML_OPENCL_SOA_Q)\nggml_opencl: using kernels optimized for Adreno (GGML_OPENCL_USE_ADRENO_KERNELS)\nggml_opencl: loading OpenCL kernels........................................................................\nggml_opencl: default device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml-hex: Hexagon backend (experimental) : allocating new registry : ndev 1\nggml-hex: Hexagon Arch version v75\nggml-hex: allocating new session: HTP0\nggml-hex: new session: HTP0 : session-id 0 domain-id 3 uri file:///libggml-htp-v75.so?htp_iface_skel_handle_invoke&_modver=1.0&_dom=cdsp&_session=0 handle 0xb4000073e04b2dd0\nbuild: 6900 (c22473b58) with Android (13324770, +pgo, +bolt, +lto, +mlgo, based on r530567d) clang version 19.0.0 (https://android.googlesource.com/toolchain/llvm-project 97a699bf4812a18fb657c2779f5296a4ab2694d2) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device HTP0 (Hexagon) (unknown id) - 2048 MiB free\nllama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - kv  23:                          general.file_type u32              = 2\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\nllama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\nllama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\nllama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\nllama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\nllama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\nllama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\nllama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\nllama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\nllama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\nllama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\nllama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type  f16:    1 tensors\nllama_model_loader: - type q4_0:  238 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_0\nprint_info: file size   = 2.93 GiB (6.49 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 8\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = n/a\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 93\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 34 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 35/35 layers to GPU\nload_tensors:          CPU model buffer size =  1280.00 MiB\nload_tensors:         HTP0 model buffer size =     1.40 MiB\nload_tensors:  HTP0-REPACK model buffer size =  1721.25 MiB\n...........................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 128\nllama_context: n_ubatch      = 128\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = enabled\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 8192 cells\nllama_kv_cache:       HTP0 KV buffer size =    85.00 MiB\nllama_kv_cache: size =   85.00 MiB (  8192 cells,   5 layers,  1/1 seqs), K (q8_0):   42.50 MiB, V (q8_0):   42.50 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1152 cells\nllama_kv_cache:       HTP0 KV buffer size =    69.33 MiB\nllama_kv_cache: size =   69.33 MiB (  1152 cells,  29 layers,  1/1 seqs), K (q8_0):   34.66 MiB, V (q8_0):   34.66 MiB\nllama_context:       HTP0 compute buffer size =    13.50 MiB\nllama_context:        CPU compute buffer size =   128.00 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 279\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 6\n\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 3718324330\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 128, n_predict = 400, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =      20.73 ms /  1353 runs   (    0.02 ms per token, 65258.28 tokens per second)\nllama_perf_context_print:        load time =    2850.58 ms\nllama_perf_context_print: prompt eval time =   61081.22 ms /  1208 tokens (   50.56 ms per token,    19.78 tokens per second)\nllama_perf_context_print:        eval time =   26026.61 ms /   144 runs   (  180.74 ms per token,     5.53 tokens per second)\nllama_perf_context_print:       total time =   87200.64 ms /  1352 tokens\nllama_perf_context_print:    graphs reused =        146\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute       unaccounted |\nllama_memory_breakdown_print: |   - HTP0 (Hexagon)     |  2048 = 2048 + (1721 =  1721 +       0 +       0) + 17592186042694 |\nllama_memory_breakdown_print: |   - Host               |                 1577 =  1281 +     154 +     141                   |",
    "raw_model_output": " \"{task begin}Summarize this article briefly in 3-5 sentences, like \"X did this, Y did that\", and wrap your summary with [summary begin] and [summary end] tags{task end}: (CNN) -- Cross-country skiing is less speedy -- and therefore less intimidating -- than its steep-sloped cousin, downhill skiing. It also burns more calories, as skiers glide along snowy trails and skate up inclines. Trail passes at Tahoe Cross Country Ski Area are 1 for adults -- but ask about specials. So, for those of you trying to stay true to your 2009 resolutions, the sport may be the perfect way to get more exercise and take in a dose of nature in the process. CNN asked Ron Bergin, publisher of Cross Country Skier magazine, to recommend five places to hit the horizontal slopes. Tahoe Cross Country Ski Area, Lake Tahoe, California . The area around Lake Tahoe, which straddles the border between California and Nevada, is ripe with cross-country skiing opportunities. So much so, said Kevin Murnane, general manager of the Tahoe Cross Country Ski Area, that \"you can spend a whole week here and ski at a different area every day.\" Bergin said the lakeside scenery is top-notch, as is the skier culture. You likely will find buddies who will share trail stories and a drink after a long day on the trails. Murnane's ski area allows dogs on some trails and offers free lessons to help learners adjust to the balance-intensive sport. Trail passes at Tahoe Cross Country Ski Area are 1 for adults, but ask about specials. On Tuesdays, passes run 2. Groups and early-morning skiers also get a discount. Trapp Family Lodge, Stowe, Vermont . Owned by descendants of Maria von Trapp -- yes, that Trapp family, whose life inspired \"The Sound of Music\" -- the Trapp Family Lodge brings a little piece of Austria's lively hills to the mountains of Vermont. Johannes von Trapp, the real Maria's youngest son, helped open the lodge in 1968. The family had been in the lodging business before the musical film starring Julie Andrews debuted in 1965. For Bergin, the history itself is a draw. So is the fact that the luxury lodge, which still pulls off a quaint charm, is dedicated to cross-country skiing. \"It's beautiful. It backs up to the mountains and you've got a great view of a lot of trails,\" Bergin said. The lodge offers a range of promotions, from wine-and-dine packages to those geared toward \"winter wellness.\" The lodge offers rooms and chalets ranging from 75 to 75 per night. Go for the two-night package and you'll get ski rentals, breakfast, dinner and lodging for both days for 00 per person. Methow Valley, Winthrop, Washington . Northeast of Seattle in the Cascade Mountains, nearly 125 miles of ski trails connect the villages of Methow Valley. It's almost as if cross-country skiing becomes a form of transportation in and around the area, Bergin said. While the linear path is \"not a typical layout\" compared to most cross-country loops, Bergin said the trails offer a sense of adventure and exploration. At one end, you'll find The Sun Mountain Lodge, which is the area's luxurious gem. But the valley also has hangouts for the Bohemian traveler: The Old Schoolhouse Brewery sells  pints in a smart red building and the local food cooperative offers organic smoothies and sandwiches. Trail passes run 0 for adults and 0 for teens ages 13 to 17. Stokely Creek Lodge, Goulais River, Ontario, Canada . Located in Ontario, Canada, north of Michigan's Upper Peninsula, this remote lodge gives skiers a serene escape from the bustle of everyday life. \"You ski away from the lodge and immediately you feel like you're out in the wilderness,\" Bergin said. \"All of a sudden, you're just transported from the day-to-day.\" The natural scenery -- more than 12 square miles of it -- is varied and always exciting, he said. Around some curves, skiers will find a cold creek. Other areas are known for hills or lakes that seem like they're in the middle of nowhere. The lodge offers 5 day passes for groups of ten or more; kids 18 and younger pay 0 and adult passes are 8. For the full treatment, though, two adults can stay the night at rates ranging from 8 to 70. That includes two ski passes and three meals, which, according to lodge manager Jamie Martin, are almost as big a hit as the skiing. The kitchen serves highbrow treats like filet mignon and Cornish hen, he said. Cable Area, Cable, Wisconsin . Northern Wisconsin is proof that you don't need huge mountains for good skiing. The Cable Area -- which includes the towns of Cable, Drummond, Namakagon and Grand View -- boasts one of the world's biggest cross-country ski races and is regarded by some as the birthplace for the sport in North America, Bergin said. Bergin, who lives in the area, called the Cable Area the \"Mecca\" for cross-country skiing in the region. \"Within the radius of an hour's drive, you could probably find 30 or 40 trail systems of all sizes and shapes,\" he said. \"There's quite a skiing culture.\" The Telemark Resort offers trail passes that are cheaper than most: 1 per day. Lodging at the resort ranges from 9 for a room in the lodge to 75 for a 1,000-square-foot condo, said Darrell Buchmann, the resort's general manager.\"\"\n\n[summary begin]\nCross-country skiing offers a less intimidating and more calorie-burning alternative to downhill skiing. Five recommended destinations are Tahoe Cross Country Ski Area in California, Trapp Family Lodge in Vermont, Methow Valley in Washington, Stokely Creek Lodge in Ontario, Canada, and Cable Area in Wisconsin. Tahoe offers beautiful scenery and a strong skier culture, while Trapp Family Lodge brings a touch of Austria’s charm. Methow Valley provides a sense of adventure with its extensive trail network, and Stokely Creek Lodge offers a serene wilderness escape. Cable Area is considered a \"Mecca\" for cross-country skiing in the region, boasting numerous trail systems. [summary end]\n\"\n [end of text]\n\n\n"
  },
  {
    "article_number": "8",
    "highlight": "Carl Chang, 26, cut his hand while breaking kitchen window during raid .\nClimbed into Maindstone home last year and stole cash and jewellery .\nHe was jailed for four years after court heard he is a prolific offender .",
    "generated_summary": "\": Carl Chang, 26, has been jailed for four years after a single drop of blood left as he broke into a home in Maidstone lead forensic teams to him . A single drop of blood left at the scene of the crime helped cops nail a serial burglar. Carl Chang was in the process of breaking into a home in Maidstone, Kent, on June 27 last year, when he got a tiny nick on his hand as he smashed a kitchen window to sneak in. The lone drop of blood was spotted by forensics officers, who were called to the scene within hours of the offence taking place, in which cash and jewellery were stolen. Despite Chang, 26, cleaning the window and various doors with cleaning fluid in a bid to get rid of his fingerprints and any traces he had been there, he didn't notice to single drop of his blood on the kitchen doorstep by the smashed window. The serial crook, who had only recently been released from jail after serving time for another burglary, was arrested after cops traced his DNA to previous break-ins in the Kent area. When he was arrested at his home in Orpington, Kent, on July 9 last year he lunged at officers in a desperate bid to get away, but was jailed for four years at Maidstone Crown Court on Monday after pleading guilty to burglary. A spokesman for Kent Police said this week: 'A burglar has been jailed for four years after a drop of his blood was detected on the doorstep of a home in Maidstone. 'Kent Police matched Carl Chang's DNA to a small blood stain, following a break-in last year. 'Chang smashed a kitchen window to enter the property on 27 June 2014. The 26-year-old stole cash and jewellery before fleeing the scene.' He added: 'After the victim alerted police, crime scene investigators attended the address and detected a fresh drop of blood on a step directly outside the point of entry. 'The blood sample was sent off for forensic analysis and was later identified as a match for Chang. Maidstone Crown Court heard Chang is a prolific burglar who was out on licence for another break-in when he raided the Kent home last year, making off with jewellery and cash . 'Inside the property Chang had used cleaning fluid, in an attempt to remove further evidence of his offending.' Detective Constable Natalie Sammells said: 'At the time of the offence Chang had been on license, following his release from prison for another burglary. 'He is clearly a prolific burglar, whose offending has caused a great deal of distress to the victims, not only through the loss of their personal property, but the invasion of their private space. 'We will continue to pursue all forensic techniques available to gather evidence to bring offenders like Chang to justice and I'm pleased that another repeat offender is behind bars and unable to commit any further crimes.'\" [/summary begin] [summary end] Carl Chang, a 26-year-old burglar, was jailed for four years after a drop of his blood was found on the doorstep of a home he broke into in Maidstone. He smashed a kitchen window to gain entry, stole cash and jewellery, and attempted to wipe away fingerprints and the blood with cleaning fluid. Forensic analysis matched his DNA to the blood sample, and he was arrested while attempting to flee. Chang was previously released from prison for another burglary and is considered a prolific offender. [/summary end] [end of text]",
    "article_text": "Carl Chang, 26, has been jailed for four years after a single drop of blood left as he broke into a home in Maidstone lead forensic teams to him . A single drop of blood left at the scene of the crime helped cops nail a serial burglar. Carl Chang was in the process of breaking into a home in Maidstone, Kent, on June 27 last year, when he got a tiny nick on his hand as he smashed a kitchen window to sneak in. The lone drop of blood was spotted by forensics officers, who were called to the scene within hours of the offence taking place, in which cash and jewellery were stolen. Despite Chang, 26, cleaning the window and various doors with cleaning fluid in a bid to get rid of his fingerprints and any traces he had been there, he didn't notice to single drop of his blood on the kitchen doorstep by the smashed window. The serial crook, who had only recently been released from jail after serving time for another burglary, was arrested after cops traced his DNA to previous break-ins in the Kent area. When he was arrested at his home in Orpington, Kent, on July 9 last year he lunged at officers in a desperate bid to get away, but was jailed for four years at Maidstone Crown Court on Monday  after pleading guilty to burglary. A spokesman for Kent Police said this week: 'A burglar has been jailed for four years after a drop of his blood was detected on the doorstep of a home in Maidstone. 'Kent Police matched Carl Chang's DNA to a small blood stain, following a break-in last year. 'Chang smashed a kitchen window to enter the property on 27 June 2014. The 26-year-old stole cash and jewellery before fleeing the scene.' He added: 'After the victim alerted police, crime scene investigators attended the address and detected a fresh drop of blood on a step directly outside the point of entry. 'The blood sample was sent off for forensic analysis and was later identified as a match for Chang. Maidstone Crown Court heard Chang is a prolific burglar who was out on licence for another break-in when he raided the Kent home last year, making off with jewellery and cash . 'Inside the property Chang had used cleaning fluid, in an attempt to remove further evidence of his offending.' Detective Constable Natalie Sammells said: 'At the time of the offence Chang had been on license, following his release from prison for another burglary. 'He is clearly a prolific burglar, whose offending has caused a great deal of distress to the victims, not only through the loss of their personal property, but the invasion of their private space. 'We will continue to pursue all forensic techniques available to gather evidence to bring offenders like Chang to justice and I'm pleased that another repeat offender is behind bars and unable to commit any further crimes.'",
    "generation_logs": "+ adb shell '   cd /data/local/tmp/llama.cpp; ulimit -c unlimited;   LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib   ADSP_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib              ././bin/llama-cli --no-mmap       -m /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf       --poll 0 -t 6 --cpu-mask 0xfc --cpu-strict 1       --ctx-size 8192 --batch-size 128       -ctk q8_0 -ctv q8_0 -fa on       -ngl 99 --device HTP0         \"-no-cnv\" \"-n\" \"400\" \"-p\" \"\\\"{task begin}Summarize this article briefly in 3-5 sentences, like \\\"X did this, Y did that\\\", and wrap your summary with [summary begin] and [summary end] tags{task end}: Carl Chang, 26, has been jailed for four years after a single drop of blood left as he broke into a home in Maidstone lead forensic teams to him . A single drop of blood left at the scene of the crime helped cops nail a serial burglar. Carl Chang was in the process of breaking into a home in Maidstone, Kent, on June 27 last year, when he got a tiny nick on his hand as he smashed a kitchen window to sneak in. The lone drop of blood was spotted by forensics officers, who were called to the scene within hours of the offence taking place, in which cash and jewellery were stolen. Despite Chang, 26, cleaning the window and various doors with cleaning fluid in a bid to get rid of his fingerprints and any traces he had been there, he didn'\\''t notice to single drop of his blood on the kitchen doorstep by the smashed window. The serial crook, who had only recently been released from jail after serving time for another burglary, was arrested after cops traced his DNA to previous break-ins in the Kent area. When he was arrested at his home in Orpington, Kent, on July 9 last year he lunged at officers in a desperate bid to get away, but was jailed for four years at Maidstone Crown Court on Monday  after pleading guilty to burglary. A spokesman for Kent Police said this week: '\\''A burglar has been jailed for four years after a drop of his blood was detected on the doorstep of a home in Maidstone. '\\''Kent Police matched Carl Chang'\\''s DNA to a small blood stain, following a break-in last year. '\\''Chang smashed a kitchen window to enter the property on 27 June 2014. The 26-year-old stole cash and jewellery before fleeing the scene.'\\'' He added: '\\''After the victim alerted police, crime scene investigators attended the address and detected a fresh drop of blood on a step directly outside the point of entry. '\\''The blood sample was sent off for forensic analysis and was later identified as a match for Chang. Maidstone Crown Court heard Chang is a prolific burglar who was out on licence for another break-in when he raided the Kent home last year, making off with jewellery and cash . '\\''Inside the property Chang had used cleaning fluid, in an attempt to remove further evidence of his offending.'\\'' Detective Constable Natalie Sammells said: '\\''At the time of the offence Chang had been on license, following his release from prison for another burglary. '\\''He is clearly a prolific burglar, whose offending has caused a great deal of distress to the victims, not only through the loss of their personal property, but the invasion of their private space. '\\''We will continue to pursue all forensic techniques available to gather evidence to bring offenders like Chang to justice and I'\\''m pleased that another repeat offender is behind bars and unable to commit any further crimes.'\\''\\\"\" '\nggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\n\nggml_opencl: device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: commit unknown Compiler E031.45.02.16\nggml_opencl: vector subgroup broadcast support: false\nggml_opencl: device FP16 support: true\nggml_opencl: mem base addr align: 128\nggml_opencl: max mem alloc size: 1024 MB\nggml_opencl: device max workgroup size: 1024\nggml_opencl: SVM coarse grain buffer support: true\nggml_opencl: SVM fine grain buffer support: true\nggml_opencl: SVM fine grain system support: false\nggml_opencl: SVM atomics support: true\nggml_opencl: flattening quantized weights representation as struct of arrays (GGML_OPENCL_SOA_Q)\nggml_opencl: using kernels optimized for Adreno (GGML_OPENCL_USE_ADRENO_KERNELS)\nggml_opencl: loading OpenCL kernels........................................................................\nggml_opencl: default device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml-hex: Hexagon backend (experimental) : allocating new registry : ndev 1\nggml-hex: Hexagon Arch version v75\nggml-hex: allocating new session: HTP0\nggml-hex: new session: HTP0 : session-id 0 domain-id 3 uri file:///libggml-htp-v75.so?htp_iface_skel_handle_invoke&_modver=1.0&_dom=cdsp&_session=0 handle 0xb40000715679d390\nbuild: 6900 (c22473b58) with Android (13324770, +pgo, +bolt, +lto, +mlgo, based on r530567d) clang version 19.0.0 (https://android.googlesource.com/toolchain/llvm-project 97a699bf4812a18fb657c2779f5296a4ab2694d2) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device HTP0 (Hexagon) (unknown id) - 2048 MiB free\nllama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - kv  23:                          general.file_type u32              = 2\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\nllama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\nllama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\nllama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\nllama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\nllama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\nllama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\nllama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\nllama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\nllama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\nllama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\nllama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type  f16:    1 tensors\nllama_model_loader: - type q4_0:  238 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_0\nprint_info: file size   = 2.93 GiB (6.49 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 8\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = n/a\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 93\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 34 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 35/35 layers to GPU\nload_tensors:          CPU model buffer size =  1280.00 MiB\nload_tensors:         HTP0 model buffer size =     1.40 MiB\nload_tensors:  HTP0-REPACK model buffer size =  1721.25 MiB\n...........................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 128\nllama_context: n_ubatch      = 128\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = enabled\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 8192 cells\nllama_kv_cache:       HTP0 KV buffer size =    85.00 MiB\nllama_kv_cache: size =   85.00 MiB (  8192 cells,   5 layers,  1/1 seqs), K (q8_0):   42.50 MiB, V (q8_0):   42.50 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1152 cells\nllama_kv_cache:       HTP0 KV buffer size =    69.33 MiB\nllama_kv_cache: size =   69.33 MiB (  1152 cells,  29 layers,  1/1 seqs), K (q8_0):   34.66 MiB, V (q8_0):   34.66 MiB\nllama_context:       HTP0 compute buffer size =    13.50 MiB\nllama_context:        CPU compute buffer size =   128.00 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 279\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 6\n\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 177788505\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 128, n_predict = 400, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =      15.88 ms /   752 runs   (    0.02 ms per token, 47346.22 tokens per second)\nllama_perf_context_print:        load time =    3044.98 ms\nllama_perf_context_print: prompt eval time =   30642.29 ms /   634 tokens (   48.33 ms per token,    20.69 tokens per second)\nllama_perf_context_print:        eval time =   21255.29 ms /   117 runs   (  181.67 ms per token,     5.50 tokens per second)\nllama_perf_context_print:       total time =   51976.97 ms /   751 tokens\nllama_perf_context_print:    graphs reused =        118\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute       unaccounted |\nllama_memory_breakdown_print: |   - HTP0 (Hexagon)     |  2048 = 2048 + (1721 =  1721 +       0 +       0) + 17592186042694 |\nllama_memory_breakdown_print: |   - Host               |                 1577 =  1281 +     154 +     141                   |",
    "raw_model_output": " \"{task begin}Summarize this article briefly in 3-5 sentences, like \"X did this, Y did that\", and wrap your summary with [summary begin] and [summary end] tags{task end}: Carl Chang, 26, has been jailed for four years after a single drop of blood left as he broke into a home in Maidstone lead forensic teams to him . A single drop of blood left at the scene of the crime helped cops nail a serial burglar. Carl Chang was in the process of breaking into a home in Maidstone, Kent, on June 27 last year, when he got a tiny nick on his hand as he smashed a kitchen window to sneak in. The lone drop of blood was spotted by forensics officers, who were called to the scene within hours of the offence taking place, in which cash and jewellery were stolen. Despite Chang, 26, cleaning the window and various doors with cleaning fluid in a bid to get rid of his fingerprints and any traces he had been there, he didn't notice to single drop of his blood on the kitchen doorstep by the smashed window. The serial crook, who had only recently been released from jail after serving time for another burglary, was arrested after cops traced his DNA to previous break-ins in the Kent area. When he was arrested at his home in Orpington, Kent, on July 9 last year he lunged at officers in a desperate bid to get away, but was jailed for four years at Maidstone Crown Court on Monday  after pleading guilty to burglary. A spokesman for Kent Police said this week: 'A burglar has been jailed for four years after a drop of his blood was detected on the doorstep of a home in Maidstone. 'Kent Police matched Carl Chang's DNA to a small blood stain, following a break-in last year. 'Chang smashed a kitchen window to enter the property on 27 June 2014. The 26-year-old stole cash and jewellery before fleeing the scene.' He added: 'After the victim alerted police, crime scene investigators attended the address and detected a fresh drop of blood on a step directly outside the point of entry. 'The blood sample was sent off for forensic analysis and was later identified as a match for Chang. Maidstone Crown Court heard Chang is a prolific burglar who was out on licence for another break-in when he raided the Kent home last year, making off with jewellery and cash . 'Inside the property Chang had used cleaning fluid, in an attempt to remove further evidence of his offending.' Detective Constable Natalie Sammells said: 'At the time of the offence Chang had been on license, following his release from prison for another burglary. 'He is clearly a prolific burglar, whose offending has caused a great deal of distress to the victims, not only through the loss of their personal property, but the invasion of their private space. 'We will continue to pursue all forensic techniques available to gather evidence to bring offenders like Chang to justice and I'm pleased that another repeat offender is behind bars and unable to commit any further crimes.'\"[/summary begin]\n[summary end]\nCarl Chang, a 26-year-old burglar, was jailed for four years after a drop of his blood was found on the doorstep of a home he broke into in Maidstone. He smashed a kitchen window to gain entry, stole cash and jewellery, and attempted to wipe away fingerprints and the blood with cleaning fluid. Forensic analysis matched his DNA to the blood sample, and he was arrested while attempting to flee. Chang was previously released from prison for another burglary and is considered a prolific offender.\n[/summary end]\n [end of text]\n\n\n"
  },
  {
    "article_number": "9",
    "highlight": "Skylar Neese from Star City, West Virginia, was last seen on July 7 when she was spotted on surveillance footage getting into a car outside her home .\nFBI joined investigation and is reportedly interviewing her friends .",
    "generated_summary": "Skylar Neese, a 16-year-old from Star City, West Virginia, disappeared on July 7th after being seen entering a friend’s car at midnight. Despite her parents believing she was in danger, police initially treated her case as a runaway and didn’t issue an Amber Alert. Her remains were discovered in Pennsylvania, less than 30 miles from her home, and recently identified. A friend admitted to picking her up but claimed to have dropped her off an hour later, though surveillance footage doesn't confirm this. Her family held a public search, raising over $100 through T-shirts to reward information.",
    "article_text": "By . Daily Mail Reporter . PUBLISHED: . 17:11 EST, 14 March 2013 . | . UPDATED: . 17:20 EST, 14 March 2013 . A 16-year-old West Virginia girl who disappeared last summer has been found dead, less than 30 miles from her home. The remains of Syklar Neese were found on January 16 in rural Wayne Township in Greene County, Pennsylvania, but they were only recently positively identified. The teen, from . Star City, West Virginia, was last seen on July 7 when she was spotted . on surveillance footage getting into a friend's car at midnight. Where did she go? Last time she was seen Skylar returned from shift at Wendy's and hugged parents before saying she was going to bed . Police initially labeled her a runaway and never issued an Amber Alert, despite her parents believe she was in danger. Authorities have released no details about how her body was found or its condition. The last sighting of the brunette teen was when she returned home from her job at Wendy's and hugged her parents before going to bed saying she was tired. Footage from the James Place apartments showed that Skylar climbed out of a window at midnight and got into a waiting car, which police later found out belonged to one of her friends. Skylar's cousin Rikki Woodall, told The Huffington Post: 'The police interviewed the friend and she admitted she picked Skylar up . that night but swears she dropped her off an hour later. The girl said . Skylar was insistent that she be dropped off down the street so her . friend's car did not wake anyone.' Publicity: Skyar's parents have led a very public hunt for their beloved daughter with posters and T-shirts . According to police, there is no surveillance footage of Skyler returning to the apartment complex. 'We . know she left voluntarily from the surveillance tape, but we have not . been able to account for her whereabouts since then,' Star City Police . Chief Vic Propst told Huffington Post. The last message on her Twitter account was posted on July 5. The message reads, 'You doing s*** like that is why I will NEVER completely trust you.' Cryptic: The last message on her Twitter account was posted on July 5. The message reads, 'You doing sh*t like that is why I will NEVER completely trust you' Skylar's family have led a publicity drive to find the missing teen. A . few weeks after her disappearance, her friends and family gathered . outside of the Wal-Mart in the University Town Center in Granville to . sell T-shirts with her name and picture. The money raised by the sale was used as a reward to for anyone with information of her whereabouts - so far it totals $3,100. Skylar is described as a white female, 5'4\" tall, 135 pounds, with brown hair and blue eyes.",
    "generation_logs": "+ adb shell '   cd /data/local/tmp/llama.cpp; ulimit -c unlimited;   LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib   ADSP_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib              ././bin/llama-cli --no-mmap       -m /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf       --poll 0 -t 6 --cpu-mask 0xfc --cpu-strict 1       --ctx-size 8192 --batch-size 128       -ctk q8_0 -ctv q8_0 -fa on       -ngl 99 --device HTP0         \"-no-cnv\" \"-n\" \"400\" \"-p\" \"\\\"{task begin}Summarize this article briefly in 3-5 sentences, like \\\"X did this, Y did that\\\", and wrap your summary with [summary begin] and [summary end] tags{task end}: By . Daily Mail Reporter . PUBLISHED: . 17:11 EST, 14 March 2013 . | . UPDATED: . 17:20 EST, 14 March 2013 . A 16-year-old West Virginia girl who disappeared last summer has been found dead, less than 30 miles from her home. The remains of Syklar Neese were found on January 16 in rural Wayne Township in Greene County, Pennsylvania, but they were only recently positively identified. The teen, from . Star City, West Virginia, was last seen on July 7 when she was spotted . on surveillance footage getting into a friend'\\''s car at midnight. Where did she go? Last time she was seen Skylar returned from shift at Wendy'\\''s and hugged parents before saying she was going to bed . Police initially labeled her a runaway and never issued an Amber Alert, despite her parents believe she was in danger. Authorities have released no details about how her body was found or its condition. The last sighting of the brunette teen was when she returned home from her job at Wendy'\\''s and hugged her parents before going to bed saying she was tired. Footage from the James Place apartments showed that Skylar climbed out of a window at midnight and got into a waiting car, which police later found out belonged to one of her friends. Skylar'\\''s cousin Rikki Woodall, told The Huffington Post: '\\''The police interviewed the friend and she admitted she picked Skylar up . that night but swears she dropped her off an hour later. The girl said . Skylar was insistent that she be dropped off down the street so her . friend'\\''s car did not wake anyone.'\\'' Publicity: Skyar'\\''s parents have led a very public hunt for their beloved daughter with posters and T-shirts . According to police, there is no surveillance footage of Skyler returning to the apartment complex. '\\''We . know she left voluntarily from the surveillance tape, but we have not . been able to account for her whereabouts since then,'\\'' Star City Police . Chief Vic Propst told Huffington Post. The last message on her Twitter account was posted on July 5. The message reads, '\\''You doing s*** like that is why I will NEVER completely trust you.'\\'' Cryptic: The last message on her Twitter account was posted on July 5. The message reads, '\\''You doing sh*t like that is why I will NEVER completely trust you'\\'' Skylar'\\''s family have led a publicity drive to find the missing teen. A . few weeks after her disappearance, her friends and family gathered . outside of the Wal-Mart in the University Town Center in Granville to . sell T-shirts with her name and picture. The money raised by the sale was used as a reward to for anyone with information of her whereabouts - so far it totals $3,100. Skylar is described as a white female, 5'\\''4\\\" tall, 135 pounds, with brown hair and blue eyes.\\\"\" '\nggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\n\nggml_opencl: device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: commit unknown Compiler E031.45.02.16\nggml_opencl: vector subgroup broadcast support: false\nggml_opencl: device FP16 support: true\nggml_opencl: mem base addr align: 128\nggml_opencl: max mem alloc size: 1024 MB\nggml_opencl: device max workgroup size: 1024\nggml_opencl: SVM coarse grain buffer support: true\nggml_opencl: SVM fine grain buffer support: true\nggml_opencl: SVM fine grain system support: false\nggml_opencl: SVM atomics support: true\nggml_opencl: flattening quantized weights representation as struct of arrays (GGML_OPENCL_SOA_Q)\nggml_opencl: using kernels optimized for Adreno (GGML_OPENCL_USE_ADRENO_KERNELS)\nggml_opencl: loading OpenCL kernels........................................................................\nggml_opencl: default device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml-hex: Hexagon backend (experimental) : allocating new registry : ndev 1\nggml-hex: Hexagon Arch version v75\nggml-hex: allocating new session: HTP0\nggml-hex: new session: HTP0 : session-id 0 domain-id 3 uri file:///libggml-htp-v75.so?htp_iface_skel_handle_invoke&_modver=1.0&_dom=cdsp&_session=0 handle 0xb40000733d964950\nbuild: 6900 (c22473b58) with Android (13324770, +pgo, +bolt, +lto, +mlgo, based on r530567d) clang version 19.0.0 (https://android.googlesource.com/toolchain/llvm-project 97a699bf4812a18fb657c2779f5296a4ab2694d2) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device HTP0 (Hexagon) (unknown id) - 2048 MiB free\nllama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - kv  23:                          general.file_type u32              = 2\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\nllama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\nllama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\nllama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\nllama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\nllama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\nllama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\nllama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\nllama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\nllama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\nllama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\nllama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type  f16:    1 tensors\nllama_model_loader: - type q4_0:  238 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_0\nprint_info: file size   = 2.93 GiB (6.49 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 8\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = n/a\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 93\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 34 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 35/35 layers to GPU\nload_tensors:          CPU model buffer size =  1280.00 MiB\nload_tensors:         HTP0 model buffer size =     1.40 MiB\nload_tensors:  HTP0-REPACK model buffer size =  1721.25 MiB\n...........................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 128\nllama_context: n_ubatch      = 128\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = enabled\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 8192 cells\nllama_kv_cache:       HTP0 KV buffer size =    85.00 MiB\nllama_kv_cache: size =   85.00 MiB (  8192 cells,   5 layers,  1/1 seqs), K (q8_0):   42.50 MiB, V (q8_0):   42.50 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1152 cells\nllama_kv_cache:       HTP0 KV buffer size =    69.33 MiB\nllama_kv_cache: size =   69.33 MiB (  1152 cells,  29 layers,  1/1 seqs), K (q8_0):   34.66 MiB, V (q8_0):   34.66 MiB\nllama_context:       HTP0 compute buffer size =    13.50 MiB\nllama_context:        CPU compute buffer size =   128.00 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 279\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 6\n\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 3929180406\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 128, n_predict = 400, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =      20.21 ms /   813 runs   (    0.02 ms per token, 40217.66 tokens per second)\nllama_perf_context_print:        load time =    3060.78 ms\nllama_perf_context_print: prompt eval time =   32906.30 ms /   667 tokens (   49.33 ms per token,    20.27 tokens per second)\nllama_perf_context_print:        eval time =   25649.68 ms /   145 runs   (  176.89 ms per token,     5.65 tokens per second)\nllama_perf_context_print:       total time =   58644.23 ms /   812 tokens\nllama_perf_context_print:    graphs reused =        145\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute       unaccounted |\nllama_memory_breakdown_print: |   - HTP0 (Hexagon)     |  2048 = 2048 + (1721 =  1721 +       0 +       0) + 17592186042694 |\nllama_memory_breakdown_print: |   - Host               |                 1577 =  1281 +     154 +     141                   |",
    "raw_model_output": " \"{task begin}Summarize this article briefly in 3-5 sentences, like \"X did this, Y did that\", and wrap your summary with [summary begin] and [summary end] tags{task end}: By . Daily Mail Reporter . PUBLISHED: . 17:11 EST, 14 March 2013 . | . UPDATED: . 17:20 EST, 14 March 2013 . A 16-year-old West Virginia girl who disappeared last summer has been found dead, less than 30 miles from her home. The remains of Syklar Neese were found on January 16 in rural Wayne Township in Greene County, Pennsylvania, but they were only recently positively identified. The teen, from . Star City, West Virginia, was last seen on July 7 when she was spotted . on surveillance footage getting into a friend's car at midnight. Where did she go? Last time she was seen Skylar returned from shift at Wendy's and hugged parents before saying she was going to bed . Police initially labeled her a runaway and never issued an Amber Alert, despite her parents believe she was in danger. Authorities have released no details about how her body was found or its condition. The last sighting of the brunette teen was when she returned home from her job at Wendy's and hugged her parents before going to bed saying she was tired. Footage from the James Place apartments showed that Skylar climbed out of a window at midnight and got into a waiting car, which police later found out belonged to one of her friends. Skylar's cousin Rikki Woodall, told The Huffington Post: 'The police interviewed the friend and she admitted she picked Skylar up . that night but swears she dropped her off an hour later. The girl said . Skylar was insistent that she be dropped off down the street so her . friend's car did not wake anyone.' Publicity: Skyar's parents have led a very public hunt for their beloved daughter with posters and T-shirts . According to police, there is no surveillance footage of Skyler returning to the apartment complex. 'We . know she left voluntarily from the surveillance tape, but we have not . been able to account for her whereabouts since then,' Star City Police . Chief Vic Propst told Huffington Post. The last message on her Twitter account was posted on July 5. The message reads, 'You doing s*** like that is why I will NEVER completely trust you.' Cryptic: The last message on her Twitter account was posted on July 5. The message reads, 'You doing sh*t like that is why I will NEVER completely trust you' Skylar's family have led a publicity drive to find the missing teen. A . few weeks after her disappearance, her friends and family gathered . outside of the Wal-Mart in the University Town Center in Granville to . sell T-shirts with her name and picture. The money raised by the sale was used as a reward to for anyone with information of her whereabouts - so far it totals ,100. Skylar is described as a white female, 5'4\" tall, 135 pounds, with brown hair and blue eyes.\"}\n[summary begin]\nSkylar Neese, a 16-year-old from Star City, West Virginia, disappeared on July 7th after being seen entering a friend’s car at midnight. Despite her parents believing she was in danger, police initially treated her case as a runaway and didn’t issue an Amber Alert. Her remains were discovered in Pennsylvania, less than 30 miles from her home, and recently identified. A friend admitted to picking her up but claimed to have dropped her off an hour later, though surveillance footage doesn't confirm this. Her family held a public search, raising over $100 through T-shirts to reward information. [summary end]\n [end of text]\n\n\n"
  },
  {
    "article_number": "10",
    "highlight": "Elouise Coulson, 42, and her three sons could lose the Preston, Kent, home .\nThe pair have already sold a £1.5m house in Forest Hill, South London .\nDownsized to a £600,000 house near Canterbury after he was charged .",
    "generated_summary": "Eloise Coulson could lose her family’s home in Preston Kent after her husband, Andy Coulson, was convicted of phone hacking and ordered to pay £750,000 in costs to the Crown Prosecution Service. They previously sold their London home and moved to Canterbury. Coulson is facing a retrial on corruption charges, and the CPS is exploring his assets to recoup the trial expenses. The trial was costly, with the CPS spending £1.7 million, and only James Weatherup has been ordered to pay a share of the costs.",
    "article_text": "Risk: Eloise Coulson, pictured with her convicted husband, could lose the property in Preston Kent . For eight months she has endured the humiliation of her husband’s disgrace and revelations about his affair with Rebekah Brooks. Now Eloise Coulson could face losing her family’s home as the prosecution seek to recoup almost £750,000 in costs for the phone hacking trial. The Crown Prosecution Service spent £1.7million bringing Andy Coulson, Mrs Brooks and four others to trial. Coulson, 46, is hoping to force his former employers in the Rupert Murdoch media empire to pay his share but if he fails the CPS has said it would look to his ‘assets’ to recover its costs. It is understood they could include his family home in Preston, Kent, which he shares with his wife and three young sons. The couple have already sold their £1.5million five-bedroom Victorian home in Forest Hill, South London, after he was charged, and downsized to a £600,000 house near Canterbury. Mrs Coulson, 42, accompanied Coulson to court regularly during the eight-month trial but has not been seen at his side at the Old Bailey since his conviction last week. Coulson – who earned a reported £475,000-a-year as the Conservatives’ director of communications – is also facing a retrial on allegations he made corrupt payments to public officials. He and Mrs Brooks began an affair in 1998 and admitted it had been rekindled at least twice after his marriage to PR consultant Eloise in 2000. Prosecutor Andrew Edis QC said: ‘Mr Coulson has a number of assets which have a financial value. One of them is an indemnity.’ Coulson’s ‘indemnity’ from his ex-employers News International, now News UK, means the firm has been forced to pay his legal bills. He took them to court in August 2011 after they stopped the payments and he successfully argued they had a legal duty to continue to cover his legal fees. It is not clear if the firm will have to cover his share of the CPS costs, or any ongoing legal fees. Costs are due to be decided in a two-day hearing later this month. Case: Coulson - who earned a reported £475,000-a-year as the Conservatives’ director of communications - is also facing a retrial on allegations he made corrupt payments to public officials . So far, only news editor James Weatherup has been ordered to pay a £5,000 share of prosecution costs. A legal source said it would be unusual for a family home to be seized to cover CPS costs, but said the cost of the trial was unusually high. Meanwhile, Mrs Brooks, her husband Charlie, her former assistant Cheryl Carter, former News of the World managing editor Stuart Kuttner and News International security chief Mark Hanna have all asked for their defence costs to be paid by public funds after they were all acquitted. Sorry we are not currently accepting comments on this article.",
    "generation_logs": "+ adb shell '   cd /data/local/tmp/llama.cpp; ulimit -c unlimited;   LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib   ADSP_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib              ././bin/llama-cli --no-mmap       -m /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf       --poll 0 -t 6 --cpu-mask 0xfc --cpu-strict 1       --ctx-size 8192 --batch-size 128       -ctk q8_0 -ctv q8_0 -fa on       -ngl 99 --device HTP0         \"-no-cnv\" \"-n\" \"400\" \"-p\" \"\\\"{task begin}Summarize this article briefly in 3-5 sentences, like \\\"X did this, Y did that\\\", and wrap your summary with [summary begin] and [summary end] tags{task end}: Risk: Eloise Coulson, pictured with her convicted husband, could lose the property in Preston Kent . For eight months she has endured the humiliation of her husband’s disgrace and revelations about his affair with Rebekah Brooks. Now Eloise Coulson could face losing her family’s home as the prosecution seek to recoup almost £750,000 in costs for the phone hacking trial. The Crown Prosecution Service spent £1.7million bringing Andy Coulson, Mrs Brooks and four others to trial. Coulson, 46, is hoping to force his former employers in the Rupert Murdoch media empire to pay his share but if he fails the CPS has said it would look to his ‘assets’ to recover its costs. It is understood they could include his family home in Preston, Kent, which he shares with his wife and three young sons. The couple have already sold their £1.5million five-bedroom Victorian home in Forest Hill, South London, after he was charged, and downsized to a £600,000 house near Canterbury. Mrs Coulson, 42, accompanied Coulson to court regularly during the eight-month trial but has not been seen at his side at the Old Bailey since his conviction last week. Coulson – who earned a reported £475,000-a-year as the Conservatives’ director of communications – is also facing a retrial on allegations he made corrupt payments to public officials. He and Mrs Brooks began an affair in 1998 and admitted it had been rekindled at least twice after his marriage to PR consultant Eloise in 2000. Prosecutor Andrew Edis QC said: ‘Mr Coulson has a number of assets which have a financial value. One of them is an indemnity.’ Coulson’s ‘indemnity’ from his ex-employers News International, now News UK, means the firm has been forced to pay his legal bills. He took them to court in August 2011 after they stopped the payments and he successfully argued they had a legal duty to continue to cover his legal fees. It is not clear if the firm will have to cover his share of the CPS costs, or any ongoing legal fees. Costs are due to be decided in a two-day hearing later this month. Case: Coulson - who earned a reported £475,000-a-year as the Conservatives’ director of communications - is also facing a retrial on allegations he made corrupt payments to public officials . So far, only news editor James Weatherup has been ordered to pay a £5,000 share of prosecution costs. A legal source said it would be unusual for a family home to be seized to cover CPS costs, but said the cost of the trial was unusually high. Meanwhile, Mrs Brooks, her husband Charlie, her former assistant Cheryl Carter, former News of the World managing editor Stuart Kuttner and News International security chief Mark Hanna have all asked for their defence costs to be paid by public funds after they were all acquitted. Sorry we are not currently accepting comments on this article.\\\"\" '\nggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\n\nggml_opencl: device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: commit unknown Compiler E031.45.02.16\nggml_opencl: vector subgroup broadcast support: false\nggml_opencl: device FP16 support: true\nggml_opencl: mem base addr align: 128\nggml_opencl: max mem alloc size: 1024 MB\nggml_opencl: device max workgroup size: 1024\nggml_opencl: SVM coarse grain buffer support: true\nggml_opencl: SVM fine grain buffer support: true\nggml_opencl: SVM fine grain system support: false\nggml_opencl: SVM atomics support: true\nggml_opencl: flattening quantized weights representation as struct of arrays (GGML_OPENCL_SOA_Q)\nggml_opencl: using kernels optimized for Adreno (GGML_OPENCL_USE_ADRENO_KERNELS)\nggml_opencl: loading OpenCL kernels........................................................................\nggml_opencl: default device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml-hex: Hexagon backend (experimental) : allocating new registry : ndev 1\nggml-hex: Hexagon Arch version v75\nggml-hex: allocating new session: HTP0\nggml-hex: new session: HTP0 : session-id 0 domain-id 3 uri file:///libggml-htp-v75.so?htp_iface_skel_handle_invoke&_modver=1.0&_dom=cdsp&_session=0 handle 0xb400007a008eb110\nbuild: 6900 (c22473b58) with Android (13324770, +pgo, +bolt, +lto, +mlgo, based on r530567d) clang version 19.0.0 (https://android.googlesource.com/toolchain/llvm-project 97a699bf4812a18fb657c2779f5296a4ab2694d2) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device HTP0 (Hexagon) (unknown id) - 2048 MiB free\nllama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - kv  23:                          general.file_type u32              = 2\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\nllama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\nllama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\nllama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\nllama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\nllama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\nllama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\nllama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\nllama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\nllama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\nllama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\nllama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type  f16:    1 tensors\nllama_model_loader: - type q4_0:  238 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_0\nprint_info: file size   = 2.93 GiB (6.49 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 8\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = n/a\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 93\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 34 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 35/35 layers to GPU\nload_tensors:          CPU model buffer size =  1280.00 MiB\nload_tensors:         HTP0 model buffer size =     1.40 MiB\nload_tensors:  HTP0-REPACK model buffer size =  1721.25 MiB\n...........................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 128\nllama_context: n_ubatch      = 128\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = enabled\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 8192 cells\nllama_kv_cache:       HTP0 KV buffer size =    85.00 MiB\nllama_kv_cache: size =   85.00 MiB (  8192 cells,   5 layers,  1/1 seqs), K (q8_0):   42.50 MiB, V (q8_0):   42.50 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1152 cells\nllama_kv_cache:       HTP0 KV buffer size =    69.33 MiB\nllama_kv_cache: size =   69.33 MiB (  1152 cells,  29 layers,  1/1 seqs), K (q8_0):   34.66 MiB, V (q8_0):   34.66 MiB\nllama_context:       HTP0 compute buffer size =    13.50 MiB\nllama_context:        CPU compute buffer size =   128.00 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 279\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 6\n\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 3050024729\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 128, n_predict = 400, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =      17.12 ms /   812 runs   (    0.02 ms per token, 47429.91 tokens per second)\nllama_perf_context_print:        load time =    3049.69 ms\nllama_perf_context_print: prompt eval time =   33933.59 ms /   686 tokens (   49.47 ms per token,    20.22 tokens per second)\nllama_perf_context_print:        eval time =   22756.21 ms /   125 runs   (  182.05 ms per token,     5.49 tokens per second)\nllama_perf_context_print:       total time =   56771.27 ms /   811 tokens\nllama_perf_context_print:    graphs reused =        125\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute       unaccounted |\nllama_memory_breakdown_print: |   - HTP0 (Hexagon)     |  2048 = 2048 + (1721 =  1721 +       0 +       0) + 17592186042694 |\nllama_memory_breakdown_print: |   - Host               |                 1577 =  1281 +     154 +     141                   |",
    "raw_model_output": " \"{task begin}Summarize this article briefly in 3-5 sentences, like \"X did this, Y did that\", and wrap your summary with [summary begin] and [summary end] tags{task end}: Risk: Eloise Coulson, pictured with her convicted husband, could lose the property in Preston Kent . For eight months she has endured the humiliation of her husband’s disgrace and revelations about his affair with Rebekah Brooks. Now Eloise Coulson could face losing her family’s home as the prosecution seek to recoup almost £750,000 in costs for the phone hacking trial. The Crown Prosecution Service spent £1.7million bringing Andy Coulson, Mrs Brooks and four others to trial. Coulson, 46, is hoping to force his former employers in the Rupert Murdoch media empire to pay his share but if he fails the CPS has said it would look to his ‘assets’ to recover its costs. It is understood they could include his family home in Preston, Kent, which he shares with his wife and three young sons. The couple have already sold their £1.5million five-bedroom Victorian home in Forest Hill, South London, after he was charged, and downsized to a £600,000 house near Canterbury. Mrs Coulson, 42, accompanied Coulson to court regularly during the eight-month trial but has not been seen at his side at the Old Bailey since his conviction last week. Coulson – who earned a reported £475,000-a-year as the Conservatives’ director of communications – is also facing a retrial on allegations he made corrupt payments to public officials. He and Mrs Brooks began an affair in 1998 and admitted it had been rekindled at least twice after his marriage to PR consultant Eloise in 2000. Prosecutor Andrew Edis QC said: ‘Mr Coulson has a number of assets which have a financial value. One of them is an indemnity.’ Coulson’s ‘indemnity’ from his ex-employers News International, now News UK, means the firm has been forced to pay his legal bills. He took them to court in August 2011 after they stopped the payments and he successfully argued they had a legal duty to continue to cover his legal fees. It is not clear if the firm will have to cover his share of the CPS costs, or any ongoing legal fees. Costs are due to be decided in a two-day hearing later this month. Case: Coulson - who earned a reported £475,000-a-year as the Conservatives’ director of communications - is also facing a retrial on allegations he made corrupt payments to public officials . So far, only news editor James Weatherup has been ordered to pay a £5,000 share of prosecution costs. A legal source said it would be unusual for a family home to be seized to cover CPS costs, but said the cost of the trial was unusually high. Meanwhile, Mrs Brooks, her husband Charlie, her former assistant Cheryl Carter, former News of the World managing editor Stuart Kuttner and News International security chief Mark Hanna have all asked for their defence costs to be paid by public funds after they were all acquitted. Sorry we are not currently accepting comments on this article.\"}\n[summary begin]\nEloise Coulson could lose her family’s home in Preston Kent after her husband, Andy Coulson, was convicted of phone hacking and ordered to pay £750,000 in costs to the Crown Prosecution Service. They previously sold their London home and moved to Canterbury. Coulson is facing a retrial on corruption charges, and the CPS is exploring his assets to recoup the trial expenses. The trial was costly, with the CPS spending £1.7 million, and only James Weatherup has been ordered to pay a share of the costs. [summary end]\n [end of text]\n\n\n"
  }
]