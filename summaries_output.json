[
  {
    "article_number": "1",
    "highlight": "The Oscar-winning actress is now almost 97 and living in Paris .\nThe sisters were notorious for their bitter quarrels .\nFriend of De Havilland said she is in mourning and will never forget Joan .",
    "generated_summary": "Olivia de Havilland, the Oscar-winning actress, is grieving the death of her sister, Joan Fontaine, who passed away at the age of 96. Despite a long and famously contentious rivalry, De Havilland expressed deep sadness and vowed never to forget Fontaine. The sisters, both renowned Hollywood stars, had a difficult childhood marked by intense competition and animosity, exacerbated by their stage mother's ambitions. Fontaine won the Best Actress Oscar for 'Suspicion' in 1941, while De Havilland received two Oscars, including one for 'To Each His Own.' De Havilland is currently living in Paris and has been inundated with sympathy after the news of Fontaine's death.",
    "article_text": "By . Peter Allen . PUBLISHED: . 09:42 EST, 17 December 2013 . | . UPDATED: . 15:06 EST, 17 December 2013 . Hollywood legend Olivia de Havilland was today 'in mourning' over the death of her equally famous sister Joan Fontaine . Hollywood legend Olivia de Havilland was today ‘in mourning’ over the death of her equally famous sister Joan Fontaine, telling friends: ‘I’ll never forget her’. The Oscar-winning actress is now almost 97, and living in an upmarket apartment block close to the Arc de Triumphe in Paris. She and her sister were notorious for their bitter quarrels, and barely spent any time together during their long and successful screen careers. But on Monday Ms De Havilland issued a statement saying she was ‘shocked and saddened’ by Joan’s death on Sunday at the age of 96. ‘Joan was her kid sister – of course she’s hugely sad at her passing,’ said one of Ms De Havilland’s friends in the French capital. ‘She’s certainly in mourning and has made it clear that she will never forget Joan.’ Olivia has been bombarded with flowers and letters since Sunday night, and said in her statement that she was overwhelmed by ‘the many kind expressions of sympathies.’ Ms De Havilland and Ms Fontaine, who lived in California at the time of her death, were among Hollywood's most famous sisters. Both were nominated for the best actress Oscar in 1941, and Ms Fontaine won it for her role in the film ‘Suspicion’. Scroll down for video . Long feud: Sisters Olivia De Havilland and Joan Fontaine were born just 15 months apart . Recalling the awards ceremony many years . later, Ms Fontaine said: ‘All the animus we'd felt toward each other as . children, the hair pullings, the savage wrestling matches, the time . Olivia fractured my collarbone, all came rushing back in kaleidoscopic . imagery. ‘My paralysis was total. I felt Olivia would spring across the table and grab me by the hair. Left to right: Fontaine opposite Laurence Olivier in Rebecca (he had wanted the part of the second Mrs de Winter to go to his real life soon-to-be wife, Vivien Leigh); and with Orson Welles in Jane Eyre . 'I felt age 4, being confronted by my older sister. Damn it, I'd incurred her wrath again!’ Ms . De Havilland is hugely respected in France, and was awarded the Legion . of Honour by the then President Nicolas Sarkozy in 2010. She won two Oscars in all, including one for her part in in the 1947 film ‘To Each His Own’. Joan Fontaine (left) said the sibling rivalry was encouraged by their ambitious stage mother. It may have also had something to do with their similarity (the photo on the right, for example, is of Olivia de Havilland) Olivia de Havilland pictured with  Frederic Mitterrand at the 36th Cesar French Film Awards in Paris in 2011 . When . Fontaine tried to congratulate her for the 1947 award, Ms De Havilland . ignored her, with a spokesman saying: ‘This goes back for years and . years, ever since they were children.’ Ms . Fontaine once told The Hollywood Reporter: ‘I married first, won the . Oscar before Olivia did, and if I die first, she’ll undoubtedly be livid . because I beat her to it.’",
    "generation_logs": "+ adb shell '   cd /data/local/tmp/llama.cpp; ulimit -c unlimited;   LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib   ADSP_LIBRARY_PATH=/data/local/tmp/llama.cpp/./lib              ././bin/llama-cli --no-mmap       -m /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf       --poll 0 -t 6 --cpu-mask 0xfc --cpu-strict 1       --ctx-size 8192 --batch-size 128       -ctk q8_0 -ctv q8_0 -fa on       -ngl 99 --device HTP0         \"-no-cnv\" \"-n\" \"400\" \"-p\" \"\\\"{task begin}Summarize this article briefly and wrap your summary with [summary begin] and [summary end] tags{task end}: By . Peter Allen . PUBLISHED: . 09:42 EST, 17 December 2013 . | . UPDATED: . 15:06 EST, 17 December 2013 . Hollywood legend Olivia de Havilland was today '\\''in mourning'\\'' over the death of her equally famous sister Joan Fontaine . Hollywood legend Olivia de Havilland was today ‘in mourning’ over the death of her equally famous sister Joan Fontaine, telling friends: ‘I’ll never forget her’. The Oscar-winning actress is now almost 97, and living in an upmarket apartment block close to the Arc de Triumphe in Paris. She and her sister were notorious for their bitter quarrels, and barely spent any time together during their long and successful screen careers. But on Monday Ms De Havilland issued a statement saying she was ‘shocked and saddened’ by Joan’s death on Sunday at the age of 96. ‘Joan was her kid sister – of course she’s hugely sad at her passing,’ said one of Ms De Havilland’s friends in the French capital. ‘She’s certainly in mourning and has made it clear that she will never forget Joan.’ Olivia has been bombarded with flowers and letters since Sunday night, and said in her statement that she was overwhelmed by ‘the many kind expressions of sympathies.’ Ms De Havilland and Ms Fontaine, who lived in California at the time of her death, were among Hollywood'\\''s most famous sisters. Both were nominated for the best actress Oscar in 1941, and Ms Fontaine won it for her role in the film ‘Suspicion’. Scroll down for video . Long feud: Sisters Olivia De Havilland and Joan Fontaine were born just 15 months apart . Recalling the awards ceremony many years . later, Ms Fontaine said: ‘All the animus we'\\''d felt toward each other as . children, the hair pullings, the savage wrestling matches, the time . Olivia fractured my collarbone, all came rushing back in kaleidoscopic . imagery. ‘My paralysis was total. I felt Olivia would spring across the table and grab me by the hair. Left to right: Fontaine opposite Laurence Olivier in Rebecca (he had wanted the part of the second Mrs de Winter to go to his real life soon-to-be wife, Vivien Leigh); and with Orson Welles in Jane Eyre . '\\''I felt age 4, being confronted by my older sister. Damn it, I'\\''d incurred her wrath again!’ Ms . De Havilland is hugely respected in France, and was awarded the Legion . of Honour by the then President Nicolas Sarkozy in 2010. She won two Oscars in all, including one for her part in in the 1947 film ‘To Each His Own’. Joan Fontaine (left) said the sibling rivalry was encouraged by their ambitious stage mother. It may have also had something to do with their similarity (the photo on the right, for example, is of Olivia de Havilland) Olivia de Havilland pictured with  Frederic Mitterrand at the 36th Cesar French Film Awards in Paris in 2011 . When . Fontaine tried to congratulate her for the 1947 award, Ms De Havilland . ignored her, with a spokesman saying: ‘This goes back for years and . years, ever since they were children.’ Ms . Fontaine once told The Hollywood Reporter: ‘I married first, won the . Oscar before Olivia did, and if I die first, she’ll undoubtedly be livid . because I beat her to it.’\\\"\" '\nggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\n\nggml_opencl: device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: commit unknown Compiler E031.45.02.16\nggml_opencl: vector subgroup broadcast support: false\nggml_opencl: device FP16 support: true\nggml_opencl: mem base addr align: 128\nggml_opencl: max mem alloc size: 1024 MB\nggml_opencl: device max workgroup size: 1024\nggml_opencl: SVM coarse grain buffer support: true\nggml_opencl: SVM fine grain buffer support: true\nggml_opencl: SVM fine grain system support: false\nggml_opencl: SVM atomics support: true\nggml_opencl: flattening quantized weights representation as struct of arrays (GGML_OPENCL_SOA_Q)\nggml_opencl: using kernels optimized for Adreno (GGML_OPENCL_USE_ADRENO_KERNELS)\nggml_opencl: loading OpenCL kernels........................................................................\nggml_opencl: default device: 'QUALCOMM Adreno(TM) 750 (OpenCL 3.0 Adreno(TM) 750)'\nggml-hex: Hexagon backend (experimental) : allocating new registry : ndev 1\nggml-hex: Hexagon Arch version v75\nggml-hex: allocating new session: HTP0\nggml-hex: new session: HTP0 : session-id 0 domain-id 3 uri file:///libggml-htp-v75.so?htp_iface_skel_handle_invoke&_modver=1.0&_dom=cdsp&_session=0 handle 0xb400006fc30f1ed0\nbuild: 6900 (c22473b58) with Android (13324770, +pgo, +bolt, +lto, +mlgo, based on r530567d) clang version 19.0.0 (https://android.googlesource.com/toolchain/llvm-project 97a699bf4812a18fb657c2779f5296a4ab2694d2) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device HTP0 (Hexagon) (unknown id) - 2048 MiB free\nllama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /data/local/tmp/llama.cpp/../gguf/gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - kv  23:                          general.file_type u32              = 2\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\nllama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\nllama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\nllama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\nllama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\nllama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\nllama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\nllama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\nllama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\nllama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\nllama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\nllama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type  f16:    1 tensors\nllama_model_loader: - type q4_0:  238 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_0\nprint_info: file size   = 2.93 GiB (6.49 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 8\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = n/a\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 93\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 34 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 35/35 layers to GPU\nload_tensors:          CPU model buffer size =  1280.00 MiB\nload_tensors:         HTP0 model buffer size =     1.40 MiB\nload_tensors:  HTP0-REPACK model buffer size =  1721.25 MiB\n...........................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 128\nllama_context: n_ubatch      = 128\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = enabled\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 8192 cells\nllama_kv_cache:       HTP0 KV buffer size =    85.00 MiB\nllama_kv_cache: size =   85.00 MiB (  8192 cells,   5 layers,  1/1 seqs), K (q8_0):   42.50 MiB, V (q8_0):   42.50 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1152 cells\nllama_kv_cache:       HTP0 KV buffer size =    69.33 MiB\nllama_kv_cache: size =   69.33 MiB (  1152 cells,  29 layers,  1/1 seqs), K (q8_0):   34.66 MiB, V (q8_0):   34.66 MiB\nllama_context:       HTP0 compute buffer size =    13.50 MiB\nllama_context:        CPU compute buffer size =   128.00 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 279\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 6\n\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 1266827910\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 128, n_predict = 400, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =      20.44 ms /   936 runs   (    0.02 ms per token, 45783.60 tokens per second)\nllama_perf_context_print:        load time =    3312.91 ms\nllama_perf_context_print: prompt eval time =   39671.86 ms /   779 tokens (   50.93 ms per token,    19.64 tokens per second)\nllama_perf_context_print:        eval time =   27052.92 ms /   156 runs   (  173.42 ms per token,     5.77 tokens per second)\nllama_perf_context_print:       total time =   66811.39 ms /   935 tokens\nllama_perf_context_print:    graphs reused =        158\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute       unaccounted |\nllama_memory_breakdown_print: |   - HTP0 (Hexagon)     |  2048 = 2048 + (1721 =  1721 +       0 +       0) + 17592186042694 |\nllama_memory_breakdown_print: |   - Host               |                 1577 =  1281 +     154 +     141                   |",
    "raw_model_output": " \"{task begin}Summarize this article briefly and wrap your summary with [summary begin] and [summary end] tags{task end}: By . Peter Allen . PUBLISHED: . 09:42 EST, 17 December 2013 . | . UPDATED: . 15:06 EST, 17 December 2013 . Hollywood legend Olivia de Havilland was today 'in mourning' over the death of her equally famous sister Joan Fontaine . Hollywood legend Olivia de Havilland was today ‘in mourning’ over the death of her equally famous sister Joan Fontaine, telling friends: ‘I’ll never forget her’. The Oscar-winning actress is now almost 97, and living in an upmarket apartment block close to the Arc de Triumphe in Paris. She and her sister were notorious for their bitter quarrels, and barely spent any time together during their long and successful screen careers. But on Monday Ms De Havilland issued a statement saying she was ‘shocked and saddened’ by Joan’s death on Sunday at the age of 96. ‘Joan was her kid sister – of course she’s hugely sad at her passing,’ said one of Ms De Havilland’s friends in the French capital. ‘She’s certainly in mourning and has made it clear that she will never forget Joan.’ Olivia has been bombarded with flowers and letters since Sunday night, and said in her statement that she was overwhelmed by ‘the many kind expressions of sympathies.’ Ms De Havilland and Ms Fontaine, who lived in California at the time of her death, were among Hollywood's most famous sisters. Both were nominated for the best actress Oscar in 1941, and Ms Fontaine won it for her role in the film ‘Suspicion’. Scroll down for video . Long feud: Sisters Olivia De Havilland and Joan Fontaine were born just 15 months apart . Recalling the awards ceremony many years . later, Ms Fontaine said: ‘All the animus we'd felt toward each other as . children, the hair pullings, the savage wrestling matches, the time . Olivia fractured my collarbone, all came rushing back in kaleidoscopic . imagery. ‘My paralysis was total. I felt Olivia would spring across the table and grab me by the hair. Left to right: Fontaine opposite Laurence Olivier in Rebecca (he had wanted the part of the second Mrs de Winter to go to his real life soon-to-be wife, Vivien Leigh); and with Orson Welles in Jane Eyre . 'I felt age 4, being confronted by my older sister. Damn it, I'd incurred her wrath again!’ Ms . De Havilland is hugely respected in France, and was awarded the Legion . of Honour by the then President Nicolas Sarkozy in 2010. She won two Oscars in all, including one for her part in in the 1947 film ‘To Each His Own’. Joan Fontaine (left) said the sibling rivalry was encouraged by their ambitious stage mother. It may have also had something to do with their similarity (the photo on the right, for example, is of Olivia de Havilland) Olivia de Havilland pictured with  Frederic Mitterrand at the 36th Cesar French Film Awards in Paris in 2011 . When . Fontaine tried to congratulate her for the 1947 award, Ms De Havilland . ignored her, with a spokesman saying: ‘This goes back for years and . years, ever since they were children.’ Ms . Fontaine once told The Hollywood Reporter: ‘I married first, won the . Oscar before Olivia did, and if I die first, she’ll undoubtedly be livid . because I beat her to it.’\"\n[summary begin]\nOlivia de Havilland, the Oscar-winning actress, is grieving the death of her sister, Joan Fontaine, who passed away at the age of 96. Despite a long and famously contentious rivalry, De Havilland expressed deep sadness and vowed never to forget Fontaine. The sisters, both renowned Hollywood stars, had a difficult childhood marked by intense competition and animosity, exacerbated by their stage mother's ambitions. Fontaine won the Best Actress Oscar for 'Suspicion' in 1941, while De Havilland received two Oscars, including one for 'To Each His Own.' De Havilland is currently living in Paris and has been inundated with sympathy after the news of Fontaine's death. [summary end]\n [end of text]\n\n\n"
  }
]