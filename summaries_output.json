[
  {
    "article_number": "1",
    "highlight": "The Oscar-winning actress is now almost 97 and living in Paris .\nThe sisters were notorious for their bitter quarrels .\nFriend of De Havilland said she is in mourning and will never forget Joan .",
    "generated_summary": "Olivia de Havilland, the Oscar-winning Hollywood legend, expressed her grief over the death of her sister Joan Fontaine, who died at 96. Despite a long history of sibling rivalry and bitter quarrels, the two sisters were both iconic figures in Hollywood, with both winning Academy Award nominations and Fontaine winning the Best Actress Oscar for 'Suspicion'. De Havilland, now almost 97 and living in Paris, said she is 'in mourning' and has never forgotten her sister. The sisters' rivalry, fueled by their stage mother and their striking resemblance, is well-documented, though they shared a deep connection as sisters.",
    "article_text": "By . Peter Allen . PUBLISHED: . 09:42 EST, 17 December 2013 . | . UPDATED: . 15:06 EST, 17 December 2013 . Hollywood legend Olivia de Havilland was today 'in mourning' over the death of her equally famous sister Joan Fontaine . Hollywood legend Olivia de Havilland was today ‘in mourning’ over the death of her equally famous sister Joan Fontaine, telling friends: ‘I’ll never forget her’. The Oscar-winning actress is now almost 97, and living in an upmarket apartment block close to the Arc de Triumphe in Paris. She and her sister were notorious for their bitter quarrels, and barely spent any time together during their long and successful screen careers. But on Monday Ms De Havilland issued a statement saying she was ‘shocked and saddened’ by Joan’s death on Sunday at the age of 96. ‘Joan was her kid sister – of course she’s hugely sad at her passing,’ said one of Ms De Havilland’s friends in the French capital. ‘She’s certainly in mourning and has made it clear that she will never forget Joan.’ Olivia has been bombarded with flowers and letters since Sunday night, and said in her statement that she was overwhelmed by ‘the many kind expressions of sympathies.’ Ms De Havilland and Ms Fontaine, who lived in California at the time of her death, were among Hollywood's most famous sisters. Both were nominated for the best actress Oscar in 1941, and Ms Fontaine won it for her role in the film ‘Suspicion’. Scroll down for video . Long feud: Sisters Olivia De Havilland and Joan Fontaine were born just 15 months apart . Recalling the awards ceremony many years . later, Ms Fontaine said: ‘All the animus we'd felt toward each other as . children, the hair pullings, the savage wrestling matches, the time . Olivia fractured my collarbone, all came rushing back in kaleidoscopic . imagery. ‘My paralysis was total. I felt Olivia would spring across the table and grab me by the hair. Left to right: Fontaine opposite Laurence Olivier in Rebecca (he had wanted the part of the second Mrs de Winter to go to his real life soon-to-be wife, Vivien Leigh); and with Orson Welles in Jane Eyre . 'I felt age 4, being confronted by my older sister. Damn it, I'd incurred her wrath again!’ Ms . De Havilland is hugely respected in France, and was awarded the Legion . of Honour by the then President Nicolas Sarkozy in 2010. She won two Oscars in all, including one for her part in in the 1947 film ‘To Each His Own’. Joan Fontaine (left) said the sibling rivalry was encouraged by their ambitious stage mother. It may have also had something to do with their similarity (the photo on the right, for example, is of Olivia de Havilland) Olivia de Havilland pictured with  Frederic Mitterrand at the 36th Cesar French Film Awards in Paris in 2011 . When . Fontaine tried to congratulate her for the 1947 award, Ms De Havilland . ignored her, with a spokesman saying: ‘This goes back for years and . years, ever since they were children.’ Ms . Fontaine once told The Hollywood Reporter: ‘I married first, won the . Oscar before Olivia did, and if I die first, she’ll undoubtedly be livid . because I beat her to it.’",
    "generation_logs": "build: 7193 (d82b7a7c1) with Android (13989888, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-w64-windows-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 32 key-value pairs and 398 tensors from /data/local/tmp/gguf/qwen3-4b-instruct-2507-q4km.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 4B Instruct 2507\nllama_model_loader: - kv   3:                            general.version str              = 2507\nllama_model_loader: - kv   4:                           general.finetune str              = Instruct\nllama_model_loader: - kv   5:                           general.basename str              = Qwen3\nllama_model_loader: - kv   6:                         general.size_label str              = 4B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   9:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv  10:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv  11:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv  12:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  13:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  14:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  16:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  17:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  19:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - kv  31:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_embd_inp       = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen3 4B Instruct 2507\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\n.........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_seq     = 8192\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_seq (8192) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =  1152.00 MiB\nllama_kv_cache: size = 1152.00 MiB (  8192 cells,  36 layers,  1/1 seqs), K (f16):  576.00 MiB, V (f16):  576.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   306.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 4063760855\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 2048, n_predict = 400, n_keep = 0\n\ncommon_perf_print:    sampling time =     180.42 ms\ncommon_perf_print:    samplers time =      15.63 ms /   925 tokens\ncommon_perf_print:        load time =    2257.17 ms\ncommon_perf_print: prompt eval time =   31921.78 ms /   783 tokens (   40.77 ms per token,    24.53 tokens per second)\ncommon_perf_print:        eval time =   15474.74 ms /   141 runs   (  109.75 ms per token,     9.11 tokens per second)\ncommon_perf_print:       total time =   47585.98 ms /   924 tokens\ncommon_perf_print: unaccounted time =       9.03 ms /   0.0 %      (total - sampling - prompt eval - eval) / (total)\ncommon_perf_print:    graphs reused =        140\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                 3821 =  2362 +    1152 +     306                |\nllama_memory_breakdown_print: |   - CPU_REPACK         |                 1683 =  1683 +       0 +       0                |\n",
    "raw_model_output": "Olivia de Havilland, the Oscar-winning Hollywood legend, expressed her grief over the death of her sister Joan Fontaine, who died at 96. Despite a long history of sibling rivalry and bitter quarrels, the two sisters were both iconic figures in Hollywood, with both winning Academy Award nominations and Fontaine winning the Best Actress Oscar for 'Suspicion'. De Havilland, now almost 97 and living in Paris, said she is 'in mourning' and has never forgotten her sister. The sisters' rivalry, fueled by their stage mother and their striking resemblance, is well-documented, though they shared a deep connection as sisters."
  },
  {
    "article_number": "2",
    "highlight": "David Lidington is concerned noise will affect his Aylesbury constituents .\nHe is also worried about spiralling cost of the £42.6billion project .",
    "generated_summary": "Europe minister David Lidington has expressed strong opposition to the High Speed 2 (HS2) rail project, citing concerns over its soaring cost, reaching £42.6 billion, and noise levels affecting his Buckinghamshire constituents. He criticized the noise study, claiming it underestimated the impact, and suggested the project should be scrapped due to economic and environmental concerns. Despite being a minister tasked with supporting the project, he voiced constituents' doubts about the budget and its justification in the current economic climate. Other senior figures, including Owen Paterson and Cheryl Gillan, also express skepticism, though the government maintains HS2 will create economic benefits and support around 100,000 jobs. The first stage of HS2 will begin in 2017, with completion expected by 2026, and the second stage to Manchester and Leeds by 2033.",
    "article_text": "By . Gerri Peev . PUBLISHED: . 18:53 EST, 7 July 2013 . | . UPDATED: . 18:53 EST, 7 July 2013 . Europe minister David Lidington has become the latest senior figure to land a blow against the High Speed 2 rail link. The Tory MP, whose Aylesbury constituency in Buckinghamshire will be affected by the line, joined the chorus of opposition to HS2 – though as a minister he is duty bound to back the scheme. He raised fears over the spiralling cost – now at £42.6billion – and noise of the trains. In a letter to Transport Secretary Patrick McLoughlin, Mr Lidington came close to calling for the project to be scrapped after its budget soared by £10 billion. Objections: Europe minister David Lidington is worried about the spiralling cost of the HS2 project . He wrote that constituents had told him this rise ‘gives them no confidence that the final figure will [not] be significantly higher and in the current economic climate that cannot be justified’. Voters believed HS2 should be cancelled and the money spent on other infrastructure projects, he added. Mr Lidington also cast doubt on the level of noise those living along the HS2 line would have to endure. In a letter to Alison Munro, HS2’s chief executive, he said the study measuring average noise levels had included the period between midnight and 5am, when no trains run. This had led to ‘very serious underestimates of what my constituents are likely to experience between 11pm and midnight and 5am and 7am’, he said. Mr Lidington has been criticised by some constituents for not backing a rebel attempt to scupper paving legislation for HS2. High speed: Mr Lidington is also worried about late night and early morning noise from the line affecting his Buckinghamshire constituents . But he made clear resigning was a ‘nuclear’ option. Environment Secretary Owen Paterson is said to be ‘not a fan’ of the project and former Welsh Secretary Cheryl Gillan has labelled the scheme a ‘boys’ toy’. The Government insists HS2 will boost the economy in the North and that scrapping it would cost close to 100,000 jobs. The first stage, between London and Birmingham, will start in 2017 and finish in 2026. The second stage, to Manchester and Leeds, should be completed by 2033.",
    "generation_logs": "build: 7193 (d82b7a7c1) with Android (13989888, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-w64-windows-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 32 key-value pairs and 398 tensors from /data/local/tmp/gguf/qwen3-4b-instruct-2507-q4km.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 4B Instruct 2507\nllama_model_loader: - kv   3:                            general.version str              = 2507\nllama_model_loader: - kv   4:                           general.finetune str              = Instruct\nllama_model_loader: - kv   5:                           general.basename str              = Qwen3\nllama_model_loader: - kv   6:                         general.size_label str              = 4B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   9:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv  10:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv  11:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv  12:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  13:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  14:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  16:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  17:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  19:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - kv  31:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_embd_inp       = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen3 4B Instruct 2507\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\n.........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_seq     = 8192\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_seq (8192) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =  1152.00 MiB\nllama_kv_cache: size = 1152.00 MiB (  8192 cells,  36 layers,  1/1 seqs), K (f16):  576.00 MiB, V (f16):  576.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   306.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 580107157\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 2048, n_predict = 400, n_keep = 0\n\ncommon_perf_print:    sampling time =     153.15 ms\ncommon_perf_print:    samplers time =      22.72 ms /   733 tokens\ncommon_perf_print:        load time =    1246.41 ms\ncommon_perf_print: prompt eval time =   21185.96 ms /   548 tokens (   38.66 ms per token,    25.87 tokens per second)\ncommon_perf_print:        eval time =   19467.19 ms /   184 runs   (  105.80 ms per token,     9.45 tokens per second)\ncommon_perf_print:       total time =   40819.12 ms /   732 tokens\ncommon_perf_print: unaccounted time =      12.81 ms /   0.0 %      (total - sampling - prompt eval - eval) / (total)\ncommon_perf_print:    graphs reused =        183\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                 3821 =  2362 +    1152 +     306                |\nllama_memory_breakdown_print: |   - CPU_REPACK         |                 1683 =  1683 +       0 +       0                |\n",
    "raw_model_output": "Europe minister David Lidington has expressed strong opposition to the High Speed 2 (HS2) rail project, citing concerns over its soaring cost, reaching £42.6 billion, and noise levels affecting his Buckinghamshire constituents. He criticized the noise study, claiming it underestimated the impact, and suggested the project should be scrapped due to economic and environmental concerns. Despite being a minister tasked with supporting the project, he voiced constituents' doubts about the budget and its justification in the current economic climate. Other senior figures, including Owen Paterson and Cheryl Gillan, also express skepticism, though the government maintains HS2 will create economic benefits and support around 100,000 jobs. The first stage of HS2 will begin in 2017, with completion expected by 2026, and the second stage to Manchester and Leeds by 2033."
  },
  {
    "article_number": "3",
    "highlight": "Chris Brown song \"Not My Fault\" appeared online Tuesday .\nSong was recorded three years ago, has nothing to do with Rihanna, say producers .\nBrown agreed to a plea deal Monday in case involving ex-girlfriend Rihanna .",
    "generated_summary": "A Chris Brown song titled 'Not My Fault' was recorded three years ago and is not about Rihanna, according to producers and his record label. The song, which includes the line 'It's not my fault, right?', appeared online a day after Brown pleaded guilty to assaulting Rihanna. Jive Records confirmed the song is an old demo and not new material, and warned that it was being falsely promoted. Brown accepted a plea deal avoiding jail time, but any sign of lack of remorse could hurt his sentencing in August.",
    "article_text": "LOS ANGELES, California (CNN) -- A Chris Brown song called \"Not My Fault\" was recorded three years ago and is not about Rihanna, its producers said. Chris Brown pleaded guilty in an assault case regarding his ex-girlfriend Rihanna. The music appeared online Tuesday, a day after the singer admitted guilt to assaulting Rihanna, his former girlfriend. Brown's record label also said the music was not \"new.\" Although Brown accepted a plea deal in which he could avoid jail time, any appearance that he is not remorseful could be damaging, since the judge is not bound by those terms when she sentences him in August. The song, which appeared on YouTube, includes the line \"It's not my fault, right?\" The publicist for the Neptunes, a music producing duo that has worked in the studio with Brown, said the music was a demo recorded three years ago. She said they do not know how the song made its way onto YouTube. Jive Records, which distributes Brown's music, said \"there are no 'new' songs from Chris Brown currently being promoted by Chris or his record label.\" \"There are several old demos circulating that are being falsely promoted as new material from Chris,\" a Jive Records statement said. In the song, Brown sings: . \"I picked up the paper and the headline reads, it says 'singer brokenhearted in some satin sheets.' And I'm like, why? We had a hell of a time.\" \"When I first met you, I told ya that you'll be safe. In the midst of arguing something, falls to the floor and breaks. You look down and see what you dropped, oh, it's your heart.\" \"Shortie's caught up from a long night. She's ready to fall but she's all right. That's just something that can happen when you put it down. She's caught up, it's not my fault, right.\" \"You see, her mama called me, really steamed. She said 'My daughter won't eat and she don't sleep. She just cries.' \" Brown was arrested in connection with an altercation that began with an argument with Rihanna inside a rented Lamborghini on a Hollywood street in February. Prosecutors agreed to recommend five years of probation and 180 days of community service for Brown in exchange for his guilty plea on one count of assault with the intent of doing great bodily injury.",
    "generation_logs": "build: 7193 (d82b7a7c1) with Android (13989888, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-w64-windows-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 32 key-value pairs and 398 tensors from /data/local/tmp/gguf/qwen3-4b-instruct-2507-q4km.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 4B Instruct 2507\nllama_model_loader: - kv   3:                            general.version str              = 2507\nllama_model_loader: - kv   4:                           general.finetune str              = Instruct\nllama_model_loader: - kv   5:                           general.basename str              = Qwen3\nllama_model_loader: - kv   6:                         general.size_label str              = 4B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   9:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv  10:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv  11:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv  12:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  13:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  14:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  16:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  17:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  19:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - kv  31:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_embd_inp       = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen3 4B Instruct 2507\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\n.........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_seq     = 8192\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_seq (8192) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =  1152.00 MiB\nllama_kv_cache: size = 1152.00 MiB (  8192 cells,  36 layers,  1/1 seqs), K (f16):  576.00 MiB, V (f16):  576.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   306.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 2111798134\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 2048, n_predict = 400, n_keep = 0\n\ncommon_perf_print:    sampling time =     179.60 ms\ncommon_perf_print:    samplers time =      15.76 ms /   629 tokens\ncommon_perf_print:        load time =    1296.73 ms\ncommon_perf_print: prompt eval time =   20166.82 ms /   516 tokens (   39.08 ms per token,    25.59 tokens per second)\ncommon_perf_print:        eval time =   11642.03 ms /   112 runs   (  103.95 ms per token,     9.62 tokens per second)\ncommon_perf_print:       total time =   31995.56 ms /   628 tokens\ncommon_perf_print: unaccounted time =       7.11 ms /   0.0 %      (total - sampling - prompt eval - eval) / (total)\ncommon_perf_print:    graphs reused =        111\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                 3821 =  2362 +    1152 +     306                |\nllama_memory_breakdown_print: |   - CPU_REPACK         |                 1683 =  1683 +       0 +       0                |\n",
    "raw_model_output": "A Chris Brown song titled 'Not My Fault' was recorded three years ago and is not about Rihanna, according to producers and his record label. The song, which includes the line 'It's not my fault, right?', appeared online a day after Brown pleaded guilty to assaulting Rihanna. Jive Records confirmed the song is an old demo and not new material, and warned that it was being falsely promoted. Brown accepted a plea deal avoiding jail time, but any sign of lack of remorse could hurt his sentencing in August."
  },
  {
    "article_number": "4",
    "highlight": "Obama says budget calls for $25 billion increase in VA funding over next five years .\nHomeless veterans will be targeted for support, Obama says .\nDramatically improved services planned for mental health, PTSD, brain injury .",
    "generated_summary": "President Obama committed to significantly improving mental health services and expanding VA funding by $25 billion over five years, aiming to serve 500,000 more veterans by 2013, with a focus on veterans with PTSD, TBI, and homeless veterans. He emphasized that the country owes veterans a home, and pledged to ensure no veteran sleeps on the streets.",
    "article_text": "WASHINGTON (CNN) -- President Obama pledged Monday to make good on his promise to transform the Department of Veterans Affairs and said he would \"dramatically improve\" mental health aid. President Obama and Veterans Affairs Secretary Eric Shinseki, seen here last month, vow to increase aid. Flanked by Veterans Affairs Secretary Eric Shinseki, the president said his budget calls for a $25 billion increase in funding for the VA over the next five years -- a commitment that will be tested by the needs of veterans returning from Iraq and Afghanistan. \"With this budget, we don't just fully fund our Veterans Affairs health care program, we expand it to serve an additional 500,000 veterans by 2013,\" he said. He promised that the VA would \"dramatically improve services\" related to mental health, post-traumatic stress disorder and traumatic brain injury, and he said homeless veterans would be targeted for support. \"Those heroes have a home,\" Obama said. \"It's the country they served, the United States of America, and until we reach a day when not a single veteran sleeps on our nation's streets, our work remains unfinished.\"",
    "generation_logs": "build: 7193 (d82b7a7c1) with Android (13989888, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-w64-windows-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 32 key-value pairs and 398 tensors from /data/local/tmp/gguf/qwen3-4b-instruct-2507-q4km.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 4B Instruct 2507\nllama_model_loader: - kv   3:                            general.version str              = 2507\nllama_model_loader: - kv   4:                           general.finetune str              = Instruct\nllama_model_loader: - kv   5:                           general.basename str              = Qwen3\nllama_model_loader: - kv   6:                         general.size_label str              = 4B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   9:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv  10:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv  11:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv  12:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  13:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  14:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  16:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  17:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  19:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - kv  31:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_embd_inp       = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen3 4B Instruct 2507\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\n.........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_seq     = 8192\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_seq (8192) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =  1152.00 MiB\nllama_kv_cache: size = 1152.00 MiB (  8192 cells,  36 layers,  1/1 seqs), K (f16):  576.00 MiB, V (f16):  576.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   306.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 1671461703\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 2048, n_predict = 400, n_keep = 0\n\ncommon_perf_print:    sampling time =     226.00 ms\ncommon_perf_print:    samplers time =      10.20 ms /   352 tokens\ncommon_perf_print:        load time =    1276.22 ms\ncommon_perf_print: prompt eval time =    9170.42 ms /   264 tokens (   34.74 ms per token,    28.79 tokens per second)\ncommon_perf_print:        eval time =    8099.91 ms /    87 runs   (   93.10 ms per token,    10.74 tokens per second)\ncommon_perf_print:       total time =   17500.89 ms /   351 tokens\ncommon_perf_print: unaccounted time =       4.56 ms /   0.0 %      (total - sampling - prompt eval - eval) / (total)\ncommon_perf_print:    graphs reused =         86\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                 3821 =  2362 +    1152 +     306                |\nllama_memory_breakdown_print: |   - CPU_REPACK         |                 1683 =  1683 +       0 +       0                |\n",
    "raw_model_output": "President Obama committed to significantly improving mental health services and expanding VA funding by $25 billion over five years, aiming to serve 500,000 more veterans by 2013, with a focus on veterans with PTSD, TBI, and homeless veterans. He emphasized that the country owes veterans a home, and pledged to ensure no veteran sleeps on the streets."
  },
  {
    "article_number": "5",
    "highlight": "The women have wombs transplanted by their own mothers .\nThe world’s first womb swap baby, Vincent, has already been delivered .\nDoctors say the pregnancies are going well and the mothers are 'excited'\nThe news gives hope to Britain’s estimated 15,000 childless women .",
    "generated_summary": "Two women in Sweden are weeks away from giving birth using transplanted wombs donated by their own mothers, marking a groundbreaking advancement in reproductive medicine. The world’s first womb transplant baby, Vincent, was delivered successfully using a donor womb from a 61-year-old woman, and the technique has now been shown to work with live donors, including those past menopause. The success offers hope to thousands of childless women globally, including in the UK, where a project is preparing to begin using donated wombs from deceased donors. While the procedure is hailed as revolutionary, experts emphasize its complexity and the need for strict safety and ethical oversight before wider adoption.",
    "article_text": "Two women are just weeks away from giving birth – using transplanted wombs donated by their own mothers. The expectant mums will be the first to carry children using the very uterus that carried them as unborn infants. The pregnancies are part of a revolutionary Swedish project to allow childless women to fulfil their dream of starting a family. The world’s first womb swap baby, named as Vincent, has already been delivered using an unrelated donor. Now doctors are preparing for two more arrivals – but this time each one is being nurtured in its grandmother’s womb. Scroll down for video . World first: Vincent was nurtured in a donated womb.  It has now emerged that two women are just weeks away from giving birth – using transplanted wombs donated by their own mothers . The news gives hope to Britain’s estimated 15,000 childless women and provides a boost for a similar project in the UK in which 60 women are waiting for a new womb, Pictured, Vincent and his mother after his delivery . The extraordinary procedure, which doctors have likened in importance to the first successful heart transplant, means each womb will have carried two generations of the same family. Dr Liza Johannesson, of Sahlgrenska University Hospital in Gothenburg, said the pregnancies are going well and the mothers are ‘really excited’ at seeing their babies, which are due next month. ‘It is also really exciting to have your mother as a donor,’ she added. ‘It is a very nice gift to give to your daughter.’ The news gives hope to Britain’s estimated 15,000 childless women and provides a boost for a similar project in the UK in which 60 women are waiting for a new womb. Many women will have been born without the organ, others will have had a hysterectomy because of cancer or as a last-ditch attempt stop life-threatening bleeding during labour. Eventually, the technique could also be used on women who have suffered the agony of repeated miscarriages. Using a donated womb also means that expectant mothers can have babies that are genetically their own and experience the joys of pregnancy. Doctors in Turkey and Saudi Arabia have carried out womb transplants before – but none has so far led to a birth. Teams in the US, China and Australia are also keen to start their own programmes. The Swedes have carried out nine womb transplants – and seven of the women have had IVF treatment. Late on Friday night, it emerged that one had been successful – and the world’s first womb transplant baby was delivered. He has been called Vincent – a name derived from the Latin for ‘to conquer’ – to mark the extraordinary lengths his mother undertook to have him. Pernilla Dahm Kähler (left) has spent 15 years perfecting the complex surgical techniques involved in a womb transplant, said she felt ‘humble and proud’ to witness the birth. Consultant gynaecologist Dr Liza Johannesson (right), of Sahlgrenska University Hospital in Gothenburg, said the pregnancies are going well . (Left to right) Specialist surgeons Andreas G Tzakis, Pernilla Dahm-Kähler, Mats Brannstrom, Michael Olausson and Liza Johannesson attend a news conference, at Sahlgrenska hospital in Gothernburg, Sweden . Born by C-section two months prematurely, Vincent weighed just under 4lb and entered the world with a scream, bringing tears of joy to those who witnessed his arrival. His 36-year-old mother, who worked until the day before he was born, said: ‘As soon as I felt this perfect baby boy on my chest, I had tears of happiness and enormous relief. ‘I felt like a mother the first time I touched my baby and was amazed that we finally did it. ‘I have always had this large sorrow because I never thought I would be a mother – and now the impossible has become real.’ A keen athlete, she was devastated when she was told at the age of 15 that she did not have a womb. She was able to carry her own child after a 61-year-old friend offered to donate her womb. Despite being long past menopause, her womb worked normally during the pregnancy. Consultant gynaecologist Dr Johannesson described Vincent’s birth, which took place a month ago but has remained a secret until now, as ‘breathtaking’. ‘It was like having your own child. It was the same feeling – no one could really believe it.’ The doctor, who was pregnant with her third child at the same time as Vincent’s mother, said: ‘I was more involved in that pregnancy than in my own.’ Vincent is now back home with his parents, who have not been named. He is breastfeeding and growing well. Dr Johannesson added: ‘They are doing perfect. The parents told me that first night at home was horrible because he was screaming – like any other child.’ Vincent’s father said: ‘We now have the most amazing baby. He’s no different from any other child, but he will have a good story to tell.’ Mats Brannstrom and his team performing a womb transplant in Gothenburg, Sweden . The first British transplants could be carried out as soon as next summer – using wombs from dead donors. Richard Smith, head of the charity Womb Transplant UK, is ready to give five women the chance to have children using the procedure and is applying for ethical approval. Some 60 potential patients have already come forward. But Mr Smith, who has already spent significant amounts of his own money on years of research, only has £40,000 of the £500,000 needed for the surgery. Unlike the Swedish project in which living donors are used, the consultant gynaecological surgeon plans to use wombs from women who have died young. He said he is uncomfortable about removing a woman’s womb for an operation that is not essential to save a life. He claims taking the organ from a dead donor will allow him to harvest extra tissue and the major blood vessels needed to take the strain of pregnancy. One of his potential patients, Sophie Lewis, said: ‘To feel your own child growing inside you would be a miracle.’ The 30-year-old, who said she burst into tears of joy when she heard of Vincent’s birth in Sweden, added. ‘It would be an absolute gift, the most amazing gift ever.’ Mr Smith added: ‘In many women, there is a deep yearning to carry children and this is not fulfilled by surrogacy. ‘I’ve had my own crisis with this project over the years – are we doing the right thing? ‘But when you meet women in this position, I know in my heart of hearts that if we do it safely, it is the right thing.’ Henrik Hagberg, a professor in foetal medicine at King’s College London, who did ultrasound scans of Vincent throughout the pregnancy, said: ‘I was quite astounded that everything went so well. I think that’s quite fantastic.’ Pernilla Dahm Kähler, who along with Mats Brännström of Gothenburg University has spent 15 years perfecting the complex surgical techniques involved in a womb transplant, said she felt ‘humble and proud’ to witness the birth. Dr Dahm Kähler helped remove the donor womb from the 61-year-old woman – and transplant it in Vincent’s mother. Professor Brännström said the birth provides scientific evidence that the technique works, adding: ‘It gives us scientific evidence that the concept of uterus transplantation can be used to treat uterine factor infertility, which up to now has remained the last untreatable form of female infertility. ‘It also shows that transplants with a live donor are possible, including if the donor is past menopause.’ Dr Allan Pacey, chairman of the British Fertility Society, likened the operations to the first heart transplant. He added: ‘I think it is quite revolutionary. It feels like it did when IVF was developed or when the first heart transplant was done in the 1960s. ‘It is a bit of a game-changer. The question is can it be done repeatedly, reliably and safely.’ Dr Geeta Nargund, medical director of the Create fertility clinic in London, said: ‘This is a significant medical breakthrough and I congratulate the highly-skilled team behind it.’ However, she said the complexity of the surgery means it will be restricted to a few specialist hospitals.",
    "generation_logs": "build: 7193 (d82b7a7c1) with Android (13989888, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-w64-windows-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 32 key-value pairs and 398 tensors from /data/local/tmp/gguf/qwen3-4b-instruct-2507-q4km.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 4B Instruct 2507\nllama_model_loader: - kv   3:                            general.version str              = 2507\nllama_model_loader: - kv   4:                           general.finetune str              = Instruct\nllama_model_loader: - kv   5:                           general.basename str              = Qwen3\nllama_model_loader: - kv   6:                         general.size_label str              = 4B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   9:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv  10:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv  11:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv  12:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  13:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  14:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  16:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  17:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  19:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - kv  31:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_embd_inp       = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen3 4B Instruct 2507\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\n.........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_seq     = 8192\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_seq (8192) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =  1152.00 MiB\nllama_kv_cache: size = 1152.00 MiB (  8192 cells,  36 layers,  1/1 seqs), K (f16):  576.00 MiB, V (f16):  576.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   306.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 1210919241\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 2048, n_predict = 400, n_keep = 0\n\ncommon_perf_print:    sampling time =     189.77 ms\ncommon_perf_print:    samplers time =      16.19 ms /  1925 tokens\ncommon_perf_print:        load time =    1295.95 ms\ncommon_perf_print: prompt eval time =  113323.05 ms /  1783 tokens (   63.56 ms per token,    15.73 tokens per second)\ncommon_perf_print:        eval time =   22316.77 ms /   141 runs   (  158.27 ms per token,     6.32 tokens per second)\ncommon_perf_print:       total time =  135845.41 ms /  1924 tokens\ncommon_perf_print: unaccounted time =      15.82 ms /   0.0 %      (total - sampling - prompt eval - eval) / (total)\ncommon_perf_print:    graphs reused =        139\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                 3821 =  2362 +    1152 +     306                |\nllama_memory_breakdown_print: |   - CPU_REPACK         |                 1683 =  1683 +       0 +       0                |\n",
    "raw_model_output": "Two women in Sweden are weeks away from giving birth using transplanted wombs donated by their own mothers, marking a groundbreaking advancement in reproductive medicine. The world’s first womb transplant baby, Vincent, was delivered successfully using a donor womb from a 61-year-old woman, and the technique has now been shown to work with live donors, including those past menopause. The success offers hope to thousands of childless women globally, including in the UK, where a project is preparing to begin using donated wombs from deceased donors. While the procedure is hailed as revolutionary, experts emphasize its complexity and the need for strict safety and ethical oversight before wider adoption."
  },
  {
    "article_number": "6",
    "highlight": "Accident investigators respond to critics who claim missile fire brought down the jet in 1996 .\nThe National Transportation Safety Board blamed the crash on a fuel tank explosion .\nSkeptics have asked for a new investigation and are releasing a documentary on the crash .\nTWA 800 crashed into the Atlantic after taking off from New York on a flight to Paris .",
    "generated_summary": "More than a decade after the TWA Flight 800 crash, senior U.S. accident investigators have defended their conclusion that a fuel tank explosion, not a missile or bomb, caused the disaster. The NTSB cited evidence such as the absence of pitting, cratering, or gas jetting on wreckage, and radar data showing no missile intercept, to support their findings. Critics, including a former investigator and a documentary producer, have questioned the results and petitioned for a re-investigation, but the board maintains the original investigation was thorough and complete. The NTSB emphasized that explosive residue found on parts was likely from contamination by recovery vessels, not a bomb. Family members of victims expressed support for the NTSB's conclusions, though some still believe in a conspiracy.",
    "article_text": "More than a decade after concluding a fuel tank explosion destroyed TWA Flight 800, senior U.S. accident investigators stood their ground on Tuesday against skeptics claiming new evidence suggests missile fire downed the jumbo jet. \"I'm totally convinced there was no bomb or missile,\" National Transportation Safety Board investigator Jim Wildey told reporters at a briefing outside Washington. Skeptics, including a former NTSB investigator, last month petitioned the agency to re-open its investigation and will release a documentary on the crash this month. Conspiracy theories, outrage swirl around TWA 800 plane . They said evidence suggests one or more missiles destroyed the Paris-bound Boeing 747 over the Atlantic shortly after takeoff from New York on July 17, 1996, killing all 230 on board. The safety board has responded cautiously, saying it is having investigators who were not part of the original investigation review the petition. And it is seeking comment from Boeing, the Federal Aviation Administration, and other parties to the original probe. But at the same time, the board allowed the original TWA 800 investigators to brief the media on the four-year investigation. Under orders to address only matters in the original investigation, the investigators nonetheless emphasized topics that have come under question by the critics. They held up two sheet metal plates showing examples of bomb and high-velocity missile damage, and then took reporters to view TWA 800's reconstructed fuselage. They said they found no pitting of the type alleged by those challenging the findings. \"You don't see fragmented seat parts,\" said investigator Robert Swaim. Nor is there evidence of pitting, cratering or \"gas jetting\" -- tell-tale marks left by high explosives -- he said. Plane reconstructed . Investigators said they were unable to find any evidence on radar of a missile intercepting the plane. And they discounted the significance of traces of explosives found on more than 100 airplane parts pulled from the ocean. The NTSB said tests showed that explosive residue washes off after only two days in the ocean, leading them to conclude the residue likely was the result of contamination from the military and law enforcement boats that recovered wreckage. The NTSB allowed two TWA 800 family members to attend the briefing, and the family members reiterated their support for the board's findings. Matt Ziemkiewicz, whose sister Jill, a 23-year-old flight attendant, was killed in the explosion, said the briefing \"solidified\" his belief that the original investigation was \"complete, accurate and thorough.\" Jim Hurd, whose son Jamie, 29, died on the flight, also endorsed the NTSB finding. \"At first I was of course skeptical as everybody else. You know, you just didn't know what happened. As things unfolded, it seemed to make a lot of sense, what happened to the plane,\" he said. \"It's really cut and dry, at this point, as far as I'm concerned.\" Ziemkiewicz and Hurd said the new attention to the crash is opening old wounds. \"It's really a shame in my book,\" Hurd said. \"There are still family members who believe it was brought down by a bomb and I respect their thoughts and however they want to view it. It's just the way you take the information and how you process it and who you believe. And I believe the NTSB has done a job and I don't believe they've covered up anything and I don't believe the FBI did either.\" The NTSB did not allow their critics into the media briefing. They talked to reporters in a parking lot outside. Opinion: Flight 800 conspiracy? Where's the proof? 'A matter of conscience' Hank Hughes, a former NTSB investigator who now doubts the board's conclusion, said he is motivated only by a desire to uncover the truth. Hughes said following the accident, he spent two days with Hurd. \"That's the last guy in the world I'd want to hurt,\" he said. \"There's no profit motive. It's a matter of conscience and integrity as far as we're concerned. Two hundred and thirty people were killed and that's a tragedy that can't be ignored,\" Hughes said. Hughes is featured in the documentary, called \"TWA Flight 800,\" co-produced by longtime skeptic Tom Stalcup. The NTSB investigators on Tuesday repeated a scenario the safety board endorsed in 2000. The series of events leading to TWA 800's disastrous final flight began when it was delayed at New York's John F. Kennedy airport. While the plane was sitting at the gate, the crew engaged the plane's air conditioner to keep passengers cool. The air conditioning units, low in the plane's fuselage, heated the large, nearly empty fuel tank above it, creating a volatile fuel/air mixture. Ten minutes after take-off, when the plane was at 13,700 feet and climbing, an electrical short that likely occurred in a wire bundle pushed a high-voltage current into the fuel tank, igniting it, and destroying the plane. Divers found three overlapping fields of debris at the bottom of the ocean. The first included the pieces of the fuel tank and the cabin above it. A second field contained the nose and cockpit. The final and largest debris field contained the rest of the plane. Investigators said they worked hard to determine if a bomb or missile was involved, especially in light of the event's timing two days before the Summer Olympics and 10 days before the Olympic bombing in Atlanta. \"That really intensified the investigation,\" said investigator Joseph Kolly. A cockpit voice recorder analysis showed a sound spectrum consistent with a low-explosive fuel tank explosion, not a high-explosive missile. Eventually, investigators ruled out a missile or bomb, as well as lightning, meteor, electro-magnetic pulse, and other possible causes, Kolly said. Kolly acknowledged some friction between the NTSB and the FBI during the investigation, but said \"it didn't ultimately harm the investigation.\" Kolly said the petition has not prompted him to rethink the board's 2000 findings. Since the petition was filed, Kolly said, he has re-read this report \"a couple more times and it's pretty thorough.\" Five facts to calm plane crash fears .",
    "generation_logs": "build: 7193 (d82b7a7c1) with Android (13989888, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-w64-windows-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 32 key-value pairs and 398 tensors from /data/local/tmp/gguf/qwen3-4b-instruct-2507-q4km.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 4B Instruct 2507\nllama_model_loader: - kv   3:                            general.version str              = 2507\nllama_model_loader: - kv   4:                           general.finetune str              = Instruct\nllama_model_loader: - kv   5:                           general.basename str              = Qwen3\nllama_model_loader: - kv   6:                         general.size_label str              = 4B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   9:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv  10:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv  11:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv  12:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  13:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  14:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  16:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  17:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  19:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - kv  31:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_embd_inp       = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen3 4B Instruct 2507\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\n.........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_seq     = 8192\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_seq (8192) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =  1152.00 MiB\nllama_kv_cache: size = 1152.00 MiB (  8192 cells,  36 layers,  1/1 seqs), K (f16):  576.00 MiB, V (f16):  576.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   306.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 1998415270\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 2048, n_predict = 400, n_keep = 0\n\ncommon_perf_print:    sampling time =     714.85 ms\ncommon_perf_print:    samplers time =      25.19 ms /  1529 tokens\ncommon_perf_print:        load time =    1263.72 ms\ncommon_perf_print: prompt eval time =   73969.12 ms /  1342 tokens (   55.12 ms per token,    18.14 tokens per second)\ncommon_perf_print:        eval time =   26551.03 ms /   186 runs   (  142.75 ms per token,     7.01 tokens per second)\ncommon_perf_print:       total time =  101248.96 ms /  1528 tokens\ncommon_perf_print: unaccounted time =      13.97 ms /   0.0 %      (total - sampling - prompt eval - eval) / (total)\ncommon_perf_print:    graphs reused =        185\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                 3821 =  2362 +    1152 +     306                |\nllama_memory_breakdown_print: |   - CPU_REPACK         |                 1683 =  1683 +       0 +       0                |\n",
    "raw_model_output": "More than a decade after the TWA Flight 800 crash, senior U.S. accident investigators have defended their conclusion that a fuel tank explosion, not a missile or bomb, caused the disaster. The NTSB cited evidence such as the absence of pitting, cratering, or gas jetting on wreckage, and radar data showing no missile intercept, to support their findings. Critics, including a former investigator and a documentary producer, have questioned the results and petitioned for a re-investigation, but the board maintains the original investigation was thorough and complete. The NTSB emphasized that explosive residue found on parts was likely from contamination by recovery vessels, not a bomb. Family members of victims expressed support for the NTSB's conclusions, though some still believe in a conspiracy."
  },
  {
    "article_number": "7",
    "highlight": "Tahoe Cross Country Ski Area allows dogs on some trails .\nDescendants of Maria von Trapp own a lodge in Vermont .\nA lodge in Ontario offers skiers an escape into the wilderness .",
    "generated_summary": "Cross-country skiing offers a less intense, more calorie-burning alternative to downhill skiing, combining physical activity with nature immersion. CNN highlights five top destinations: Tahoe Cross Country Ski Area (California/Nevada) with scenic trails, dog-friendly areas, and discounted passes; Trapp Family Lodge (Vermont) with Austrian-inspired charm and wellness packages; Methow Valley (Washington) with a unique trail network and local hangouts; Stokely Creek Lodge (Ontario) providing wilderness escapes and group discounts; and the Cable Area (Wisconsin), known as a skiing mecca with affordable passes and extensive trail access. Each location features unique cultural, scenic, and logistical benefits for skiers seeking both adventure and relaxation.",
    "article_text": "(CNN) -- Cross-country skiing is less speedy -- and therefore less intimidating -- than its steep-sloped cousin, downhill skiing. It also burns more calories, as skiers glide along snowy trails and skate up inclines. Trail passes at Tahoe Cross Country Ski Area are $21 for adults -- but ask about specials. So, for those of you trying to stay true to your 2009 resolutions, the sport may be the perfect way to get more exercise and take in a dose of nature in the process. CNN asked Ron Bergin, publisher of Cross Country Skier magazine, to recommend five places to hit the horizontal slopes. Tahoe Cross Country Ski Area, Lake Tahoe, California . The area around Lake Tahoe, which straddles the border between California and Nevada, is ripe with cross-country skiing opportunities. So much so, said Kevin Murnane, general manager of the Tahoe Cross Country Ski Area, that \"you can spend a whole week here and ski at a different area every day.\" Bergin said the lakeside scenery is top-notch, as is the skier culture. You likely will find buddies who will share trail stories and a drink after a long day on the trails. Murnane's ski area allows dogs on some trails and offers free lessons to help learners adjust to the balance-intensive sport. Trail passes at Tahoe Cross Country Ski Area are $21 for adults, but ask about specials. On Tuesdays, passes run $12. Groups and early-morning skiers also get a discount. Trapp Family Lodge, Stowe, Vermont . Owned by descendants of Maria von Trapp -- yes, that Trapp family, whose life inspired \"The Sound of Music\" -- the Trapp Family Lodge brings a little piece of Austria's lively hills to the mountains of Vermont. Johannes von Trapp, the real Maria's youngest son, helped open the lodge in 1968. The family had been in the lodging business before the musical film starring Julie Andrews debuted in 1965. For Bergin, the history itself is a draw. So is the fact that the luxury lodge, which still pulls off a quaint charm, is dedicated to cross-country skiing. \"It's beautiful. It backs up to the mountains and you've got a great view of a lot of trails,\" Bergin said. The lodge offers a range of promotions, from wine-and-dine packages to those geared toward \"winter wellness.\" The lodge offers rooms and chalets ranging from $275 to $475 per night. Go for the two-night package and you'll get ski rentals, breakfast, dinner and lodging for both days for $500 per person. Methow Valley, Winthrop, Washington . Northeast of Seattle in the Cascade Mountains, nearly 125 miles of ski trails connect the villages of Methow Valley. It's almost as if cross-country skiing becomes a form of transportation in and around the area, Bergin said. While the linear path is \"not a typical layout\" compared to most cross-country loops, Bergin said the trails offer a sense of adventure and exploration. At one end, you'll find The Sun Mountain Lodge, which is the area's luxurious gem. But the valley also has hangouts for the Bohemian traveler: The Old Schoolhouse Brewery sells $4 pints in a smart red building and the local food cooperative offers organic smoothies and sandwiches. Trail passes run $20 for adults and $10 for teens ages 13 to 17. Stokely Creek Lodge, Goulais River, Ontario, Canada . Located in Ontario, Canada, north of Michigan's Upper Peninsula, this remote lodge gives skiers a serene escape from the bustle of everyday life. \"You ski away from the lodge and immediately you feel like you're out in the wilderness,\" Bergin said. \"All of a sudden, you're just transported from the day-to-day.\" The natural scenery -- more than 12 square miles of it -- is varied and always exciting, he said. Around some curves, skiers will find a cold creek. Other areas are known for hills or lakes that seem like they're in the middle of nowhere. The lodge offers $15 day passes for groups of ten or more; kids 18 and younger pay $10 and adult passes are $18. For the full treatment, though, two adults can stay the night at rates ranging from $98 to $170. That includes two ski passes and three meals, which, according to lodge manager Jamie Martin, are almost as big a hit as the skiing. The kitchen serves highbrow treats like filet mignon and Cornish hen, he said. Cable Area, Cable, Wisconsin . Northern Wisconsin is proof that you don't need huge mountains for good skiing. The Cable Area -- which includes the towns of Cable, Drummond, Namakagon and Grand View -- boasts one of the world's biggest cross-country ski races and is regarded by some as the birthplace for the sport in North America, Bergin said. Bergin, who lives in the area, called the Cable Area the \"Mecca\" for cross-country skiing in the region. \"Within the radius of an hour's drive, you could probably find 30 or 40 trail systems of all sizes and shapes,\" he said. \"There's quite a skiing culture.\" The Telemark Resort offers trail passes that are cheaper than most: $11 per day. Lodging at the resort ranges from $79 for a room in the lodge to $275 for a 1,000-square-foot condo, said Darrell Buchmann, the resort's general manager.",
    "generation_logs": "build: 7193 (d82b7a7c1) with Android (13989888, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-w64-windows-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 32 key-value pairs and 398 tensors from /data/local/tmp/gguf/qwen3-4b-instruct-2507-q4km.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 4B Instruct 2507\nllama_model_loader: - kv   3:                            general.version str              = 2507\nllama_model_loader: - kv   4:                           general.finetune str              = Instruct\nllama_model_loader: - kv   5:                           general.basename str              = Qwen3\nllama_model_loader: - kv   6:                         general.size_label str              = 4B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   9:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv  10:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv  11:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv  12:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  13:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  14:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  16:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  17:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  19:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - kv  31:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_embd_inp       = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen3 4B Instruct 2507\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\n.........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_seq     = 8192\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_seq (8192) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =  1152.00 MiB\nllama_kv_cache: size = 1152.00 MiB (  8192 cells,  36 layers,  1/1 seqs), K (f16):  576.00 MiB, V (f16):  576.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   306.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 4036548485\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 2048, n_predict = 400, n_keep = 0\n\ncommon_perf_print:    sampling time =     201.53 ms\ncommon_perf_print:    samplers time =      19.82 ms /  1349 tokens\ncommon_perf_print:        load time =    1268.62 ms\ncommon_perf_print: prompt eval time =   64277.19 ms /  1195 tokens (   53.79 ms per token,    18.59 tokens per second)\ncommon_perf_print:        eval time =   20772.44 ms /   153 runs   (  135.77 ms per token,     7.37 tokens per second)\ncommon_perf_print:       total time =   85265.41 ms /  1348 tokens\ncommon_perf_print: unaccounted time =      14.24 ms /   0.0 %      (total - sampling - prompt eval - eval) / (total)\ncommon_perf_print:    graphs reused =        151\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                 3821 =  2362 +    1152 +     306                |\nllama_memory_breakdown_print: |   - CPU_REPACK         |                 1683 =  1683 +       0 +       0                |\n",
    "raw_model_output": "Cross-country skiing offers a less intense, more calorie-burning alternative to downhill skiing, combining physical activity with nature immersion. CNN highlights five top destinations: Tahoe Cross Country Ski Area (California/Nevada) with scenic trails, dog-friendly areas, and discounted passes; Trapp Family Lodge (Vermont) with Austrian-inspired charm and wellness packages; Methow Valley (Washington) with a unique trail network and local hangouts; Stokely Creek Lodge (Ontario) providing wilderness escapes and group discounts; and the Cable Area (Wisconsin), known as a skiing mecca with affordable passes and extensive trail access. Each location features unique cultural, scenic, and logistical benefits for skiers seeking both adventure and relaxation."
  },
  {
    "article_number": "8",
    "highlight": "Carl Chang, 26, cut his hand while breaking kitchen window during raid .\nClimbed into Maindstone home last year and stole cash and jewellery .\nHe was jailed for four years after court heard he is a prolific offender .",
    "generated_summary": "Carl Chang, 26, has been jailed for four years after a single drop of blood at a Maidstone home helped authorities identify him. The blood was found on a doorstep after he broke into a property on June 27, 2014, using a smashed kitchen window. Despite cleaning the scene, Chang failed to notice the bloodstain, which matched his DNA. Chang, who was on licence after serving time for another burglary, is a known repeat offender. Kent Police praised the forensic detection and emphasized their ongoing efforts to bring repeat offenders to justice.",
    "article_text": "Carl Chang, 26, has been jailed for four years after a single drop of blood left as he broke into a home in Maidstone lead forensic teams to him . A single drop of blood left at the scene of the crime helped cops nail a serial burglar. Carl Chang was in the process of breaking into a home in Maidstone, Kent, on June 27 last year, when he got a tiny nick on his hand as he smashed a kitchen window to sneak in. The lone drop of blood was spotted by forensics officers, who were called to the scene within hours of the offence taking place, in which cash and jewellery were stolen. Despite Chang, 26, cleaning the window and various doors with cleaning fluid in a bid to get rid of his fingerprints and any traces he had been there, he didn't notice to single drop of his blood on the kitchen doorstep by the smashed window. The serial crook, who had only recently been released from jail after serving time for another burglary, was arrested after cops traced his DNA to previous break-ins in the Kent area. When he was arrested at his home in Orpington, Kent, on July 9 last year he lunged at officers in a desperate bid to get away, but was jailed for four years at Maidstone Crown Court on Monday  after pleading guilty to burglary. A spokesman for Kent Police said this week: 'A burglar has been jailed for four years after a drop of his blood was detected on the doorstep of a home in Maidstone. 'Kent Police matched Carl Chang's DNA to a small blood stain, following a break-in last year. 'Chang smashed a kitchen window to enter the property on 27 June 2014. The 26-year-old stole cash and jewellery before fleeing the scene.' He added: 'After the victim alerted police, crime scene investigators attended the address and detected a fresh drop of blood on a step directly outside the point of entry. 'The blood sample was sent off for forensic analysis and was later identified as a match for Chang. Maidstone Crown Court heard Chang is a prolific burglar who was out on licence for another break-in when he raided the Kent home last year, making off with jewellery and cash . 'Inside the property Chang had used cleaning fluid, in an attempt to remove further evidence of his offending.' Detective Constable Natalie Sammells said: 'At the time of the offence Chang had been on license, following his release from prison for another burglary. 'He is clearly a prolific burglar, whose offending has caused a great deal of distress to the victims, not only through the loss of their personal property, but the invasion of their private space. 'We will continue to pursue all forensic techniques available to gather evidence to bring offenders like Chang to justice and I'm pleased that another repeat offender is behind bars and unable to commit any further crimes.'",
    "generation_logs": "build: 7193 (d82b7a7c1) with Android (13989888, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-w64-windows-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 32 key-value pairs and 398 tensors from /data/local/tmp/gguf/qwen3-4b-instruct-2507-q4km.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 4B Instruct 2507\nllama_model_loader: - kv   3:                            general.version str              = 2507\nllama_model_loader: - kv   4:                           general.finetune str              = Instruct\nllama_model_loader: - kv   5:                           general.basename str              = Qwen3\nllama_model_loader: - kv   6:                         general.size_label str              = 4B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   9:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv  10:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv  11:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv  12:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  13:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  14:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  16:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  17:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  19:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - kv  31:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_embd_inp       = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen3 4B Instruct 2507\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\n.........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_seq     = 8192\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_seq (8192) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =  1152.00 MiB\nllama_kv_cache: size = 1152.00 MiB (  8192 cells,  36 layers,  1/1 seqs), K (f16):  576.00 MiB, V (f16):  576.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   306.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 1273899531\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 2048, n_predict = 400, n_keep = 0\n\ncommon_perf_print:    sampling time =     215.89 ms\ncommon_perf_print:    samplers time =      15.47 ms /   741 tokens\ncommon_perf_print:        load time =    1270.95 ms\ncommon_perf_print: prompt eval time =   26655.04 ms /   615 tokens (   43.34 ms per token,    23.07 tokens per second)\ncommon_perf_print:        eval time =   13842.34 ms /   125 runs   (  110.74 ms per token,     9.03 tokens per second)\ncommon_perf_print:       total time =   40722.15 ms /   740 tokens\ncommon_perf_print: unaccounted time =       8.88 ms /   0.0 %      (total - sampling - prompt eval - eval) / (total)\ncommon_perf_print:    graphs reused =        124\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                 3821 =  2362 +    1152 +     306                |\nllama_memory_breakdown_print: |   - CPU_REPACK         |                 1683 =  1683 +       0 +       0                |\n",
    "raw_model_output": "Carl Chang, 26, has been jailed for four years after a single drop of blood at a Maidstone home helped authorities identify him. The blood was found on a doorstep after he broke into a property on June 27, 2014, using a smashed kitchen window. Despite cleaning the scene, Chang failed to notice the bloodstain, which matched his DNA. Chang, who was on licence after serving time for another burglary, is a known repeat offender. Kent Police praised the forensic detection and emphasized their ongoing efforts to bring repeat offenders to justice."
  },
  {
    "article_number": "9",
    "highlight": "Skylar Neese from Star City, West Virginia, was last seen on July 7 when she was spotted on surveillance footage getting into a car outside her home .\nFBI joined investigation and is reportedly interviewing her friends .",
    "generated_summary": "A 16-year-old West Virginia girl, Skylar Neese, was last seen on July 7 entering a friend's car at midnight after returning from a job at Wendy's. Her remains were found in Pennsylvania, less than 30 miles from her home, and positively identified on January 16. Police initially treated her disappearance as a runaway case and did not issue an Amber Alert, despite her parents' concerns. Her family launched a public search, selling T-shirts with her name and picture, raising $3,100 for a reward. The last message on her Twitter account, posted on July 5, read, 'You doing s*** like that is why I will NEVER completely trust you.'",
    "article_text": "By . Daily Mail Reporter . PUBLISHED: . 17:11 EST, 14 March 2013 . | . UPDATED: . 17:20 EST, 14 March 2013 . A 16-year-old West Virginia girl who disappeared last summer has been found dead, less than 30 miles from her home. The remains of Syklar Neese were found on January 16 in rural Wayne Township in Greene County, Pennsylvania, but they were only recently positively identified. The teen, from . Star City, West Virginia, was last seen on July 7 when she was spotted . on surveillance footage getting into a friend's car at midnight. Where did she go? Last time she was seen Skylar returned from shift at Wendy's and hugged parents before saying she was going to bed . Police initially labeled her a runaway and never issued an Amber Alert, despite her parents believe she was in danger. Authorities have released no details about how her body was found or its condition. The last sighting of the brunette teen was when she returned home from her job at Wendy's and hugged her parents before going to bed saying she was tired. Footage from the James Place apartments showed that Skylar climbed out of a window at midnight and got into a waiting car, which police later found out belonged to one of her friends. Skylar's cousin Rikki Woodall, told The Huffington Post: 'The police interviewed the friend and she admitted she picked Skylar up . that night but swears she dropped her off an hour later. The girl said . Skylar was insistent that she be dropped off down the street so her . friend's car did not wake anyone.' Publicity: Skyar's parents have led a very public hunt for their beloved daughter with posters and T-shirts . According to police, there is no surveillance footage of Skyler returning to the apartment complex. 'We . know she left voluntarily from the surveillance tape, but we have not . been able to account for her whereabouts since then,' Star City Police . Chief Vic Propst told Huffington Post. The last message on her Twitter account was posted on July 5. The message reads, 'You doing s*** like that is why I will NEVER completely trust you.' Cryptic: The last message on her Twitter account was posted on July 5. The message reads, 'You doing sh*t like that is why I will NEVER completely trust you' Skylar's family have led a publicity drive to find the missing teen. A . few weeks after her disappearance, her friends and family gathered . outside of the Wal-Mart in the University Town Center in Granville to . sell T-shirts with her name and picture. The money raised by the sale was used as a reward to for anyone with information of her whereabouts - so far it totals $3,100. Skylar is described as a white female, 5'4\" tall, 135 pounds, with brown hair and blue eyes.",
    "generation_logs": "build: 7193 (d82b7a7c1) with Android (13989888, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-w64-windows-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 32 key-value pairs and 398 tensors from /data/local/tmp/gguf/qwen3-4b-instruct-2507-q4km.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 4B Instruct 2507\nllama_model_loader: - kv   3:                            general.version str              = 2507\nllama_model_loader: - kv   4:                           general.finetune str              = Instruct\nllama_model_loader: - kv   5:                           general.basename str              = Qwen3\nllama_model_loader: - kv   6:                         general.size_label str              = 4B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   9:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv  10:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv  11:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv  12:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  13:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  14:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  16:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  17:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  19:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - kv  31:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_embd_inp       = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen3 4B Instruct 2507\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\n.........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_seq     = 8192\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_seq (8192) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =  1152.00 MiB\nllama_kv_cache: size = 1152.00 MiB (  8192 cells,  36 layers,  1/1 seqs), K (f16):  576.00 MiB, V (f16):  576.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   306.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 3304672833\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 2048, n_predict = 400, n_keep = 0\n\ncommon_perf_print:    sampling time =     220.00 ms\ncommon_perf_print:    samplers time =      19.60 ms /   797 tokens\ncommon_perf_print:        load time =    1279.28 ms\ncommon_perf_print: prompt eval time =   28208.25 ms /   641 tokens (   44.01 ms per token,    22.72 tokens per second)\ncommon_perf_print:        eval time =   17623.56 ms /   155 runs   (  113.70 ms per token,     8.80 tokens per second)\ncommon_perf_print:       total time =   46062.56 ms /   796 tokens\ncommon_perf_print: unaccounted time =      10.75 ms /   0.0 %      (total - sampling - prompt eval - eval) / (total)\ncommon_perf_print:    graphs reused =        153\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                 3821 =  2362 +    1152 +     306                |\nllama_memory_breakdown_print: |   - CPU_REPACK         |                 1683 =  1683 +       0 +       0                |\n",
    "raw_model_output": "A 16-year-old West Virginia girl, Skylar Neese, was last seen on July 7 entering a friend's car at midnight after returning from a job at Wendy's. Her remains were found in Pennsylvania, less than 30 miles from her home, and positively identified on January 16. Police initially treated her disappearance as a runaway case and did not issue an Amber Alert, despite her parents' concerns. Her family launched a public search, selling T-shirts with her name and picture, raising $3,100 for a reward. The last message on her Twitter account, posted on July 5, read, 'You doing s*** like that is why I will NEVER completely trust you.'"
  },
  {
    "article_number": "10",
    "highlight": "Elouise Coulson, 42, and her three sons could lose the Preston, Kent, home .\nThe pair have already sold a £1.5m house in Forest Hill, South London .\nDownsized to a £600,000 house near Canterbury after he was charged .",
    "generated_summary": "Eloise Coulson, wife of convicted Andy Coulson, could lose their family home in Preston, Kent, as the Crown Prosecution Service seeks to recover nearly £750,000 in trial costs. The CPS spent £1.7 million to prosecute Coulson, Mrs Brooks and four others, and may use Coulson’s assets, including their home, to cover the costs. Coulson, who earned £475,000 annually as a Conservative communications director, has an indemnity from News International for legal fees, and is facing a retrial over corrupt payments to officials. A two-day hearing will decide whether the firm must cover his share of the CPS costs or ongoing legal fees. Mrs Brooks and four others have requested public funding for their defence costs after being acquitted.",
    "article_text": "Risk: Eloise Coulson, pictured with her convicted husband, could lose the property in Preston Kent . For eight months she has endured the humiliation of her husband’s disgrace and revelations about his affair with Rebekah Brooks. Now Eloise Coulson could face losing her family’s home as the prosecution seek to recoup almost £750,000 in costs for the phone hacking trial. The Crown Prosecution Service spent £1.7million bringing Andy Coulson, Mrs Brooks and four others to trial. Coulson, 46, is hoping to force his former employers in the Rupert Murdoch media empire to pay his share but if he fails the CPS has said it would look to his ‘assets’ to recover its costs. It is understood they could include his family home in Preston, Kent, which he shares with his wife and three young sons. The couple have already sold their £1.5million five-bedroom Victorian home in Forest Hill, South London, after he was charged, and downsized to a £600,000 house near Canterbury. Mrs Coulson, 42, accompanied Coulson to court regularly during the eight-month trial but has not been seen at his side at the Old Bailey since his conviction last week. Coulson – who earned a reported £475,000-a-year as the Conservatives’ director of communications – is also facing a retrial on allegations he made corrupt payments to public officials. He and Mrs Brooks began an affair in 1998 and admitted it had been rekindled at least twice after his marriage to PR consultant Eloise in 2000. Prosecutor Andrew Edis QC said: ‘Mr Coulson has a number of assets which have a financial value. One of them is an indemnity.’ Coulson’s ‘indemnity’ from his ex-employers News International, now News UK, means the firm has been forced to pay his legal bills. He took them to court in August 2011 after they stopped the payments and he successfully argued they had a legal duty to continue to cover his legal fees. It is not clear if the firm will have to cover his share of the CPS costs, or any ongoing legal fees. Costs are due to be decided in a two-day hearing later this month. Case: Coulson - who earned a reported £475,000-a-year as the Conservatives’ director of communications - is also facing a retrial on allegations he made corrupt payments to public officials . So far, only news editor James Weatherup has been ordered to pay a £5,000 share of prosecution costs. A legal source said it would be unusual for a family home to be seized to cover CPS costs, but said the cost of the trial was unusually high. Meanwhile, Mrs Brooks, her husband Charlie, her former assistant Cheryl Carter, former News of the World managing editor Stuart Kuttner and News International security chief Mark Hanna have all asked for their defence costs to be paid by public funds after they were all acquitted. Sorry we are not currently accepting comments on this article.",
    "generation_logs": "build: 7193 (d82b7a7c1) with Android (13989888, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-w64-windows-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 32 key-value pairs and 398 tensors from /data/local/tmp/gguf/qwen3-4b-instruct-2507-q4km.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 4B Instruct 2507\nllama_model_loader: - kv   3:                            general.version str              = 2507\nllama_model_loader: - kv   4:                           general.finetune str              = Instruct\nllama_model_loader: - kv   5:                           general.basename str              = Qwen3\nllama_model_loader: - kv   6:                         general.size_label str              = 4B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   9:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv  10:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv  11:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv  12:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  13:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  14:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  16:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  17:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  19:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - kv  31:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_embd_inp       = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen3 4B Instruct 2507\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\n.........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_seq     = 8192\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_seq (8192) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =  1152.00 MiB\nllama_kv_cache: size = 1152.00 MiB (  8192 cells,  36 layers,  1/1 seqs), K (f16):  576.00 MiB, V (f16):  576.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   306.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 1694117639\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 2048, n_predict = 400, n_keep = 0\n\ncommon_perf_print:    sampling time =     222.95 ms\ncommon_perf_print:    samplers time =      20.57 ms /   834 tokens\ncommon_perf_print:        load time =    1290.25 ms\ncommon_perf_print: prompt eval time =   29539.28 ms /   661 tokens (   44.69 ms per token,    22.38 tokens per second)\ncommon_perf_print:        eval time =   19775.51 ms /   172 runs   (  114.97 ms per token,     8.70 tokens per second)\ncommon_perf_print:       total time =   49548.28 ms /   833 tokens\ncommon_perf_print: unaccounted time =      10.53 ms /   0.0 %      (total - sampling - prompt eval - eval) / (total)\ncommon_perf_print:    graphs reused =        170\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                 3821 =  2362 +    1152 +     306                |\nllama_memory_breakdown_print: |   - CPU_REPACK         |                 1683 =  1683 +       0 +       0                |\n",
    "raw_model_output": "Eloise Coulson, wife of convicted Andy Coulson, could lose their family home in Preston, Kent, as the Crown Prosecution Service seeks to recover nearly £750,000 in trial costs. The CPS spent £1.7 million to prosecute Coulson, Mrs Brooks and four others, and may use Coulson’s assets, including their home, to cover the costs. Coulson, who earned £475,000 annually as a Conservative communications director, has an indemnity from News International for legal fees, and is facing a retrial over corrupt payments to officials. A two-day hearing will decide whether the firm must cover his share of the CPS costs or ongoing legal fees. Mrs Brooks and four others have requested public funding for their defence costs after being acquitted."
  }
]