[
  {
    "article_number": "1",
    "highlight": "The Oscar-winning actress is now almost 97 and living in Paris .\nThe sisters were notorious for their bitter quarrels .\nFriend of De Havilland said she is in mourning and will never forget Joan .",
    "generated_summary": "Olivia de Havilland mourns the loss of her sister Joan Fontaine, both actresses having had nominations for the best actress Oscar in 1941 and feuding throughout their careers. The two were born only 15 months apart and their rivalry seemed to be encouraged by their ambitious stage mother. Olivia, who now lives in Paris, has been overwhelmed by the expressions of sympathy since Joan's death at the age of 96. Both sisters were nominated for the best actress Oscar in 1941, with Joan winning for her role in 'Suspicion'. Their relationship was fraught with tension, including a reported incident where Olivia fractured Fontaine's collarbone. The sisters' strained relationship was well-known, with Fontaine stating she would be angry if she died before Olivia.",
    "article_text": "By . Peter Allen . PUBLISHED: . 09:42 EST, 17 December 2013 . | . UPDATED: . 15:06 EST, 17 December 2013 . Hollywood legend Olivia de Havilland was today 'in mourning' over the death of her equally famous sister Joan Fontaine . Hollywood legend Olivia de Havilland was today ‘in mourning’ over the death of her equally famous sister Joan Fontaine, telling friends: ‘I’ll never forget her’. The Oscar-winning actress is now almost 97, and living in an upmarket apartment block close to the Arc de Triumphe in Paris. She and her sister were notorious for their bitter quarrels, and barely spent any time together during their long and successful screen careers. But on Monday Ms De Havilland issued a statement saying she was ‘shocked and saddened’ by Joan’s death on Sunday at the age of 96. ‘Joan was her kid sister – of course she’s hugely sad at her passing,’ said one of Ms De Havilland’s friends in the French capital. ‘She’s certainly in mourning and has made it clear that she will never forget Joan.’ Olivia has been bombarded with flowers and letters since Sunday night, and said in her statement that she was overwhelmed by ‘the many kind expressions of sympathies.’ Ms De Havilland and Ms Fontaine, who lived in California at the time of her death, were among Hollywood's most famous sisters. Both were nominated for the best actress Oscar in 1941, and Ms Fontaine won it for her role in the film ‘Suspicion’. Scroll down for video . Long feud: Sisters Olivia De Havilland and Joan Fontaine were born just 15 months apart . Recalling the awards ceremony many years . later, Ms Fontaine said: ‘All the animus we'd felt toward each other as . children, the hair pullings, the savage wrestling matches, the time . Olivia fractured my collarbone, all came rushing back in kaleidoscopic . imagery. ‘My paralysis was total. I felt Olivia would spring across the table and grab me by the hair. Left to right: Fontaine opposite Laurence Olivier in Rebecca (he had wanted the part of the second Mrs de Winter to go to his real life soon-to-be wife, Vivien Leigh); and with Orson Welles in Jane Eyre . 'I felt age 4, being confronted by my older sister. Damn it, I'd incurred her wrath again!’ Ms . De Havilland is hugely respected in France, and was awarded the Legion . of Honour by the then President Nicolas Sarkozy in 2010. She won two Oscars in all, including one for her part in in the 1947 film ‘To Each His Own’. Joan Fontaine (left) said the sibling rivalry was encouraged by their ambitious stage mother. It may have also had something to do with their similarity (the photo on the right, for example, is of Olivia de Havilland) Olivia de Havilland pictured with  Frederic Mitterrand at the 36th Cesar French Film Awards in Paris in 2011 . When . Fontaine tried to congratulate her for the 1947 award, Ms De Havilland . ignored her, with a spokesman saying: ‘This goes back for years and . years, ever since they were children.’ Ms . Fontaine once told The Hollywood Reporter: ‘I married first, won the . Oscar before Olivia did, and if I die first, she’ll undoubtedly be livid . because I beat her to it.’",
    "generation_logs": "build: 7193 (d82b7a7c1) with Android (13989888, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-w64-windows-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /data/local/tmp/gguf/phi-3.5-mini-instruct.Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = phi3\nllama_model_loader: - kv   1:              phi3.rope.scaling.attn_factor f32              = 1.190238\nllama_model_loader: - kv   2:                               general.type str              = model\nllama_model_loader: - kv   3:                               general.name str              = Phi 3.5 Mini Instruct\nllama_model_loader: - kv   4:                           general.finetune str              = instruct\nllama_model_loader: - kv   5:                           general.basename str              = Phi-3.5\nllama_model_loader: - kv   6:                         general.size_label str              = mini\nllama_model_loader: - kv   7:                            general.license str              = mit\nllama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...\nllama_model_loader: - kv   9:                               general.tags arr[str,3]       = [\"nlp\", \"code\", \"text-generation\"]\nllama_model_loader: - kv  10:                          general.languages arr[str,1]       = [\"multilingual\"]\nllama_model_loader: - kv  11:                        phi3.context_length u32              = 131072\nllama_model_loader: - kv  12:  phi3.rope.scaling.original_context_length u32              = 4096\nllama_model_loader: - kv  13:                      phi3.embedding_length u32              = 3072\nllama_model_loader: - kv  14:                   phi3.feed_forward_length u32              = 8192\nllama_model_loader: - kv  15:                           phi3.block_count u32              = 32\nllama_model_loader: - kv  16:                  phi3.attention.head_count u32              = 32\nllama_model_loader: - kv  17:               phi3.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  18:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  19:                  phi3.rope.dimension_count u32              = 96\nllama_model_loader: - kv  20:                        phi3.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144\nllama_model_loader: - kv  22:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  25:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 32000\nllama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 32000\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  32:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\nllama_model_loader: - kv  34:               general.quantization_version u32              = 2\nllama_model_loader: - kv  35:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:   67 tensors\nllama_model_loader: - type q4_K:   81 tensors\nllama_model_loader: - type q5_K:   32 tensors\nllama_model_loader: - type q6_K:   17 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.23 GiB (5.01 BPW) \nload_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)\nload: printing all EOG tokens:\nload:   - 32000 ('<|endoftext|>')\nload:   - 32007 ('<|end|>')\nload: special tokens cache size = 14\nload: token to piece cache size = 0.1685 MB\nprint_info: arch             = phi3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 3072\nprint_info: n_embd_inp       = 3072\nprint_info: n_layer          = 32\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 32\nprint_info: n_rot            = 96\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 96\nprint_info: n_embd_head_v    = 96\nprint_info: n_gqa            = 1\nprint_info: n_embd_k_gqa     = 3072\nprint_info: n_embd_v_gqa     = 3072\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 8192\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 4096\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 3B\nprint_info: model params     = 3.82 B\nprint_info: general.name     = Phi 3.5 Mini Instruct\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 32064\nprint_info: n_merges         = 0\nprint_info: BOS token        = 1 '<s>'\nprint_info: EOS token        = 32000 '<|endoftext|>'\nprint_info: EOT token        = 32007 '<|end|>'\nprint_info: UNK token        = 0 '<unk>'\nprint_info: PAD token        = 32000 '<|endoftext|>'\nprint_info: LF token         = 13 '<0x0A>'\nprint_info: EOG token        = 32000 '<|endoftext|>'\nprint_info: EOG token        = 32007 '<|end|>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2254.66 MiB\nload_tensors:   CPU_REPACK model buffer size =  1242.00 MiB\n............................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_seq     = 8192\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 10000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.12 MiB\nllama_kv_cache:        CPU KV buffer size =  3072.00 MiB\nllama_kv_cache: size = 3072.00 MiB (  8192 cells,  32 layers,  1/1 seqs), K (f16): 1536.00 MiB, V (f16): 1536.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =    68.62 MiB\nllama_context: graph nodes  = 935\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|end|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | REPACK = 1 | \n\nsampler seed: 3933202565\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 8192, n_batch = 2048, n_predict = 400, n_keep = 0\n\ncommon_perf_print:    sampling time =      42.81 ms\ncommon_perf_print:    samplers time =       7.29 ms /  1100 tokens\ncommon_perf_print:        load time =     745.89 ms\ncommon_perf_print: prompt eval time =   40434.05 ms /   898 tokens (   45.03 ms per token,    22.21 tokens per second)\ncommon_perf_print:        eval time =   19142.84 ms /   201 runs   (   95.24 ms per token,    10.50 tokens per second)\ncommon_perf_print:       total time =   59629.18 ms /  1099 tokens\ncommon_perf_print: unaccounted time =       9.48 ms /   0.0 %      (total - sampling - prompt eval - eval) / (total)\ncommon_perf_print:    graphs reused =        199\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                 5395 =  2254 +    3072 +      68                |\nllama_memory_breakdown_print: |   - CPU_REPACK         |                 1242 =  1242 +       0 +       0                |\n",
    "raw_model_output": "Olivia de Havilland mourns the loss of her sister Joan Fontaine, both actresses having had nominations for the best actress Oscar in 1941 and feuding throughout their careers. The two were born only 15 months apart and their rivalry seemed to be encouraged by their ambitious stage mother. Olivia, who now lives in Paris, has been overwhelmed by the expressions of sympathy since Joan's death at the age of 96. Both sisters were nominated for the best actress Oscar in 1941, with Joan winning for her role in 'Suspicion'. Their relationship was fraught with tension, including a reported incident where Olivia fractured Fontaine's collarbone. The sisters' strained relationship was well-known, with Fontaine stating she would be angry if she died before Olivia."
  }
]